# NOFX Python å®æˆ˜éƒ¨ç½²æŒ‡å—

## Practical Implementation & Deployment Guide

**æ–‡æ¡£ç±»å‹ï¼šå®æˆ˜éƒ¨ç½²æŒ‡å—**
**ç›®æ ‡ï¼šç”Ÿäº§å¯ç”¨çš„äº¤æ˜“ç³»ç»Ÿ**
**çŠ¶æ€ï¼šå®é™…å¯éƒ¨ç½²**

---

## æ–‡æ¡£è¯´æ˜

æœ¬æ–‡æ¡£ä¸“æ³¨äº **NOFX äº¤æ˜“ç³»ç»Ÿçš„å®é™…éƒ¨ç½²å’Œè¿ç»´**ï¼ŒåŒ…å«å¯ç›´æ¥ç”¨äºç”Ÿäº§ç¯å¢ƒçš„ä»£ç ã€é…ç½®å’Œæ¶æ„è®¾è®¡ã€‚

### ä¸å…¶ä»–æ–‡æ¡£çš„å…³ç³»

| æ–‡æ¡£ | ç« èŠ‚ | å†…å®¹ | å®é™…ç”¨é€” |
|------|------|------|----------|
| **å®æˆ˜éƒ¨ç½²æŒ‡å—** | æœ¬æ–‡æ¡£ | ç”Ÿäº§ä»£ç ã€éƒ¨ç½²é…ç½® | **ç«‹å³å¯ç”¨** |
| åŸºç¡€-ä¸“å®¶çº§ | 1-50 | æ ¸å¿ƒå®ç°ä»£ç  | å¯å‚è€ƒå®ç° |
| é«˜çº§ç†è®º | 51-85 | å‰æ²¿ç ”ç©¶ | ç ”ç©¶å‚è€ƒ |

---

## ç¬¬1ç«  ç³»ç»Ÿæ¶æ„è®¾è®¡

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NOFX Trading System                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Market     â”‚    â”‚    Trading    â”‚    â”‚    Risk      â”‚   â”‚
â”‚  â”‚   Data       â”‚â”€â”€â”€â†’â”‚    Engine     â”‚â”€â”€â”€â†’â”‚    Manager   â”‚   â”‚
â”‚  â”‚   Connectors â”‚    â”‚               â”‚    â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â†“                    â†“                    â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Message    â”‚    â”‚    Order      â”‚    â”‚   Position   â”‚   â”‚
â”‚  â”‚    Queue     â”‚    â”‚   Management  â”‚    â”‚   Tracking   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                    â”‚                    â”‚          â”‚
â”‚         â†“                    â†“                    â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Database & Cache Layer                   â”‚ â”‚
â”‚  â”‚  (PostgreSQL + Redis + TimescaleDB)                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æŠ€æœ¯æ ˆé€‰å‹

```yaml
æ ¸å¿ƒæ¡†æ¶:
  è¯­è¨€: Python 3.11+
  å¼‚æ­¥æ¡†æ¶: asyncio + aiohttp
  WebSocket: websockets

æ•°æ®å­˜å‚¨:
  æ—¶åºæ•°æ®: TimescaleDB (PostgreSQL extension)
  ç¼“å­˜: Redis 7+
  æ¶ˆæ¯é˜Ÿåˆ—: RabbitMQ / Redis Streams
  é…ç½®å­˜å‚¨: etcd / Consul

æ•°æ®å¤„ç†:
  æ•°å€¼è®¡ç®—: NumPy, Pandas
  æœºå™¨å­¦ä¹ : PyTorch, scikit-learn
  æŠ€æœ¯æŒ‡æ ‡: TA-Lib, pandas-ta

éƒ¨ç½²:
  å®¹å™¨åŒ–: Docker, Docker Compose
  ç¼–æ’: Kubernetes
  CI/CD: GitHub Actions / GitLab CI
  ç›‘æ§: Prometheus + Grafana
  æ—¥å¿—: ELK Stack (Elasticsearch, Logstash, Kibana)

äº¤æ˜“æ‰€API:
  Aè‚¡: åŒèŠ±é¡ºiFinD, ä¸œæ–¹è´¢å¯ŒChoice
  æ¸¯è‚¡: å¯Œé€”ç‰›ç‰›, è€è™è¯åˆ¸
  åŠ å¯†è´§å¸: ccxt (ç»Ÿä¸€æ¥å£)
```

### 1.3 é¡¹ç›®ç»“æ„

```
nofx_trading/
â”œâ”€â”€ docker/                      # Dockeré…ç½®
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ kubernetes/              # K8sé…ç½®
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/                  # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ settings.py          # åŸºç¡€é…ç½®
â”‚   â”‚   â”œâ”€â”€ exchanges.py         # äº¤æ˜“æ‰€é…ç½®
â”‚   â”‚   â””â”€â”€ logging.py           # æ—¥å¿—é…ç½®
â”‚   â”œâ”€â”€ core/                    # æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ engine.py            # äº¤æ˜“å¼•æ“
â”‚   â”‚   â”œâ”€â”€ order_manager.py     # è®¢å•ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ position_tracker.py  # æŒä»“è·Ÿè¸ª
â”‚   â”‚   â””â”€â”€ risk_manager.py      # é£é™©ç®¡ç†
â”‚   â”œâ”€â”€ data/                    # æ•°æ®å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ connectors/          # äº¤æ˜“æ‰€è¿æ¥å™¨
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”œâ”€â”€ ashare.py        # Aè‚¡
â”‚   â”‚   â”‚   â”œâ”€â”€ hkstock.py        # æ¸¯è‚¡
â”‚   â”‚   â”‚   â””â”€â”€ crypto.py         # åŠ å¯†è´§å¸
â”‚   â”‚   â”œâ”€â”€ processors/          # æ•°æ®å¤„ç†å™¨
â”‚   â”‚   â””â”€â”€ storage/             # æ•°æ®å­˜å‚¨
â”‚   â”œâ”€â”€ strategies/              # äº¤æ˜“ç­–ç•¥
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â””â”€â”€ implementations/
â”‚   â”œâ”€â”€ analysis/                # åˆ†ææ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ technical.py         # æŠ€æœ¯åˆ†æ
â”‚   â”‚   â”œâ”€â”€ fundamental.py       # åŸºæœ¬é¢åˆ†æ
â”‚   â”‚   â””â”€â”€ sentiment.py         # æƒ…ç»ªåˆ†æ
â”‚   â”œâ”€â”€ execution/               # æ‰§è¡Œæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ order_execution.py
â”‚   â”‚   â””â”€â”€ slippage_model.py
â”‚   â””â”€â”€ utils/                   # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ metrics.py
â”œâ”€â”€ tests/                       # æµ‹è¯•
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ e2e/
â”œâ”€â”€ scripts/                     # è„šæœ¬
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ backup.sh
â”œâ”€â”€ docs/                        # æ–‡æ¡£
â”œâ”€â”€ requirements.txt             # Pythonä¾èµ–
â”œâ”€â”€ pyproject.toml              # é¡¹ç›®é…ç½®
â””â”€â”€ README.md
```

---

## ç¬¬2ç«  æ ¸å¿ƒä»£ç å®ç°

### 2.1 äº¤æ˜“å¼•æ“

```python
# src/core/engine.py

import asyncio
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class OrderSide(Enum):
    BUY = "buy"
    SELL = "sell"

class OrderType(Enum):
    MARKET = "market"
    LIMIT = "limit"
    STOP = "stop"

class OrderStatus(Enum):
    PENDING = "pending"
    SUBMITTED = "submitted"
    PARTIAL_FILLED = "partial_filled"
    FILLED = "filled"
    CANCELLED = "cancelled"
    REJECTED = "rejected"

@dataclass
class Order:
    """è®¢å•æ•°æ®ç»“æ„"""
    order_id: str
    symbol: str
    side: OrderSide
    order_type: OrderType
    quantity: float
    price: Optional[float] = None
    status: OrderStatus = OrderStatus.PENDING
    filled_quantity: float = 0.0
    filled_price: float = 0.0
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: Optional[datetime] = None
    exchange_order_id: Optional[str] = None

class TradingEngine:
    """äº¤æ˜“å¼•æ“"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.running = False
        self.orders: Dict[str, Order] = {}
        self.order_queue = asyncio.Queue()
        self.position_tracker = None  # æ³¨å…¥ä¾èµ–
        self.risk_manager = None  # æ³¨å…¥ä¾èµ–
        self.connectors = {}  # äº¤æ˜“æ‰€è¿æ¥å™¨

    async def start(self):
        """å¯åŠ¨äº¤æ˜“å¼•æ“"""
        logger.info("Starting Trading Engine...")
        self.running = True

        # å¯åŠ¨å„ä¸ªç»„ä»¶
        await self._start_connectors()
        await self._start_order_processing()
        await self._start_position_tracking()

        logger.info("Trading Engine started")

    async def stop(self):
        """åœæ­¢äº¤æ˜“å¼•æ“"""
        logger.info("Stopping Trading Engine...")
        self.running = False

        # å…³é—­æ‰€æœ‰è¿æ¥
        for connector in self.connectors.values():
            await connector.close()

        logger.info("Trading Engine stopped")

    async def _start_connectors(self):
        """å¯åŠ¨äº¤æ˜“æ‰€è¿æ¥å™¨"""
        from src.data.connectors.ashare import AShareConnector
        from src.data.connectors.hkstock import HKStockConnector
        from src.data.connectors.crypto import CryptoConnector

        # Aè‚¡è¿æ¥å™¨
        if self.config.get('ashare', {}).get('enabled', False):
            ashare_connector = AShareConnector(self.config['ashare'])
            await ashare_connector.connect()
            self.connectors['ashare'] = ashare_connector

        # æ¸¯è‚¡è¿æ¥å™¨
        if self.config.get('hkstock', {}).get('enabled', False):
            hk_connector = HKStockConnector(self.config['hkstock'])
            await hk_connector.connect()
            self.connectors['hkstock'] = hk_connector

        # åŠ å¯†è´§å¸è¿æ¥å™¨
        if self.config.get('crypto', {}).get('enabled', False):
            crypto_connector = CryptoConnector(self.config['crypto'])
            await crypto_connector.connect()
            self.connectors['crypto'] = crypto_connector

    async def submit_order(self, order: Order) -> bool:
        """æäº¤è®¢å•"""
        logger.info(f"Submitting order: {order.order_id}")

        # é£é™©æ£€æŸ¥
        if not await self._check_risk(order):
            logger.warning(f"Order {order.order_id} rejected by risk manager")
            order.status = OrderStatus.REJECTED
            return False

        # æ·»åŠ åˆ°é˜Ÿåˆ—
        await self.order_queue.put(order)
        self.orders[order.order_id] = order

        return True

    async def _start_order_processing(self):
        """å¯åŠ¨è®¢å•å¤„ç†"""
        while self.running:
            try:
                order = await asyncio.wait_for(
                    self.order_queue.get(),
                    timeout=1.0
                )

                # å¤„ç†è®¢å•
                await self._process_order(order)

            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Error processing order: {e}")

    async def _process_order(self, order: Order):
        """å¤„ç†å•ä¸ªè®¢å•"""
        try:
            # æ ¹æ®symbolé€‰æ‹©è¿æ¥å™¨
            connector = self._get_connector_for_symbol(order.symbol)

            if connector is None:
                logger.error(f"No connector found for {order.symbol}")
                order.status = OrderStatus.REJECTED
                return

            # æäº¤åˆ°äº¤æ˜“æ‰€
            result = await connector.submit_order(order)

            if result.get('success'):
                order.status = OrderStatus.SUBMITTED
                order.exchange_order_id = result.get('exchange_order_id')
                order.updated_at = datetime.now()
                logger.info(f"Order {order.order_id} submitted successfully")
            else:
                order.status = OrderStatus.REJECTED
                logger.error(f"Order {order.order_id} rejected: {result.get('message')}")

        except Exception as e:
            logger.error(f"Error processing order {order.order_id}: {e}")
            order.status = OrderStatus.REJECTED

    def _get_connector_for_symbol(self, symbol: str):
        """æ ¹æ®symbolè·å–è¿æ¥å™¨"""
        # ç®€åŒ–å®ç°ï¼šæ ¹æ®symbolå‰ç¼€åˆ¤æ–­
        if symbol.endswith('.SH') or symbol.endswith('.SZ'):
            return self.connectors.get('ashare')
        elif symbol.isdigit() and len(symbol) == 5:  # æ¸¯è‚¡ä»£ç é€šå¸¸æ˜¯5ä½
            return self.connectors.get('hkstock')
        else:
            return self.connectors.get('crypto')

    async def _check_risk(self, order: Order) -> bool:
        """é£é™©æ£€æŸ¥"""
        if self.risk_manager:
            return await self.risk_manager.check_order(order)
        return True

    async def _start_position_tracking(self):
        """å¯åŠ¨æŒä»“è·Ÿè¸ª"""
        # å®ç°æŒä»“è·Ÿè¸ªé€»è¾‘
        pass

    async def cancel_order(self, order_id: str) -> bool:
        """å–æ¶ˆè®¢å•"""
        if order_id not in self.orders:
            logger.warning(f"Order {order_id} not found")
            return False

        order = self.orders[order_id]

        if order.status not in [OrderStatus.PENDING, OrderStatus.SUBMITTED]:
            logger.warning(f"Order {order_id} cannot be cancelled (status: {order.status})")
            return False

        # å–æ¶ˆè®¢å•
        connector = self._get_connector_for_symbol(order.symbol)
        if connector:
            result = await connector.cancel_order(order)
            if result.get('success'):
                order.status = OrderStatus.CANCELLED
                order.updated_at = datetime.now()
                return True

        return False

    def get_order_status(self, order_id: str) -> Optional[Order]:
        """è·å–è®¢å•çŠ¶æ€"""
        return self.orders.get(order_id)
```

### 2.2 è®¢å•ç®¡ç†å™¨

```python
# src/core/order_manager.py

from typing import Dict, List, Optional
from datetime import datetime, timedelta
import asyncio
import logging

logger = logging.getLogger(__name__)

class OrderManager:
    """è®¢å•ç®¡ç†å™¨"""

    def __init__(self):
        self.active_orders: Dict[str, Order] = {}
        self.completed_orders: List[Order] = []
        self.order_history: Dict[str, List[Order]] = {}
        self.lock = asyncio.Lock()

    async def add_order(self, order: Order) -> bool:
        """æ·»åŠ è®¢å•"""
        async with self.lock:
            if order.order_id in self.active_orders:
                logger.warning(f"Order {order.order_id} already exists")
                return False

            self.active_orders[order.order_id] = order
            logger.info(f"Order {order.order_id} added to active orders")
            return True

    async def update_order(self, order_id: str, **kwargs) -> bool:
        """æ›´æ–°è®¢å•"""
        async with self.lock:
            if order_id not in self.active_orders:
                return False

            order = self.active_orders[order_id]
            for key, value in kwargs.items():
                if hasattr(order, key):
                    setattr(order, key, value)

            order.updated_at = datetime.now()
            return True

    async def complete_order(self, order_id: str):
        """å®Œæˆè®¢å•"""
        async with self.lock:
            if order_id not in self.active_orders:
                return

            order = self.active_orders.pop(order_id)
            self.completed_orders.append(order)

            # æ·»åŠ åˆ°å†å²è®°å½•
            if order.symbol not in self.order_history:
                self.order_history[order.symbol] = []
            self.order_history[order.symbol].append(order)

    def get_active_orders(self, symbol: Optional[str] = None) -> List[Order]:
        """è·å–æ´»è·ƒè®¢å•"""
        if symbol:
            return [o for o in self.active_orders.values() if o.symbol == symbol]
        return list(self.active_orders.values())

    def get_order_history(self, symbol: str,
                         days: int = 30) -> List[Order]:
        """è·å–è®¢å•å†å²"""
        if symbol not in self.order_history:
            return []

        cutoff_date = datetime.now() - timedelta(days=days)
        return [
            o for o in self.order_history[symbol]
            if o.created_at >= cutoff_date
        ]
```

---

## ç¬¬3ç«  æ•°æ®è¿æ¥å™¨

### 3.1 Aè‚¡è¿æ¥å™¨

```python
# src/data/connectors/ashare.py

import asyncio
import aiohttp
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class AShareConnector:
    """Aè‚¡å¸‚åœºæ•°æ®è¿æ¥å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = config.get('api_key')
        self.base_url = config.get('base_url', 'https://api.example.com')
        self.session: Optional[aiohttp.ClientSession] = None

    async def connect(self):
        """å»ºç«‹è¿æ¥"""
        self.session = aiohttp.ClientSession()
        logger.info("AShare connector connected")

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.session:
            await self.session.close()
            logger.info("AShare connector closed")

    async def get_quote(self, symbol: str) -> Optional[Dict[str, Any]]:
        """è·å–å®æ—¶è¡Œæƒ…"""
        if not self.session:
            return None

        url = f"{self.base_url}/quote"
        params = {'symbol': symbol, 'api_key': self.api_key}

        try:
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        'symbol': symbol,
                        'price': data.get('price'),
                        'volume': data.get('volume'),
                        'timestamp': datetime.now()
                    }
        except Exception as e:
            logger.error(f"Error getting quote for {symbol}: {e}")

        return None

    async def submit_order(self, order) -> Dict[str, Any]:
        """æäº¤è®¢å•"""
        # å®ç°å®é™…çš„è®¢å•æäº¤é€»è¾‘
        return {'success': True, 'exchange_order_id': f"ASH_{order.order_id}"}

    async def cancel_order(self, order) -> Dict[str, Any]:
        """å–æ¶ˆè®¢å•"""
        # å®ç°å®é™…çš„è®¢å•å–æ¶ˆé€»è¾‘
        return {'success': True}
```

### 3.2 åŠ å¯†è´§å¸è¿æ¥å™¨

```python
# src/data/connectors/crypto.py

import ccxt.async_support as ccxt
import logging
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)

class CryptoConnector:
    """åŠ å¯†è´§å¸è¿æ¥å™¨ï¼ˆä½¿ç”¨ccxtï¼‰"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.exchange_id = config.get('exchange', 'binance')
        self.exchange: Optional[ccxt.Exchange] = None

    async def connect(self):
        """å»ºç«‹è¿æ¥"""
        exchange_class = getattr(ccxt, self.exchange_id)
        self.exchange = exchange_class({
            'apiKey': self.config.get('api_key'),
            'secret': self.config.get('api_secret'),
            'enableRateLimit': True,
        })

        await self.exchange.load_markets()
        logger.info(f"Crypto connector connected to {self.exchange_id}")

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.exchange:
            await self.exchange.close()
            logger.info("Crypto connector closed")

    async def get_quote(self, symbol: str) -> Optional[Dict[str, Any]]:
        """è·å–å®æ—¶è¡Œæƒ…"""
        if not self.exchange:
            return None

        try:
            ticker = await self.exchange.fetch_ticker(symbol)
            return {
                'symbol': symbol,
                'price': ticker['last'],
                'volume': ticker['baseVolume'],
                'timestamp': ticker['timestamp']
            }
        except Exception as e:
            logger.error(f"Error getting quote for {symbol}: {e}")

        return None

    async def submit_order(self, order) -> Dict[str, Any]:
        """æäº¤è®¢å•"""
        if not self.exchange:
            return {'success': False, 'message': 'Not connected'}

        try:
            if order.side == OrderSide.BUY:
                result = await self.exchange.create_market_buy_order(
                    order.symbol,
                    order.quantity
                )
            else:
                result = await self.exchange.create_market_sell_order(
                    order.symbol,
                    order.quantity
                )

            return {
                'success': True,
                'exchange_order_id': result.get('id')
            }
        except Exception as e:
            logger.error(f"Error submitting order: {e}")
            return {'success': False, 'message': str(e)}

    async def cancel_order(self, order) -> Dict[str, Any]:
        """å–æ¶ˆè®¢å•"""
        if not self.exchange or not order.exchange_order_id:
            return {'success': False}

        try:
            await self.exchange.cancel_order(order.exchange_order_id)
            return {'success': True}
        except Exception as e:
            logger.error(f"Error cancelling order: {e}")
            return {'success': False, 'message': str(e)}
```

---

## ç¬¬4ç«  éƒ¨ç½²é…ç½®

### 4.1 Dockeré…ç½®

```dockerfile
# docker/Dockerfile

FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "src.main"]
```

```yaml
# docker/docker-compose.yml

version: '3.8'

services:
  trading-engine:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    container_name: nofx-trading
    environment:
      - ENV=production
      - LOG_LEVEL=INFO
    volumes:
      - ../config:/app/config:ro
      - ../logs:/app/logs
    restart: unless-stopped
    depends_on:
      - postgres
      - redis

  postgres:
    image: timescale/timescaledb:latest-pg15
    container_name: nofx-postgres
    environment:
      POSTGRES_DB: nofx_trading
      POSTGRES_USER: nofx
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: nofx-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: nofx-grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  grafana_data:
```

### 4.2 Kubernetesé…ç½®

```yaml
# docker/kubernetes/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nofx-trading-engine
  namespace: trading
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nofx-trading
  template:
    metadata:
      labels:
        app: nofx-trading
        version: v1
    spec:
      containers:
      - name: trading-engine
        image: nofx/trading:latest
        ports:
        - containerPort: 8000
        env:
        - name: POSTGRES_HOST
          value: "postgres-service"
        - name: REDIS_HOST
          value: "redis-service"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: nofx-trading-service
  namespace: trading
spec:
  selector:
    app: nofx-trading
  ports:
  - port: 8000
    targetPort: 8000
  type: ClusterIP
```

---

## ç¬¬5ç«  å¯åŠ¨è„šæœ¬

### 5.1 éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

echo "ğŸš€ Deploying NOFX Trading System..."

# æ£€æŸ¥Docker
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker not found. Please install Docker first."
    exit 1
fi

# æ„å»ºé•œåƒ
echo "ğŸ“¦ Building Docker image..."
docker build -t nofx/trading:latest -f docker/Dockerfile .

# å¯åŠ¨æœåŠ¡
echo "ğŸ”„ Starting services..."
docker-compose -f docker/docker-compose.yml up -d

# ç­‰å¾…æœåŠ¡å°±ç»ª
echo "â³ Waiting for services to be ready..."
sleep 10

# æ£€æŸ¥çŠ¶æ€
echo "ğŸ“Š Service status:"
docker-compose -f docker/docker-compose.yml ps

echo "âœ… Deployment complete!"
echo "ğŸ“ˆ Dashboard: http://localhost:3000"
echo "ğŸ“š API Docs: http://localhost:8000/docs"
```

### 5.2 é…ç½®æ–‡ä»¶

```python
# config/settings.py

from typing import Dict, Any

import os
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
BASE_DIR = Path(__file__).parent.parent

# ç¯å¢ƒé…ç½®
ENV = os.getenv('ENV', 'development')

# æ—¥å¿—é…ç½®
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO')

# æ•°æ®åº“é…ç½®
DATABASE = {
    'host': os.getenv('POSTGRES_HOST', 'localhost'),
    'port': int(os.getenv('POSTGRES_PORT', 5432)),
    'database': os.getenv('POSTGRES_DB', 'nofx_trading'),
    'user': os.getenv('POSTGRES_USER', 'nofx'),
    'password': os.getenv('POSTGRES_PASSWORD', ''),
}

# Redisé…ç½®
REDIS = {
    'host': os.getenv('REDIS_HOST', 'localhost'),
    'port': int(os.getenv('REDIS_PORT', 6379)),
    'db': int(os.getenv('REDIS_DB', 0)),
}

# äº¤æ˜“æ‰€APIé…ç½®
EXCHANGES = {
    'ashare': {
        'enabled': os.getenv('ASHARE_ENABLED', 'false').lower() == 'true',
        'api_key': os.getenv('ASHARE_API_KEY', ''),
        'base_url': os.getenv('ASHARE_API_URL', ''),
    },
    'hkstock': {
        'enabled': os.getenv('HKSTOCK_ENABLED', 'false').lower() == 'true',
        'api_key': os.getenv('HKSTOCK_API_KEY', ''),
        'api_secret': os.getenv('HKSTOCK_API_SECRET', ''),
    },
    'crypto': {
        'enabled': os.getenv('CRYPTO_ENABLED', 'false').lower() == 'true',
        'exchange': os.getenv('CRYPTO_EXCHANGE', 'binance'),
        'api_key': os.getenv('CRYPTO_API_KEY', ''),
        'api_secret': os.getenv('CRYPTO_API_SECRET', ''),
    },
}

# é£é™©ç®¡ç†å‚æ•°
RISK_MANAGEMENT = {
    'max_position_size': float(os.getenv('MAX_POSITION_SIZE', 100000)),
    'max_daily_loss': float(os.getenv('MAX_DAILY_LOSS', 0.02)),
    'max_orders_per_minute': int(os.getenv('MAX_ORDERS_PER_MINUTE', 10)),
}

# äº¤æ˜“å‚æ•°
TRADING = {
    'default_slippage': float(os.getenv('DEFAULT_SLIPPAGE', 0.001)),
    'min_order_size': float(os.getenv('MIN_ORDER_SIZE', 100)),
    'commission_rate': float(os.getenv('COMMISSION_RATE', 0.0003)),
}
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£æä¾›äº† **NOFX äº¤æ˜“ç³»ç»Ÿçš„å®é™…éƒ¨ç½²æŒ‡å—**ï¼ŒåŒ…å«ï¼š

1. **ç³»ç»Ÿæ¶æ„è®¾è®¡** - æ¸…æ™°çš„æ¨¡å—åˆ’åˆ†
2. **æ ¸å¿ƒä»£ç å®ç°** - äº¤æ˜“å¼•æ“ã€è®¢å•ç®¡ç†
3. **æ•°æ®è¿æ¥å™¨** - Aè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸
4. **éƒ¨ç½²é…ç½®** - Dockerã€Kubernetes
5. **å¯åŠ¨è„šæœ¬** - ä¸€é”®éƒ¨ç½²

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼š

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository>

# 2. é…ç½®ç¯å¢ƒ
cp config/config.example.yml config/config.yml

# 3. éƒ¨ç½²
./scripts/deploy.sh

# 4. è®¿é—®ä»ªè¡¨æ¿
open http://localhost:3000
```

---

**æ–‡æ¡£çŠ¶æ€ï¼šç”Ÿäº§å°±ç»ª**
**æ›´æ–°æ—¥æœŸï¼š2026**
**é€‚ç”¨å¸‚åœºï¼šAè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸**

---

## ç¬¬6ç«  é£é™©ç®¡ç†ç³»ç»Ÿ

### 6.1 é£é™©ç®¡ç†å™¨å®ç°

```python
# src/core/risk_manager.py

from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import logging
import numpy as np

logger = logging.getLogger(__name__)

class RiskCheck(Enum):
    """é£é™©æ£€æŸ¥ç±»å‹"""
    POSITION_SIZE = "position_size"
    DAILY_LOSS = "daily_loss"
    EXPOSURE = "exposure"
    CORRELATION = "correlation"
    VOLATILITY = "volatility"
    CONCENTRATION = "concentration"

@dataclass
class RiskLimit:
    """é£é™©é™åˆ¶"""
    name: str
    check_type: RiskCheck
    limit: float
    current_value: float = 0.0
    alert_threshold: float = 0.8  # è¾¾åˆ°80%æ—¶å‘Šè­¦
    action: str = "reject"  # reject, warn, close

class RiskManager:
    """é£é™©ç®¡ç†å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.limits: Dict[str, RiskLimit] = {}
        self.positions: Dict[str, Dict[str, float]] = {}  # {symbol: {quantity, avg_cost}}
        self.daily_pnl: float = 0.0
        self.daily_start_value: float = 0.0
        self.alert_callbacks: List[Callable] = []

        # åˆå§‹åŒ–é£é™©é™åˆ¶
        self._init_limits()

    def _init_limits(self):
        """åˆå§‹åŒ–é£é™©é™åˆ¶"""
        # æœ€å¤§å•ç¬”æŒä»“
        self.limits['max_position'] = RiskLimit(
            name='max_position',
            check_type=RiskCheck.POSITION_SIZE,
            limit=self.config.get('max_position_size', 100000),
            action='reject'
        )

        # æœ€å¤§æ—¥æŸå¤±
        self.limits['daily_loss'] = RiskLimit(
            name='daily_loss',
            check_type=RiskCheck.DAILY_LOSS,
            limit=self.config.get('max_daily_loss', 0.02),  # 2%
            current_value=0.0,
            action='close'
        )

        # æœ€å¤§æ€»æ•å£
        self.limits['max_exposure'] = RiskLimit(
            name='max_exposure',
            check_type=RiskCheck.EXPOSURE,
            limit=self.config.get('max_exposure', 500000),
            action='reject'
        )

    async def check_order(self, order) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥è®¢å•æ˜¯å¦ç¬¦åˆé£é™©é™åˆ¶

        è¿”å›: (æ˜¯å¦é€šè¿‡, å¤±è´¥åŸå› åˆ—è¡¨)
        """
        reasons = []

        # 1. æ£€æŸ¥å•ç¬”æŒä»“å¤§å°
        if not await self._check_position_size(order):
            reasons.append(f"Position size exceeds limit")

        # 2. æ£€æŸ¥æ—¥æŸå¤±é™åˆ¶
        if not await self._check_daily_loss():
            reasons.append(f"Daily loss limit reached")

        # 3. æ£€æŸ¥æ€»æ•å£
        if not await self._check_exposure(order):
            reasons.append(f"Total exposure exceeds limit")

        # 4. æ£€æŸ¥é›†ä¸­åº¦
        if not await self._check_concentration(order):
            reasons.append(f"Concentration limit exceeded")

        is_valid = len(reasons) == 0

        if not is_valid:
            logger.warning(f"Order {order.order_id} rejected: {', '.join(reasons)}")

        return is_valid, reasons

    async def _check_position_size(self, order) -> bool:
        """æ£€æŸ¥æŒä»“å¤§å°"""
        limit = self.limits['max_position']
        notional = order.quantity * (order.price or 0)

        # æ£€æŸ¥å•ç¬”è®¢å•
        if notional > limit.limit:
            logger.warning(f"Order size {notional} exceeds limit {limit.limit}")
            return False

        # æ£€æŸ¥ç´¯ç§¯æŒä»“
        current_pos = self.positions.get(order.symbol, {}).get('quantity', 0)
        total_notional = (current_pos + order.quantity) * (order.price or 0)

        if total_notional > limit.limit:
            logger.warning(f"Total position {total_notional} would exceed limit {limit.limit}")
            return False

        return True

    async def _check_daily_loss(self) -> bool:
        """æ£€æŸ¥æ—¥æŸå¤±"""
        limit = self.limits['daily_loss']

        # è®¡ç®—å½“å‰æ—¥æŸå¤±ç‡
        if self.daily_start_value > 0:
            loss_rate = abs(min(self.daily_pnl, 0)) / self.daily_start_value
            limit.current_value = loss_rate

            # æ£€æŸ¥å‘Šè­¦é˜ˆå€¼
            if loss_rate > limit.alert_threshold:
                await self._send_alert(limit)

            if loss_rate >= limit.limit:
                logger.error(f"Daily loss {loss_rate:.2%} reached limit {limit.limit:.2%}")
                await self._send_alert(limit, critical=True)
                return False

        return True

    async def _check_exposure(self, order) -> bool:
        """æ£€æŸ¥æ€»æ•å£"""
        limit = self.limits['max_exposure']

        # è®¡ç®—å½“å‰æ€»æ•å£
        total_exposure = sum(
            pos.get('quantity', 0) * pos.get('current_price', 0)
            for pos in self.positions.values()
        )

        # åŠ ä¸Šæ–°è®¢å•
        new_exposure = total_exposure + (order.quantity * (order.price or 0))
        limit.current_value = new_exposure

        if new_exposure > limit.limit:
            logger.warning(f"Total exposure {new_exposure} would exceed limit {limit.limit}")
            return False

        return True

    async def _check_concentration(self, order) -> bool:
        """æ£€æŸ¥é›†ä¸­åº¦"""
        concentration_limit = self.config.get('max_concentration', 0.3)  # 30%

        # è®¡ç®—å•ä¸ªå“ç§å æ¯”
        total_value = sum(
            pos.get('quantity', 0) * pos.get('current_price', 0)
            for pos in self.positions.values()
        )

        if total_value > 0:
            current_pos_value = self.positions.get(order.symbol, {}).get('quantity', 0) * (order.price or 0)
            new_pos_value = current_pos_value + (order.quantity * (order.price or 0))
            concentration = new_pos_value / (total_value + (order.quantity * (order.price or 0)))

            if concentration > concentration_limit:
                logger.warning(f"Concentration {concentration:.2%} exceeds limit {concentration_limit:.2%}")
                return False

        return True

    def update_position(self, symbol: str, quantity: float, price: float):
        """æ›´æ–°æŒä»“"""
        if symbol not in self.positions:
            self.positions[symbol] = {'quantity': 0, 'avg_cost': 0}

        pos = self.positions[symbol]
        old_quantity = pos['quantity']
        old_cost = pos['quantity'] * pos['avg_cost']

        # æ›´æ–°æŒä»“æ•°é‡
        pos['quantity'] = old_quantity + quantity

        # æ›´æ–°å¹³å‡æˆæœ¬
        if pos['quantity'] != 0:
            pos['avg_cost'] = (old_cost + quantity * price) / pos['quantity']

        # æ›´æ–°å½“å‰ä»·æ ¼
        pos['current_price'] = price

        logger.info(f"Position updated: {symbol} quantity={pos['quantity']}, avg_cost={pos['avg_cost']:.2f}")

    def update_pnl(self, realized_pnl: float):
        """æ›´æ–°ç›ˆäº"""
        self.daily_pnl += realized_pnl
        logger.info(f"Daily PnL updated: {self.daily_pnl:.2f}")

    async def _send_alert(self, limit: RiskLimit, critical: bool = False):
        """å‘é€å‘Šè­¦"""
        message = f"Risk alert: {limit.name} at {limit.current_value:.2%} of limit {limit.limit:.2%}"

        for callback in self.alert_callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(message, critical)
                else:
                    callback(message, critical)
            except Exception as e:
                logger.error(f"Error in alert callback: {e}")

    def add_alert_callback(self, callback: Callable):
        """æ·»åŠ å‘Šè­¦å›è°ƒ"""
        self.alert_callbacks.append(callback)

    def reset_daily(self):
        """é‡ç½®æ¯æ—¥æ•°æ®"""
        self.daily_pnl = 0.0
        self.daily_start_value = sum(
            pos.get('quantity', 0) * pos.get('current_price', pos.get('avg_cost', 0))
            for pos in self.positions.values()
        )
        logger.info(f"Daily risk reset, start value: {self.daily_start_value:.2f}")

    def get_risk_report(self) -> Dict[str, Any]:
        """è·å–é£é™©æŠ¥å‘Š"""
        return {
            'timestamp': datetime.now().isoformat(),
            'daily_pnl': self.daily_pnl,
            'daily_pnl_rate': self.daily_pnl / self.daily_start_value if self.daily_start_value > 0 else 0,
            'positions': self.positions.copy(),
            'limits': {
                name: {
                    'current': limit.current_value,
                    'limit': limit.limit,
                    'utilization': limit.current_value / limit.limit if limit.limit > 0 else 0
                }
                for name, limit in self.limits.items()
            }
        }
```

### 6.2 æŒä»“è·Ÿè¸ªå™¨

```python
# src/core/position_tracker.py

from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass, field
from enum import Enum
import asyncio
import logging

logger = logging.getLogger(__name__)

class PositionSide(Enum):
    LONG = "long"
    SHORT = "short"

@dataclass
class Position:
    """æŒä»“æ•°æ®"""
    symbol: str
    side: PositionSide
    quantity: float
    avg_cost: float
    current_price: float = 0.0
    market_value: float = 0.0
    unrealized_pnl: float = 0.0
    realized_pnl: float = 0.0
    opened_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

class PositionTracker:
    """æŒä»“è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.positions: Dict[str, Position] = {}
        self.closed_positions: List[Position] = []
        self.lock = asyncio.Lock()

    async def update_position(self, symbol: str, side: PositionSide,
                              quantity: float, price: float, is_open: bool = True) -> Position:
        """æ›´æ–°æŒä»“"""
        async with self.lock:
            if is_open:
                # å¼€ä»“
                if symbol in self.positions:
                    # åŠ ä»“
                    pos = self.positions[symbol]
                    old_quantity = pos.quantity
                    old_cost = pos.quantity * pos.avg_cost

                    pos.quantity = old_quantity + quantity
                    pos.avg_cost = (old_cost + quantity * price) / pos.quantity
                    pos.updated_at = datetime.now()

                    logger.info(f"Position increased: {symbol} {side.value} +{quantity} @ {price:.2f}")
                else:
                    # æ–°å¼€ä»“
                    self.positions[symbol] = Position(
                        symbol=symbol,
                        side=side,
                        quantity=quantity,
                        avg_cost=price,
                        current_price=price,
                        market_value=quantity * price,
                        opened_at=datetime.now()
                    )

                    logger.info(f"Position opened: {symbol} {side.value} {quantity} @ {price:.2f}")

                return self.positions[symbol]

            else:
                # å¹³ä»“
                return await self._close_position(symbol, quantity, price)

    async def _close_position(self, symbol: str, quantity: float, price: float) -> Optional[Position]:
        """å¹³ä»“"""
        if symbol not in self.positions:
            logger.warning(f"Cannot close position {symbol}: position not found")
            return None

        pos = self.positions[symbol]

        if quantity > pos.quantity:
            logger.warning(f"Cannot close {quantity} of {symbol}: only {pos.quantity} available")
            return None

        # è®¡ç®—å·²å®ç°ç›ˆäº
        if pos.side == PositionSide.LONG:
            realized_pnl = (price - pos.avg_cost) * quantity
        else:
            realized_pnl = (pos.avg_cost - price) * quantity

        pos.realized_pnl += realized_pnl
        pos.quantity -= quantity
        pos.updated_at = datetime.now()

        logger.info(f"Position closed: {symbol} {quantity} @ {price:.2f}, PnL: {realized_pnl:.2f}")

        # å¦‚æœå…¨éƒ¨å¹³ä»“ï¼Œç§»åˆ°å†å²è®°å½•
        if pos.quantity == 0:
            closed_pos = self.positions.pop(symbol)
            closed_pos.quantity = 0  # å·²å…¨éƒ¨å¹³ä»“
            self.closed_positions.append(closed_pos)
            return closed_pos

        return pos

    async def update_market_data(self, symbol: str, current_price: float):
        """æ›´æ–°å¸‚åœºæ•°æ®"""
        async with self.lock:
            if symbol in self.positions:
                pos = self.positions[symbol]
                pos.current_price = current_price
                pos.market_value = pos.quantity * current_price
                pos.updated_at = datetime.now()

                # è®¡ç®—æœªå®ç°ç›ˆäº
                if pos.side == PositionSide.LONG:
                    pos.unrealized_pnl = (current_price - pos.avg_cost) * pos.quantity
                else:
                    pos.unrealized_pnl = (pos.avg_cost - current_price) * pos.quantity

    def get_open_positions(self) -> List[Position]:
        """è·å–æ‰€æœ‰å¼€æ”¾æŒä»“"""
        return list(self.positions.values())

    def get_position(self, symbol: str) -> Optional[Position]:
        """è·å–ç‰¹å®šæŒä»“"""
        return self.positions.get(symbol)

    def get_portfolio_summary(self) -> Dict[str, Any]:
        """è·å–æŠ•èµ„ç»„åˆæ‘˜è¦"""
        long_positions = [p for p in self.positions.values() if p.side == PositionSide.LONG]
        short_positions = [p for p in self.positions.values() if p.side == PositionSide.SHORT]

        total_long_value = sum(p.market_value for p in long_positions)
        total_short_value = sum(p.market_value for p in short_positions)
        total_unrealized_pnl = sum(p.unrealized_pnl for p in self.positions.values())
        total_realized_pnl = sum(p.realized_pnl for p in self.positions.values())

        return {
            'timestamp': datetime.now().isoformat(),
            'num_positions': len(self.positions),
            'long_count': len(long_positions),
            'short_count': len(short_positions),
            'long_value': total_long_value,
            'short_value': total_short_value,
            'net_value': total_long_value - total_short_value,
            'unrealized_pnl': total_unrealized_pnl,
            'realized_pnl': total_realized_pnl,
            'total_pnl': total_unrealized_pnl + total_realized_pnl
        }
```

---

## ç¬¬7ç«  äº¤æ˜“ç­–ç•¥å®ç°

### 7.1 ç­–ç•¥åŸºç±»

```python
# src/strategies/base.py

from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any
from datetime import datetime
import pandas as pd
import numpy as np
import logging

logger = logging.getLogger(__name__)

class Signal:
    """äº¤æ˜“ä¿¡å·"""
    BUY = "buy"
    SELL = "sell"
    HOLD = "hold"

class Strategy(ABC):
    """ç­–ç•¥åŸºç±»"""

    def __init__(self, name: str, config: Dict[str, Any]):
        self.name = name
        self.config = config
        self.positions: Dict[str, float] = {}  # å½“å‰æŒä»“

    @abstractmethod
    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """
        ç”Ÿæˆäº¤æ˜“ä¿¡å·

        è¿”å›: {symbol: signal}
        signal: Signal.BUY | Signal.SELL | Signal.HOLD
        """
        pass

    @abstractmethod
    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        pass

    def get_name(self) -> str:
        """è·å–ç­–ç•¥åç§°"""
        return self.name

    async def on_data(self, market_data: Dict[str, pd.DataFrame]):
        """å¤„ç†å¸‚åœºæ•°æ®"""
        signals = await self.generate_signals(market_data.get('quotes', pd.DataFrame()))

        # è®¡ç®—ä»“ä½å¤§å°
        positions = {}
        for symbol, signal in signals.items():
            if signal != Signal.HOLD:
                price = self._get_current_price(market_data, symbol)
                portfolio_value = self._get_portfolio_value()
                size = await self.calculate_position_size(symbol, signal, price, portfolio_value)
                positions[symbol] = size

        return {
            'signals': signals,
            'positions': positions,
            'timestamp': datetime.now().isoformat()
        }

    def _get_current_price(self, market_data: Dict[str, pd.DataFrame], symbol: str) -> float:
        """è·å–å½“å‰ä»·æ ¼"""
        quotes = market_data.get('quotes', pd.DataFrame())
        if not quotes.empty and symbol in quotes.index:
            return quotes.loc[symbol, 'close']
        return 0.0

    def _get_portfolio_value(self) -> float:
        """è·å–æŠ•èµ„ç»„åˆä»·å€¼"""
        return sum(self.positions.values())  # ç®€åŒ–å®ç°
```

### 7.2 ç§»åŠ¨å¹³å‡ç­–ç•¥

```python
# src/strategies/implementations/moving_average.py

from typing import Dict, List
import pandas as pd
import numpy as np
from src.strategies.base import Strategy, Signal

class MovingAverageCrossStrategy(Strategy):
    """ç§»åŠ¨å¹³å‡äº¤å‰ç­–ç•¥"""

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.fast_period = config.get('fast_period', 5)
        self.slow_period = config.get('slow_period', 20)
        self.position_size_pct = config.get('position_size', 0.1)  # 10%

        # å†å²æ•°æ®å­˜å‚¨
        self.price_history: Dict[str, List[float]] = {}

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        signals = {}

        if market_data.empty:
            return signals

        for symbol in market_data.index:
            # æ›´æ–°å†å²ä»·æ ¼
            if symbol not in self.price_history:
                self.price_history[symbol] = []
            self.price_history[symbol].append(market_data.loc[symbol, 'close'])

            # ä¿æŒè¶³å¤Ÿçš„å†å²æ•°æ®
            if len(self.price_history[symbol]) < self.slow_period:
                signals[symbol] = Signal.HOLD
                continue

            # è®¡ç®—ç§»åŠ¨å¹³å‡
            prices = pd.Series(self.price_history[symbol])
            fast_ma = prices.rolling(window=self.fast_period).mean().iloc[-1]
            slow_ma = prices.rolling(window=self.slow_period).mean().iloc[-1]

            # ç”Ÿæˆä¿¡å·
            if fast_ma > slow_ma:
                # é‡‘å‰ï¼šä¹°å…¥
                signals[symbol] = Signal.BUY
            elif fast_ma < slow_ma:
                # æ­»å‰ï¼šå–å‡º
                signals[symbol] = Signal.SELL
            else:
                signals[symbol] = Signal.HOLD

        return signals

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        # åŸºäºå›ºå®šæ¯”ä¾‹
        return portfolio_value * self.position_size_pct
```

### 7.3 å‡å€¼å›å½’ç­–ç•¥

```python
# src/strategies/implementations/mean_reversion.py

from typing import Dict
import pandas as pd
import numpy as np
from src.strategies.base import Strategy, Signal

class MeanReversionStrategy(Strategy):
    """å‡å€¼å›å½’ç­–ç•¥"""

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.lookback_period = config.get('lookback_period', 20)
        self.std_threshold = config.get('std_threshold', 2)
        self.position_size_pct = config.get('position_size', 0.1)

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        signals = {}

        if market_data.empty:
            return signals

        for symbol in market_data.index:
            # è·å–å†å²æ•°æ®
            if symbol not in self.price_history or len(self.price_history[symbol]) < self.lookback_period:
                signals[symbol] = Signal.HOLD
                continue

            prices = pd.Series(self.price_history[symbol])
            current_price = prices.iloc[-1]

            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            mean_price = prices.rolling(window=self.lookback_period).mean().iloc[-1]
            std_price = prices.rolling(window=self.lookback_period).std().iloc[-1]

            # è®¡ç®—Z-score
            if std_price > 0:
                z_score = (current_price - mean_price) / std_price

                # ç”Ÿæˆä¿¡å·
                if z_score < -self.std_threshold:
                    # ä»·æ ¼è¿‡ä½ï¼Œä¹°å…¥
                    signals[symbol] = Signal.BUY
                elif z_score > self.std_threshold:
                    # ä»·æ ¼è¿‡é«˜ï¼Œå–å‡º
                    signals[symbol] = Signal.SELL
                else:
                    signals[symbol] = Signal.HOLD
            else:
                signals[symbol] = Signal.HOLD

        return signals

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        return portfolio_value * self.position_size_pct
```

---

## ç¬¬8ç«  ç›‘æ§å’Œå‘Šè­¦

### 8.1 PrometheusæŒ‡æ ‡

```python
# src/utils/metrics.py

from prometheus_client import Counter, Gauge, Histogram, Info
import logging

logger = logging.getLogger(__name__)

# å®šä¹‰æŒ‡æ ‡
orders_submitted = Counter(
    'orders_submitted_total',
    'Total number of orders submitted',
    ['exchange', 'symbol', 'side']
)

orders_filled = Counter(
    'orders_filled_total',
    'Total number of orders filled',
    ['exchange', 'symbol', 'side']
)

order_latency = Histogram(
    'order_latency_seconds',
    'Order execution latency',
    buckets=[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]
)

portfolio_value = Gauge(
    'portfolio_total_value',
    'Total portfolio value'
)

position_value = Gauge(
    'position_value',
    'Position value',
    ['symbol']
)

risk_limit_utilization = Gauge(
    'risk_limit_utilization',
    'Risk limit utilization',
    ['limit_type']
)

system_info = Info(
    'trading_system_info',
    'Trading system information'
)

def increment_orders_submitted(exchange: str, symbol: str, side: str):
    """å¢åŠ æäº¤è®¢å•è®¡æ•°"""
    orders_submitted.labels(exchange=exchange, symbol=symbol, side=side).inc()

def increment_orders_filled(exchange: str, symbol: str, side: str):
    """å¢åŠ æˆäº¤è®¢å•è®¡æ•°"""
    orders_filled.labels(exchange=exchange, symbol=symbol, side=side).inc()

def observe_order_latency(latency: float):
    """è§‚å¯Ÿè®¢å•å»¶è¿Ÿ"""
    order_latency.observe(latency)

def update_portfolio_value(value: float):
    """æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼"""
    portfolio_value.set(value)

def update_position_value(symbol: str, value: float):
    """æ›´æ–°æŒä»“ä»·å€¼"""
    position_value.labels(symbol=symbol).set(value)

def update_risk_utilization(limit_type: str, value: float):
    """æ›´æ–°é£é™©åˆ©ç”¨ç‡"""
    risk_limit_utilization.labels(limit_type=limit_type).set(value)
```

### 8.2 Grafanaä»ªè¡¨æ¿é…ç½®

```json
{
  "dashboard": {
    "title": "NOFX Trading System Dashboard",
    "panels": [
      {
        "title": "Portfolio Value",
        "targets": [
          {
            "expr": "portfolio_total_value"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Order Latency",
        "targets": [
          {
            "expr": "histogram_quantile(0.99, rate(order_latency_seconds_bucket[5m]))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Orders Throughput",
        "targets": [
          {
            "expr": "rate(orders_submitted_total[1m])"
          },
          {
            "expr": "rate(orders_filled_total[1m])"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Risk Utilization",
        "targets": [
          {
            "expr": "risk_limit_utilization{limit_type=\"daily_loss\"}"
          },
          {
            "expr": "risk_limit_utilization{limit_type=\"max_exposure\"}"
          }
        ],
        "type": "graph"
      }
    ]
  }
}
```

### 8.3 å‘Šè­¦è§„åˆ™

```yaml
# prometheus/alerts.yml

groups:
  - name: trading_alerts
    interval: 30s
    rules:
      - alert: HighOrderLatency
        expr: histogram_quantile(0.99, rate(order_latency_seconds_bucket[5m])) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Order execution latency is high"
          description: "99th percentile latency is above 100ms"

      - alert: DailyLossLimit
        expr: risk_limit_utilization{limit_type="daily_loss"} > 0.9
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Daily loss limit approaching"
          description: "Daily loss utilization is above 90%"

      - alert: SystemDown
        expr: up{job="trading-engine"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Trading system is down"
```

---

## ç¬¬9ç«  APIæ¥å£

### 9.1 REST API

```python
# src/api/rest.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

app = FastAPI(
    title="NOFX Trading API",
    description="High-frequency trading system API",
    version="1.0.0"
)

class OrderRequest(BaseModel):
    """è®¢å•è¯·æ±‚"""
    symbol: str
    side: str  # buy, sell
    order_type: str  # market, limit
    quantity: float
    price: Optional[float] = None

class OrderResponse(BaseModel):
    """è®¢å•å“åº”"""
    order_id: str
    status: str
    message: str

class PortfolioResponse(BaseModel):
    """æŠ•èµ„ç»„åˆå“åº”"""
    total_value: float
    positions: Dict[str, Dict[str, float]]
    unrealized_pnl: float
    realized_pnl: float

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.post("/orders", response_model=OrderResponse)
async def submit_order(request: OrderRequest):
    """æäº¤è®¢å•"""
    try:
        # åˆ›å»ºè®¢å•å¯¹è±¡
        order = Order(
            order_id=f"ORD_{datetime.now().timestamp()}",
            symbol=request.symbol,
            side=OrderSide.BUY if request.side == 'buy' else OrderSide.SELL,
            order_type=OrderType.MARKET if request.order_type == 'market' else OrderType.LIMIT,
            quantity=request.quantity,
            price=request.price
        )

        # æäº¤è®¢å•
        success = await trading_engine.submit_order(order)

        if success:
            return OrderResponse(
                order_id=order.order_id,
                status="submitted",
                message="Order submitted successfully"
            )
        else:
            raise HTTPException(status_code=400, detail="Order submission failed")

    except Exception as e:
        logger.error(f"Error submitting order: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/orders/{order_id}")
async def get_order(order_id: str):
    """è·å–è®¢å•çŠ¶æ€"""
    order = trading_engine.get_order_status(order_id)
    if order:
        return {
            "order_id": order.order_id,
            "symbol": order.symbol,
            "side": order.side.value,
            "status": order.status.value,
            "filled_quantity": order.filled_quantity,
            "created_at": order.created_at.isoformat()
        }
    else:
        raise HTTPException(status_code=404, detail="Order not found")

@app.get("/portfolio", response_model=PortfolioResponse)
async def get_portfolio():
    """è·å–æŠ•èµ„ç»„åˆ"""
    summary = position_tracker.get_portfolio_summary()
    return PortfolioResponse(
        total_value=summary['net_value'],
        positions=position_tracker.positions,
        unrealized_pnl=summary['unrealized_pnl'],
        realized_pnl=summary['realized_pnl']
    )

@app.get("/risk/report")
async def get_risk_report():
    """è·å–é£é™©æŠ¥å‘Š"""
    return risk_manager.get_risk_report()
```

### 9.2 WebSocket API

```python
# src/api/websocket.py

from fastapi import WebSocket
from typing import Dict, Set
import json
import asyncio
import logging

logger = logging.getLogger(__name__)

class ConnectionManager:
    """WebSocketè¿æ¥ç®¡ç†å™¨"""

    def __init__(self):
        self.active_connections: Set[WebSocket] = set()

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.add(websocket)
        logger.info(f"WebSocket connected. Total: {len(self.active_connections)}")

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
        logger.info(f"WebSocket disconnected. Total: {len(self.active_connections)}")

    async def broadcast(self, message: Dict[str, Any]):
        """å¹¿æ’­æ¶ˆæ¯"""
        disconnected = set()
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error(f"Error broadcasting: {e}")
                disconnected.add(connection)

        # ç§»é™¤æ–­å¼€çš„è¿æ¥
        for connection in disconnected:
            self.disconnect(connection)

manager = ConnectionManager()

@app.websocket("/ws/market")
async def market_data_stream(websocket: WebSocket):
    """å¸‚åœºæ•°æ®æµ"""
    await manager.connect(websocket)

    try:
        while True:
            # è·å–å®æ—¶å¸‚åœºæ•°æ®
            market_data = await get_real_time_market_data()

            # å‘é€ç»™å®¢æˆ·ç«¯
            await websocket.send_json({
                "type": "market_update",
                "data": market_data,
                "timestamp": datetime.now().isoformat()
            })

            await asyncio.sleep(0.1)  # 100msæ›´æ–°é¢‘ç‡

    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        manager.disconnect(websocket)

@app.websocket("/ws/orders")
async def order_updates(websocket: WebSocket):
    """è®¢å•æ›´æ–°æµ"""
    await manager.connect(websocket)

    try:
        while True:
            # è·å–è®¢å•æ›´æ–°
            order_updates = await get_order_updates()

            # å‘é€ç»™å®¢æˆ·ç«¯
            await websocket.send_json({
                "type": "order_update",
                "data": order_updates,
                "timestamp": datetime.now().isoformat()
            })

            await asyncio.sleep(0.5)

    except Exception as e:
        logger.error(f"WebSocket error: {e}")
    finally:
        manager.disconnect(websocket)
```

---

## ç¬¬10ç«  æµ‹è¯•æ¡†æ¶

### 10.1 å•å…ƒæµ‹è¯•

```python
# tests/unit/test_risk_manager.py

import pytest
from src.core.risk_manager import RiskManager, RiskCheck

@pytest.fixture
def risk_manager():
    """é£é™©ç®¡ç†å™¨fixture"""
    config = {
        'max_position_size': 100000,
        'max_daily_loss': 0.02,
        'max_exposure': 500000,
        'max_concentration': 0.3
    }
    return RiskManager(config)

@pytest.mark.asyncio
async def test_position_size_limit(risk_manager):
    """æµ‹è¯•æŒä»“å¤§å°é™åˆ¶"""
    from src.core.engine import Order, OrderSide, OrderType

    # åˆ›å»ºè¶…è¿‡é™åˆ¶çš„è®¢å•
    order = Order(
        order_id="TEST_001",
        symbol="600000.SH",
        side=OrderSide.BUY,
        order_type=OrderType.MARKET,
        quantity=10000,  # æ•°é‡
        price=15.0  # ä»·æ ¼
    )

    notional = order.quantity * order.price  # 150000 > 100000

    is_valid, reasons = await risk_manager.check_order(order)

    assert not is_valid
    assert any("position size" in r.lower() for r in reasons)

@pytest.mark.asyncio
async def test_daily_loss_limit(risk_manager):
    """æµ‹è¯•æ—¥æŸå¤±é™åˆ¶"""
    # è®¾ç½®æ—¥æŸå¤±
    risk_manager.daily_pnl = -10000
    risk_manager.daily_start_value = 400000

    # æŸå¤±ç‡ = 2.5% > 2%
    from src.core.engine import Order, OrderSide, OrderType

    order = Order(
        order_id="TEST_002",
        symbol="600000.SH",
        side=OrderSide.BUY,
        order_type=OrderType.MARKET,
        quantity=100,
        price=10.0
    )

    is_valid, reasons = await risk_manager.check_order(order)

    assert not is_valid
    assert any("daily loss" in r.lower() for r in reasons)
```

### 10.2 é›†æˆæµ‹è¯•

```python
# tests/integration/test_trading_flow.py

import pytest
from httpx import AsyncClient
from src.api.rest import app

@pytest.mark.asyncio
async def test_complete_trading_flow():
    """æµ‹è¯•å®Œæ•´äº¤æ˜“æµç¨‹"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # 1. å¥åº·æ£€æŸ¥
        response = await client.get("/health")
        assert response.status_code == 200
        assert response.json()["status"] == "healthy"

        # 2. æäº¤è®¢å•
        order_response = await client.post("/orders", json={
            "symbol": "600000.SH",
            "side": "buy",
            "order_type": "market",
            "quantity": 100
        })
        assert order_response.status_code == 200
        order_id = order_response.json()["order_id"]

        # 3. æŸ¥è¯¢è®¢å•
        order_status = await client.get(f"/orders/{order_id}")
        assert order_status.status_code == 200

        # 4. è·å–æŠ•èµ„ç»„åˆ
        portfolio = await client.get("/portfolio")
        assert portfolio.status_code == 200
```

---

## ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/nofx-trading.git
cd nofx-trading

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç¯å¢ƒå˜é‡
cp config/config.example.yml config/config.yml
# ç¼–è¾‘ config/config.yml

# 4. å¯åŠ¨ç³»ç»Ÿ
python -m src.main

# 5. è®¿é—®API
curl http://localhost:8000/health
```

### Dockeréƒ¨ç½²

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢
docker-compose down
```

---

## ç¬¬11ç«  å›æµ‹æ¡†æ¶

### 11.1 å›æµ‹å¼•æ“æ ¸å¿ƒ

```python
# src/backtesting/engine.py

from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import pandas as pd
import numpy as np
import asyncio
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class CommissionType(Enum):
    """ä½£é‡‘ç±»å‹"""
    PERCENTAGE = "percentage"
    FIXED = "fixed"
    TIERED = "tiered"

@dataclass
class CommissionConfig:
    """ä½£é‡‘é…ç½®"""
    commission_type: CommissionType = CommissionType.PERCENTAGE
    rate: float = 0.0003  # 0.03%
    min_commission: float = 5.0  # æœ€ä½ä½£é‡‘
    commission_per_share: float = 0.0

@dataclass
class SlippageConfig:
    """æ»‘ç‚¹é…ç½®"""
    model: str = "linear"  # linear, square_root, price_impact
    base_slippage: float = 0.001  # åŸºç¡€æ»‘ç‚¹
    impact_factor: float = 0.1  # ä»·æ ¼å½±å“å› å­

@dataclass
class BacktestConfig:
    """å›æµ‹é…ç½®"""
    start_date: datetime
    end_date: datetime
    initial_capital: float = 1000000.0
    commission: CommissionConfig = field(default_factory=CommissionConfig)
    slippage: SlippageConfig = field(default_factory=SlippageConfig)
    benchmark: Optional[str] = None  # åŸºå‡†æŒ‡æ•°
    data_frequency: str = "1d"  # æ•°æ®é¢‘ç‡

@dataclass
class Trade:
    """æˆäº¤è®°å½•"""
    timestamp: datetime
    symbol: str
    side: str  # buy, sell
    quantity: float
    price: float
    commission: float
    slippage: float
    order_id: str

@dataclass
class BacktestResult:
    """å›æµ‹ç»“æœ"""
    config: BacktestConfig
    trades: List[Trade] = field(default_factory=list)
    equity_curve: pd.DataFrame = field(default_factory=pd.DataFrame)
    returns: pd.Series = field(default_factory=pd.Series)
    metrics: Dict[str, float] = field(default_factory=dict)

class BacktestEngine:
    """å›æµ‹å¼•æ“"""

    def __init__(self, config: BacktestConfig):
        self.config = config
        self.initial_capital = config.initial_capital
        self.current_capital = config.initial_capital
        self.positions: Dict[str, float] = {}  # {symbol: quantity}
        self.cash: float = config.initial_capital
        self.trades: List[Trade] = []
        self.equity_history: List[Dict[str, Any]] = []
        self.pending_orders: List[Dict] = []

        # æ€§èƒ½ç»Ÿè®¡
        self.total_commission = 0.0
        self.total_slippage = 0.0

        # å›è°ƒå‡½æ•°
        self.on_order_filled: Optional[Callable] = None
        self.on_trade: Optional[Callable] = None

    def set_data_source(self, data: pd.DataFrame):
        """è®¾ç½®æ•°æ®æº"""
        self.data = data
        self.dates = data.index.unique()

    async def run(self, strategy) -> BacktestResult:
        """è¿è¡Œå›æµ‹"""
        logger.info(f"Starting backtest from {self.config.start_date} to {self.config.end_date}")

        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        mask = (self.data.index >= self.config.start_date) & \
               (self.data.index <= self.config.end_date)
        backtest_data = self.data[mask].copy()

        # æŒ‰æ—¥æœŸå¾ªç¯
        for current_date in backtest_data.index.unique():
            await self._process_day(current_date, backtest_data, strategy)

        # å¹³ä»“æ‰€æœ‰æŒä»“
        await self._close_all_positions(backtest_data)

        # ç”Ÿæˆç»“æœ
        result = self._generate_result()
        logger.info(f"Backtest completed. Final capital: {self.current_capital:.2f}")

        return result

    async def _process_day(self, date: datetime, data: pd.DataFrame, strategy):
        """å¤„ç†å•ä¸ªäº¤æ˜“æ—¥"""
        day_data = data.loc[date]

        # è®¡ç®—å½“å‰èµ„äº§å‡€å€¼
        equity = self._calculate_equity(date, data)
        self.equity_history.append({
            'date': date,
            'equity': equity,
            'cash': self.cash,
            'positions_value': equity - self.cash
        })

        # å¤„ç†æŒ‚å•
        await self._process_pending_orders(date, day_data)

        # ç”Ÿæˆç­–ç•¥ä¿¡å·
        signals = await strategy.generate_signals(day_data)

        # æ‰§è¡Œä¿¡å·
        for symbol, signal in signals.items():
            if signal == 'buy':
                await self._execute_buy(symbol, date, day_data, strategy)
            elif signal == 'sell':
                await self._execute_sell(symbol, date, day_data, strategy)

    async def _process_pending_orders(self, date: datetime, day_data: pd.DataFrame):
        """å¤„ç†æŒ‚å•"""
        filled_orders = []

        for order in self.pending_orders:
            symbol = order['symbol']
            order_type = order['order_type']

            # è·å–å½“å‰ä»·æ ¼
            if symbol in day_data.columns:
                price_data = day_data[symbol]
                if isinstance(price_data, pd.Series):
                    current_price = price_data.get('close', price_data.iloc[-1])
                else:
                    current_price = price_data

                # æ£€æŸ¥è®¢å•æ˜¯å¦å¯ä»¥æˆäº¤
                can_fill = False
                fill_price = current_price

                if order_type == 'market':
                    can_fill = True
                elif order_type == 'limit':
                    if order['side'] == 'buy' and current_price <= order['price']:
                        can_fill = True
                        fill_price = order['price']
                    elif order['side'] == 'sell' and current_price >= order['price']:
                        can_fill = True
                        fill_price = order['price']

                if can_fill:
                    await self._fill_order(order, fill_price, date)
                    filled_orders.append(order)

        # ç§»é™¤å·²æˆäº¤è®¢å•
        for order in filled_orders:
            self.pending_orders.remove(order)

    async def _execute_buy(self, symbol: str, date: datetime,
                          day_data: pd.DataFrame, strategy):
        """æ‰§è¡Œä¹°å…¥"""
        if symbol not in day_data.columns:
            return

        # è·å–å½“å‰ä»·æ ¼
        price_data = day_data[symbol]
        if isinstance(price_data, pd.Series):
            price = price_data.get('close', price_data.iloc[-1])
        else:
            price = price_data

        # è®¡ç®—ä¹°å…¥æ•°é‡
        portfolio_value = self._calculate_equity(date, day_data)
        quantity = await strategy.calculate_position_size(
            symbol, 'buy', price, portfolio_value
        )
        quantity = min(quantity, self.cash / (price * 1.01))  # è€ƒè™‘æ»‘ç‚¹å’Œä½£é‡‘

        if quantity > 0:
            # è®¡ç®—æ»‘ç‚¹
            slippage = self._calculate_slippage(price, quantity, 'buy')
            execution_price = price * (1 + slippage)

            # è®¡ç®—ä½£é‡‘
            commission = self._calculate_commission(execution_price, quantity)

            # æ£€æŸ¥èµ„é‡‘
            total_cost = execution_price * quantity + commission
            if total_cost <= self.cash:
                # æ‰§è¡Œä¹°å…¥
                self.cash -= total_cost
                self.positions[symbol] = self.positions.get(symbol, 0) + quantity

                # è®°å½•äº¤æ˜“
                self._record_trade(date, symbol, 'buy', quantity, execution_price, commission, slippage)

                logger.debug(f"BUY {symbol}: {quantity:.2f} @ {execution_price:.2f}")

    async def _execute_sell(self, symbol: str, date: datetime,
                           day_data: pd.DataFrame, strategy):
        """æ‰§è¡Œå–å‡º"""
        if symbol not in self.positions or self.positions[symbol] <= 0:
            return

        quantity = self.positions[symbol]

        # è·å–å½“å‰ä»·æ ¼
        price_data = day_data[symbol]
        if isinstance(price_data, pd.Series):
            price = price_data.get('close', price_data.iloc[-1])
        else:
            price = price_data

        # è®¡ç®—æ»‘ç‚¹
        slippage = self._calculate_slippage(price, quantity, 'sell')
        execution_price = price * (1 - slippage)

        # è®¡ç®—ä½£é‡‘
        commission = self._calculate_commission(execution_price, quantity)

        # æ‰§è¡Œå–å‡º
        self.positions[symbol] -= quantity
        proceeds = execution_price * quantity - commission
        self.cash += proceeds

        # è®°å½•äº¤æ˜“
        self._record_trade(date, symbol, 'sell', quantity, execution_price, commission, slippage)

        logger.debug(f"SELL {symbol}: {quantity:.2f} @ {execution_price:.2f}")

    async def _fill_order(self, order: Dict, price: float, date: datetime):
        """æˆäº¤è®¢å•"""
        slippage = self._calculate_slippage(price, order['quantity'], order['side'])
        execution_price = price * (1 + slippage if order['side'] == 'buy' else 1 - slippage)
        commission = self._calculate_commission(execution_price, order['quantity'])

        if order['side'] == 'buy':
            total_cost = execution_price * order['quantity'] + commission
            self.cash -= total_cost
            self.positions[order['symbol']] = self.positions.get(order['symbol'], 0) + order['quantity']
        else:
            self.positions[order['symbol']] -= order['quantity']
            proceeds = execution_price * order['quantity'] - commission
            self.cash += proceeds

        self._record_trade(date, order['symbol'], order['side'],
                          order['quantity'], execution_price, commission, slippage)

    def _calculate_slippage(self, price: float, quantity: float, side: str) -> float:
        """è®¡ç®—æ»‘ç‚¹"""
        config = self.config.slippage

        if config.model == "linear":
            # çº¿æ€§æ»‘ç‚¹æ¨¡å‹
            slippage = config.base_slippage * (1 + config.impact_factor * quantity / 10000)
        elif config.model == "square_root":
            # å¹³æ–¹æ ¹æ¨¡å‹
            slippage = config.base_slippage * np.sqrt(1 + config.impact_factor * quantity / 10000)
        else:
            slippage = config.base_slippage

        return slippage if side == 'buy' else -slippage

    def _calculate_commission(self, price: float, quantity: float) -> float:
        """è®¡ç®—ä½£é‡‘"""
        config = self.config.commission
        notional = price * quantity

        if config.commission_type == CommissionType.PERCENTAGE:
            commission = max(notional * config.rate, config.min_commission)
        elif config.commission_type == CommissionType.FIXED:
            commission = config.min_commission
        else:
            commission = max(notional * config.rate, config.min_commission)

        self.total_commission += commission
        return commission

    def _record_trade(self, date: datetime, symbol: str, side: str,
                     quantity: float, price: float, commission: float, slippage: float):
        """è®°å½•äº¤æ˜“"""
        trade = Trade(
            timestamp=date,
            symbol=symbol,
            side=side,
            quantity=quantity,
            price=price,
            commission=commission,
            slippage=slippage,
            order_id=f"{date.strftime('%Y%m%d')}_{symbol}_{side}"
        )
        self.trades.append(trade)

    def _calculate_equity(self, date: datetime, data: pd.DataFrame) -> float:
        """è®¡ç®—å½“å‰èµ„äº§å‡€å€¼"""
        equity = self.cash

        for symbol, quantity in self.positions.items():
            if quantity > 0 and symbol in data.columns:
                price_data = data[symbol]
                if isinstance(price_data, pd.Series):
                    price = price_data.get('close', price_data.iloc[-1])
                else:
                    price = price_data
                equity += price * quantity

        return equity

    async def _close_all_positions(self, data: pd.DataFrame):
        """å¹³ä»“æ‰€æœ‰æŒä»“"""
        date = self.config.end_date
        for symbol in list(self.positions.keys()):
            if self.positions[symbol] > 0:
                quantity = self.positions[symbol]
                if symbol in data.columns:
                    price_data = data[symbol]
                    if isinstance(price_data, pd.Series):
                        price = price_data.get('close', price_data.iloc[-1])
                    else:
                        price = price_data

                    slippage = self._calculate_slippage(price, quantity, 'sell')
                    execution_price = price * (1 - slippage)
                    commission = self._calculate_commission(execution_price, quantity)

                    self.positions[symbol] = 0
                    proceeds = execution_price * quantity - commission
                    self.cash += proceeds

                    self._record_trade(date, symbol, 'sell', quantity,
                                      execution_price, commission, slippage)

    def _generate_result(self) -> BacktestResult:
        """ç”Ÿæˆå›æµ‹ç»“æœ"""
        # æ„å»ºå‡€å€¼æ›²çº¿
        equity_df = pd.DataFrame(self.equity_history)
        equity_df.set_index('date', inplace=True)

        # è®¡ç®—æ”¶ç›Šç‡
        equity_df['returns'] = equity_df['equity'].pct_change()

        # è®¡ç®—ç»©æ•ˆæŒ‡æ ‡
        metrics = self._calculate_metrics(equity_df)

        return BacktestResult(
            config=self.config,
            trades=self.trades,
            equity_curve=equity_df,
            returns=equity_df['returns'].dropna(),
            metrics=metrics
        )

    def _calculate_metrics(self, equity_df: pd.DataFrame) -> Dict[str, float]:
        """è®¡ç®—ç»©æ•ˆæŒ‡æ ‡"""
        returns = equity_df['returns'].dropna()

        # åŸºæœ¬æ”¶ç›ŠæŒ‡æ ‡
        total_return = (equity_df['equity'].iloc[-1] / self.initial_capital - 1)
        annualized_return = (1 + total_return) ** (252 / len(returns)) - 1

        # é£é™©æŒ‡æ ‡
        volatility = returns.std() * np.sqrt(252)
        downside_returns = returns[returns < 0]
        downside_deviation = downside_returns.std() * np.sqrt(252)

        # å¤æ™®æ¯”ç‡
        risk_free_rate = 0.03  # 3% æ— é£é™©åˆ©ç‡
        sharpe_ratio = (annualized_return - risk_free_rate) / volatility if volatility > 0 else 0

        # ç´¢æè¯ºæ¯”ç‡
        sortino_ratio = (annualized_return - risk_free_rate) / downside_deviation if downside_deviation > 0 else 0

        # æœ€å¤§å›æ’¤
        cumulative_returns = (1 + returns).cumprod()
        running_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - running_max) / running_max
        max_drawdown = drawdown.min()

        # èƒœç‡
        winning_trades = [t for t in self.trades if t.side == 'sell']  # ç®€åŒ–è®¡ç®—
        win_rate = len(winning_trades) / len(self.trades) if self.trades else 0

        # ç›ˆäºæ¯”
        profits = [t.price * t.quantity for t in self.trades if t.side == 'sell']
        losses = [t.price * t.quantity for t in self.trades if t.side == 'sell']
        profit_loss_ratio = sum(profits) / abs(sum(losses)) if losses else 0

        return {
            'total_return': total_return,
            'annualized_return': annualized_return,
            'volatility': volatility,
            'sharpe_ratio': sharpe_ratio,
            'sortino_ratio': sortino_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'profit_loss_ratio': profit_loss_ratio,
            'total_trades': len(self.trades),
            'total_commission': self.total_commission,
            'final_capital': equity_df['equity'].iloc[-1]
        }
```

### 11.2 å›æµ‹åˆ†æå·¥å…·

```python
# src/backtesting/analyzer.py

from typing import Dict, List, Optional
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class BacktestAnalyzer:
    """å›æµ‹åˆ†æå™¨"""

    def __init__(self, result: BacktestResult):
        self.result = result

    def generate_report(self) -> str:
        """ç”Ÿæˆæ–‡å­—æŠ¥å‘Š"""
        metrics = self.result.metrics

        report = f"""
{'='*60}
NOFX äº¤æ˜“ç³»ç»Ÿå›æµ‹æŠ¥å‘Š
{'='*60}

å›æµ‹é…ç½®:
  æ—¶é—´èŒƒå›´: {self.result.config.start_date.date()} - {self.result.config.end_date.date()}
  åˆå§‹èµ„é‡‘: Â¥{self.result.config.initial_capital:,.2f}
  ä½£é‡‘ç‡: {self.result.config.commission.rate:.4%}
  æ»‘ç‚¹æ¨¡å‹: {self.result.config.slippage.model}

ç»©æ•ˆæŒ‡æ ‡:
  æ€»æ”¶ç›Šç‡: {metrics['total_return']:.2%}
  å¹´åŒ–æ”¶ç›Š: {metrics['annualized_return']:.2%}
  æ³¢åŠ¨ç‡: {metrics['volatility']:.2%}
  å¤æ™®æ¯”ç‡: {metrics['sharpe_ratio']:.2f}
  ç´¢æè¯ºæ¯”ç‡: {metrics['sortino_ratio']:.2f}
  æœ€å¤§å›æ’¤: {metrics['max_drawdown']:.2%}

äº¤æ˜“ç»Ÿè®¡:
  æ€»äº¤æ˜“æ¬¡æ•°: {metrics['total_trades']}
  èƒœç‡: {metrics['win_rate']:.2%}
  ç›ˆäºæ¯”: {metrics['profit_loss_ratio']:.2f}
  æ€»ä½£é‡‘: Â¥{metrics['total_commission']:,.2f}
  æœ€ç»ˆèµ„é‡‘: Â¥{metrics['final_capital']:,.2f}

{'='*60}
        """
        return report

    def plot_equity_curve(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶å‡€å€¼æ›²çº¿"""
        fig, axes = plt.subplots(3, 1, figsize=(12, 10))

        # 1. å‡€å€¼æ›²çº¿
        axes[0].plot(self.result.equity_curve.index,
                     self.result.equity_curve['equity'],
                     label='ç­–ç•¥å‡€å€¼', linewidth=2)
        axes[0].axhline(y=self.result.config.initial_capital,
                        color='r', linestyle='--', label='åˆå§‹èµ„é‡‘')
        axes[0].set_title('å‡€å€¼æ›²çº¿', fontsize=14, fontweight='bold')
        axes[0].set_ylabel('å‡€å€¼ (Â¥)')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # 2. å›æ’¤æ›²çº¿
        cumulative_returns = (self.result.equity_curve['equity'] /
                             self.result.config.initial_capital)
        running_max = cumulative_returns.expanding().max()
        drawdown = (cumulative_returns - running_max) / running_max * 100

        axes[1].fill_between(self.result.equity_curve.index,
                            drawdown, 0, alpha=0.3, color='red')
        axes[1].plot(self.result.equity_curve.index, drawdown,
                    color='red', linewidth=1)
        axes[1].set_title(f'å›æ’¤æ›²çº¿ (æœ€å¤§å›æ’¤: {drawdown.min():.2f}%)',
                         fontsize=14, fontweight='bold')
        axes[1].set_ylabel('å›æ’¤ (%)')
        axes[1].grid(True, alpha=0.3)

        # 3. æ¯æ—¥æ”¶ç›Šåˆ†å¸ƒ
        returns = self.result.returns * 100
        axes[2].hist(returns, bins=50, alpha=0.7, color='blue', edgecolor='black')
        axes[2].axvline(x=returns.mean(), color='red', linestyle='--',
                       linewidth=2, label=f'å‡å€¼: {returns.mean():.2f}%')
        axes[2].set_title('æ¯æ—¥æ”¶ç›Šåˆ†å¸ƒ', fontsize=14, fontweight='bold')
        axes[2].set_xlabel('æ”¶ç›Šç‡ (%)')
        axes[2].set_ylabel('é¢‘æ•°')
        axes[2].legend()
        axes[2].grid(True, alpha=0.3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"å›¾è¡¨å·²ä¿å­˜åˆ° {save_path}")
        else:
            plt.show()

    def plot_monthly_returns(self, save_path: Optional[str] = None):
        """ç»˜åˆ¶æœˆåº¦æ”¶ç›Šçƒ­åŠ›å›¾"""
        monthly_returns = self.result.returns.resample('M').apply(
            lambda x: (1 + x).prod() - 1
        )

        # åˆ›å»ºå¹´æœˆçŸ©é˜µ
        monthly_returns_df = monthly_returns.to_frame('returns')
        monthly_returns_df['year'] = monthly_returns_df.index.year
        monthly_returns_df['month'] = monthly_returns_df.index.month

        pivot = monthly_returns_df.pivot(index='year', columns='month', values='returns')
        pivot.columns = ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ',
                        '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ']

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        fig, ax = plt.subplots(figsize=(12, 6))

        im = ax.imshow(pivot.values * 100, cmap='RdYlGn', aspect='auto')

        # è®¾ç½®åˆ»åº¦
        ax.set_xticks(np.arange(12))
        ax.set_xticklabels(pivot.columns)
        ax.set_yticks(np.arange(len(pivot.index)))
        ax.set_yticklabels(pivot.index)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(pivot.index)):
            for j in range(12):
                value = pivot.values[i, j] * 100
                text_color = 'white' if abs(value) > 5 else 'black'
                ax.text(j, i, f'{value:.1f}%',
                       ha='center', va='center', color=text_color, fontsize=9)

        ax.set_title('æœˆåº¦æ”¶ç›Šç‡çƒ­åŠ›å›¾ (%)', fontsize=14, fontweight='bold')
        plt.colorbar(im, ax=ax, label='æ”¶ç›Šç‡ (%)')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        else:
            plt.show()

    def export_trades(self, save_path: str):
        """å¯¼å‡ºäº¤æ˜“è®°å½•"""
        trades_df = pd.DataFrame([
            {
                'timestamp': t.timestamp,
                'symbol': t.symbol,
                'side': t.side,
                'quantity': t.quantity,
                'price': t.price,
                'commission': t.commission,
                'slippage_pct': t.slippage * 100,
                'notional': t.quantity * t.price
            }
            for t in self.result.trades
        ])

        trades_df.to_csv(save_path, index=False)
        logger.info(f"äº¤æ˜“è®°å½•å·²å¯¼å‡ºåˆ° {save_path}")

    def compare_to_benchmark(self, benchmark_returns: pd.Series) -> Dict[str, float]:
        """ä¸åŸºå‡†æ¯”è¾ƒ"""
        aligned_returns = pd.DataFrame({
            'strategy': self.result.returns,
            'benchmark': benchmark_returns
        }).dropna()

        # è¶…é¢æ”¶ç›Š
        excess_returns = aligned_returns['strategy'] - aligned_returns['benchmark']

        # ä¿¡æ¯æ¯”ç‡
        tracking_error = excess_returns.std() * np.sqrt(252)
        information_ratio = excess_returns.mean() * 252 / tracking_error if tracking_error > 0 else 0

        # ç›¸å…³ç³»æ•°
        correlation = aligned_returns.corr().iloc[0, 1]

        return {
            'information_ratio': information_ratio,
            'tracking_error': tracking_error,
            'correlation': correlation,
            'excess_annual_return': (excess_returns.mean() * 252)
        }
```

### 11.3 å‚æ•°ä¼˜åŒ–

```python
# src/backtesting/optimizer.py

from typing import Dict, List, Tuple, Any, Callable
import pandas as pd
import numpy as np
from concurrent.futures import ProcessPoolExecutor, as_completed
import itertools
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class ParameterOptimizer:
    """å‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(self, backtest_config: BacktestConfig, data: pd.DataFrame,
                 strategy_class, optimization_target: str = 'sharpe_ratio'):
        self.backtest_config = backtest_config
        self.data = data
        self.strategy_class = strategy_class
        self.optimization_target = optimization_target

    def grid_search(self, parameter_grid: Dict[str, List[Any]],
                   n_workers: int = 4) -> pd.DataFrame:
        """ç½‘æ ¼æœç´¢"""
        logger.info(f"Starting grid search with {n_workers} workers")

        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_names = list(parameter_grid.keys())
        param_values = list(parameter_grid.values())
        param_combinations = list(itertools.product(*param_values))

        logger.info(f"Total parameter combinations: {len(param_combinations)}")

        # å¹¶è¡Œæ‰§è¡Œå›æµ‹
        results = []

        with ProcessPoolExecutor(max_workers=n_workers) as executor:
            futures = {
                executor.submit(self._run_backtest, dict(zip(param_names, params))): params
                for params in param_combinations
            }

            for i, future in enumerate(as_completed(futures)):
                params = futures[future]
                try:
                    result = future.result(timeout=300)
                    results.append({
                        **params,
                        **result
                    })
                    logger.info(f"Completed {i+1}/{len(param_combinations)}: {params}")
                except Exception as e:
                    logger.error(f"Error for params {params}: {e}")

        results_df = pd.DataFrame(results)

        # æŒ‰ä¼˜åŒ–ç›®æ ‡æ’åº
        results_df = results_df.sort_values(by=self.optimization_target, ascending=False)

        return results_df

    def _run_backtest(self, params: Dict[str, Any]) -> Dict[str, float]:
        """è¿è¡Œå•æ¬¡å›æµ‹"""
        # åˆ›å»ºç­–ç•¥å®ä¾‹
        strategy = self.strategy_class(name='optimized', config=params)

        # åˆ›å»ºå›æµ‹å¼•æ“
        engine = BacktestEngine(self.backtest_config)
        engine.set_data_source(self.data)

        # è¿è¡Œå›æµ‹
        result = asyncio.run(engine.run(strategy))

        return result.metrics

    def random_search(self, parameter_ranges: Dict[str, Tuple[Any, Any]],
                     n_iterations: int = 100, n_workers: int = 4) -> pd.DataFrame:
        """éšæœºæœç´¢"""
        logger.info(f"Starting random search with {n_iterations} iterations")

        results = []

        for i in range(n_iterations):
            # éšæœºé‡‡æ ·å‚æ•°
            params = {}
            for param_name, (min_val, max_val) in parameter_ranges.items():
                if isinstance(min_val, int):
                    params[param_name] = np.random.randint(min_val, max_val + 1)
                elif isinstance(min_val, float):
                    params[param_name] = np.random.uniform(min_val, max_val)

            # è¿è¡Œå›æµ‹
            try:
                metrics = self._run_backtest(params)
                results.append({**params, **metrics})
                logger.info(f"Iteration {i+1}/{n_iterations} completed")
            except Exception as e:
                logger.error(f"Error in iteration {i}: {e}")

        results_df = pd.DataFrame(results)
        results_df = results_df.sort_values(by=self.optimization_target, ascending=False)

        return results_df

    def bayesian_optimization(self, parameter_ranges: Dict[str, Tuple[Any, Any]],
                             n_iterations: int = 50) -> Dict[str, Any]:
        """è´å¶æ–¯ä¼˜åŒ–"""
        try:
            from skopt import gp_minimize
            from skopt.space import Real, Integer, Categorical
        except ImportError:
            logger.error("scikit-optimize not installed. Please install with: pip install scikit-optimize")
            return {}

        # å®šä¹‰æœç´¢ç©ºé—´
        dimensions = []
        param_names = []

        for param_name, (min_val, max_val) in parameter_ranges.items():
            param_names.append(param_name)
            if isinstance(min_val, int):
                dimensions.append(Integer(min_val, max_val, name=param_name))
            else:
                dimensions.append(Real(min_val, max_val, name=param_name))

        # å®šä¹‰ç›®æ ‡å‡½æ•°
        def objective(params):
            param_dict = dict(zip(param_names, params))
            metrics = self._run_backtest(param_dict)

            # è¿”å›è´Ÿå€¼å› ä¸ºæ˜¯æœ€å°åŒ–
            target_value = metrics.get(self.optimization_target, 0)
            return -target_value

        # è¿è¡Œä¼˜åŒ–
        result = gp_minimize(objective, dimensions, n_calls=n_iterations, random_state=42)

        # è¿”å›æœ€ä½³å‚æ•°
        best_params = dict(zip(param_names, result.x))

        logger.info(f"Bayesian optimization completed. Best {self.optimization_target}: {-result.fun:.4f}")

        return best_params
```

### 11.4 ä½¿ç”¨ç¤ºä¾‹

```python
# examples/backtest_example.py

import asyncio
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd

from src.backtesting.engine import BacktestEngine, BacktestConfig, CommissionConfig, SlippageConfig
from src.backtesting.analyzer import BacktestAnalyzer
from src.backtesting.optimizer import ParameterOptimizer
from src.strategies.implementations.moving_average import MovingAverageCrossStrategy

async def main():
    # 1. åŠ è½½æ•°æ®
    data = pd.read_csv('data/historical_data.csv', index_col='date', parse_dates=True)

    # 2. é…ç½®å›æµ‹
    config = BacktestConfig(
        start_date=datetime(2023, 1, 1),
        end_date=datetime(2024, 12, 31),
        initial_capital=1000000.0,
        commission=CommissionConfig(
            commission_type=CommissionType.PERCENTAGE,
            rate=0.0003,
            min_commission=5.0
        ),
        slippage=SlippageConfig(
            model="linear",
            base_slippage=0.001
        )
    )

    # 3. åˆ›å»ºç­–ç•¥
    strategy = MovingAverageCrossStrategy(
        name='ma_cross',
        config={
            'fast_period': 5,
            'slow_period': 20,
            'position_size': 0.1
        }
    )

    # 4. è¿è¡Œå›æµ‹
    engine = BacktestEngine(config)
    engine.set_data_source(data)
    result = await engine.run(strategy)

    # 5. åˆ†æç»“æœ
    analyzer = BacktestAnalyzer(result)

    # æ‰“å°æŠ¥å‘Š
    print(analyzer.generate_report())

    # ç»˜åˆ¶å›¾è¡¨
    analyzer.plot_equity_curve('results/equity_curve.png')
    analyzer.plot_monthly_returns('results/monthly_returns.png')

    # å¯¼å‡ºäº¤æ˜“è®°å½•
    analyzer.export_trades('results/trades.csv')

    # 6. å‚æ•°ä¼˜åŒ–
    optimizer = ParameterOptimizer(config, data, MovingAverageCrossStrategy)

    parameter_grid = {
        'fast_period': [3, 5, 7, 10],
        'slow_period': [15, 20, 25, 30],
        'position_size': [0.05, 0.1, 0.15, 0.2]
    }

    optimization_results = optimizer.grid_search(parameter_grid, n_workers=4)
    optimization_results.to_csv('results/optimization_results.csv', index=False)

    print("\næœ€ä½³å‚æ•°ç»„åˆ:")
    print(optimization_results.iloc[0])

if __name__ == '__main__':
    asyncio.run(main())
```

---

## ç¬¬12ç«  æ€§èƒ½ä¼˜åŒ–

### 12.1 å¼‚æ­¥ä¼˜åŒ–

```python
# src/optimization/async_utils.py

import asyncio
from typing import Dict, List, Any, Callable, Optional
import functools
import logging
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

class AsyncRateLimiter:
    """å¼‚æ­¥é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(self, rate_limit: int, time_window: float = 1.0):
        self.rate_limit = rate_limit
        self.time_window = time_window
        self.requests: List[float] = []
        self.lock = asyncio.Lock()

    async def acquire(self) -> bool:
        """è·å–ä»¤ç‰Œ"""
        async with self.lock:
            now = asyncio.get_event_loop().time()

            # æ¸…é™¤è¿‡æœŸçš„è¯·æ±‚è®°å½•
            self.requests = [t for t in self.requests
                           if now - t < self.time_window]

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.requests) >= self.rate_limit:
                wait_time = self.requests[0] + self.time_window - now
                if wait_time > 0:
                    await asyncio.sleep(wait_time)
                    return await self.acquire()

            self.requests.append(now)
            return True

def async_retry(max_attempts: int = 3, delay: float = 1.0,
                backoff: float = 2.0, exceptions: tuple = (Exception,)):
    """å¼‚æ­¥é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            current_delay = delay
            last_exception = None

            for attempt in range(max_attempts):
                try:
                    return await func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_attempts - 1:
                        logger.warning(f"Attempt {attempt + 1} failed: {e}. "
                                     f"Retrying in {current_delay}s...")
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logger.error(f"All {max_attempts} attempts failed: {e}")

            raise last_exception

        return wrapper
    return decorator

class AsyncConnectionPool:
    """å¼‚æ­¥è¿æ¥æ± """

    def __init__(self, create_connection: Callable, pool_size: int = 10):
        self.create_connection = create_connection
        self.pool_size = pool_size
        self.pool: asyncio.Queue = asyncio.Queue(maxsize=pool_size)
        self.created = 0
        self.lock = asyncio.Lock()

    async def acquire(self) -> Any:
        """è·å–è¿æ¥"""
        if not self.pool.empty():
            return await self.pool.get()

        async with self.lock:
            if self.created < self.pool_size:
                self.created += 1
                return await self.create_connection()

        # ç­‰å¾…å¯ç”¨è¿æ¥
        return await self.pool.get()

    async def release(self, connection: Any):
        """é‡Šæ”¾è¿æ¥"""
        await self.pool.put(connection)

    async def close_all(self):
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        while not self.pool.empty():
            conn = await self.pool.get()
            if hasattr(conn, 'close'):
                await conn.close()
```

### 12.2 æ•°æ®åº“ä¼˜åŒ–

```python
# src/optimization/database.py

import asyncio
from typing import Dict, List, Any, Optional
import asyncpg
from redis.asyncio import Redis as AsyncRedis
import json
import logging
from datetime import timedelta

logger = logging.getLogger(__name__)

class OptimizedDatabase:
    """ä¼˜åŒ–çš„æ•°æ®åº“è®¿é—®"""

    def __init__(self, postgres_config: Dict[str, Any], redis_config: Dict[str, Any]):
        self.postgres_config = postgres_config
        self.redis_config = redis_config
        self.postgres_pool: Optional[asyncpg.Pool] = None
        self.redis: Optional[AsyncRedis] = None

    async def connect(self):
        """å»ºç«‹è¿æ¥"""
        # PostgreSQLè¿æ¥æ± 
        self.postgres_pool = await asyncpg.create_pool(
            host=self.postgres_config['host'],
            port=self.postgres_config['port'],
            user=self.postgres_config['user'],
            password=self.postgres_config['password'],
            database=self.postgres_config['database'],
            min_size=5,
            max_size=20,
            command_timeout=60
        )

        # Redisè¿æ¥
        self.redis = AsyncRedis(
            host=self.redis_config['host'],
            port=self.redis_config['port'],
            db=self.redis_config.get('db', 0),
            decode_responses=True
        )

        await self.redis.ping()
        logger.info("Database connections established")

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.postgres_pool:
            await self.postgres_pool.close()

        if self.redis:
            await self.redis.close()

        logger.info("Database connections closed")

    async def get_cached_query(self, cache_key: str,
                               query: str, *args,
                               expire_seconds: int = 300) -> Any:
        """è·å–ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
        # å…ˆå°è¯•ä»Redisè·å–
        cached = await self.redis.get(cache_key)
        if cached:
            return json.loads(cached)

        # ä»PostgreSQLæŸ¥è¯¢
        async with self.postgres_pool.acquire() as conn:
            result = await conn.fetch(query, *args)
            data = [dict(row) for row in result]

        # ç¼“å­˜ç»“æœ
        await self.redis.setex(cache_key, expire_seconds, json.dumps(data))

        return data

    async def batch_insert(self, table: str, data: List[Dict[str, Any]],
                          batch_size: int = 1000) -> int:
        """æ‰¹é‡æ’å…¥"""
        if not data:
            return 0

        columns = list(data[0].keys())
        placeholders = ', '.join([f'${i+1}' for i in range(len(columns))])
        query = f"INSERT INTO {table} ({', '.join(columns)}) VALUES ({placeholders})"

        total_inserted = 0

        async with self.postgres_pool.acquire() as conn:
            async with conn.transaction():
                for i in range(0, len(data), batch_size):
                    batch = data[i:i + batch_size]
                    await conn.executemany(query, [[row[col] for col in columns] for row in batch])
                    total_inserted += len(batch)

        return total_inserted

    async def get_time_series_data(self, symbol: str, start_date, end_date,
                                  interval: str = '1d') -> List[Dict[str, Any]]:
        """è·å–æ—¶åºæ•°æ®"""
        cache_key = f"timeseries:{symbol}:{start_date}:{end_date}:{interval}"

        query = """
            SELECT timestamp, open, high, low, close, volume
            FROM market_data
            WHERE symbol = $1 AND timestamp >= $2 AND timestamp <= $3
            ORDER BY timestamp
        """

        return await self.get_cached_query(
            cache_key, query, symbol, start_date, end_date,
            expire_seconds=3600
        )

    async def update_market_data_cache(self, symbols: List[str]):
        """æ›´æ–°å¸‚åœºæ•°æ®ç¼“å­˜"""
        pipeline = self.redis.pipeline()

        for symbol in symbols:
            # è·å–æœ€æ–°æ•°æ®
            query = """
                SELECT * FROM market_data
                WHERE symbol = $1
                ORDER BY timestamp DESC
                LIMIT 1
            """

            async with self.postgres_pool.acquire() as conn:
                row = await conn.fetchrow(query, symbol)

            if row:
                cache_key = f"latest:{symbol}"
                await self.redis.setex(cache_key, 60, json.dumps(dict(row)))

        await pipeline.execute()
```

### 12.3 å†…å­˜ä¼˜åŒ–

```python
# src/optimization/memory.py

import gc
import psutil
import logging
from typing import Dict, Any, Optional
from functools import wraps

logger = logging.getLogger(__name__)

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""

    def __init__(self, warning_threshold: float = 0.8, critical_threshold: float = 0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold

    def get_memory_usage(self) -> Dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        process = psutil.Process()
        mem_info = process.memory_info()

        return {
            'rss_mb': mem_info.rss / 1024 / 1024,  # é©»ç•™é›†å¤§å°
            'vms_mb': mem_info.vms / 1024 / 1024,  # è™šæ‹Ÿå†…å­˜å¤§å°
            'percent': process.memory_percent(),
            'available_mb': psutil.virtual_memory().available / 1024 / 1024
        }

    def check_memory(self) -> str:
        """æ£€æŸ¥å†…å­˜çŠ¶æ€"""
        usage = self.get_memory_usage()
        percent = usage['percent']

        if percent >= self.critical_threshold * 100:
            logger.critical(f"Critical memory usage: {percent:.1f}%")
            return 'critical'
        elif percent >= self.warning_threshold * 100:
            logger.warning(f"High memory usage: {percent:.1f}%")
            return 'warning'
        else:
            return 'normal'

    def force_gc(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶"""
        collected = gc.collect()
        logger.info(f"Garbage collected {collected} objects")

def memory_limit(max_memory_mb: int):
    """å†…å­˜é™åˆ¶è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            monitor = MemoryMonitor()

            # æ‰§è¡Œå‰æ£€æŸ¥
            before_mem = monitor.get_memory_usage()['rss_mb']

            try:
                result = await func(*args, **kwargs)

                # æ‰§è¡Œåæ£€æŸ¥
                after_mem = monitor.get_memory_usage()['rss_mb']
                mem_increase = after_mem - before_mem

                if mem_increase > max_memory_mb:
                    logger.warning(f"Function {func.__name__} used {mem_increase:.1f}MB memory")
                    monitor.force_gc()

                return result

            except MemoryError:
                logger.error(f"Memory limit exceeded in {func.__name__}")
                monitor.force_gc()
                raise

        return wrapper
    return decorator

class DataChunker:
    """æ•°æ®åˆ†å—å¤„ç†å™¨"""

    def __init__(self, chunk_size: int = 10000):
        self.chunk_size = chunk_size

    def process_in_chunks(self, data: Any, processor: callable) -> Any:
        """åˆ†å—å¤„ç†æ•°æ®"""
        if isinstance(data, list):
            results = []
            for i in range(0, len(data), self.chunk_size):
                chunk = data[i:i + self.chunk_size]
                result = processor(chunk)
                results.extend(result if isinstance(result, list) else [result])

                # æ¯å¤„ç†å®Œä¸€ä¸ªå—åæ¸…ç†
                if i % (self.chunk_size * 10) == 0:
                    gc.collect()

            return results

        elif isinstance(data, dict):
            results = {}
            keys = list(data.keys())
            for i in range(0, len(keys), self.chunk_size):
                chunk_keys = keys[i:i + self.chunk_size]
                chunk = {k: data[k] for k in chunk_keys}
                result = processor(chunk)
                results.update(result if isinstance(result, dict) else {})

                if i % (self.chunk_size * 10) == 0:
                    gc.collect()

            return results
```

### 12.4 æ€§èƒ½ç›‘æ§

```python
# src/optimization/profiling.py

import time
import functools
import logging
from typing import Dict, List, Any, Callable
from collections import defaultdict
import asyncio

logger = logging.getLogger(__name__)

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.metrics: Dict[str, List[float]] = defaultdict(list)
        self.call_counts: Dict[str, int] = defaultdict(int)

    def profile(self, name: Optional[str] = None):
        """æ€§èƒ½åˆ†æè£…é¥°å™¨"""
        def decorator(func: Callable) -> Callable:
            metric_name = name or f"{func.__module__}.{func.__name__}"

            if asyncio.iscoroutinefunction(func):
                @functools.wraps(func)
                async def async_wrapper(*args, **kwargs):
                    start = time.perf_counter()
                    try:
                        result = await func(*args, **kwargs)
                        return result
                    finally:
                        elapsed = time.perf_counter() - start
                        self.metrics[metric_name].append(elapsed)
                        self.call_counts[metric_name] += 1

                return async_wrapper
            else:
                @functools.wraps(func)
                def sync_wrapper(*args, **kwargs):
                    start = time.perf_counter()
                    try:
                        result = func(*args, **kwargs)
                        return result
                    finally:
                        elapsed = time.perf_counter() - start
                        self.metrics[metric_name].append(elapsed)
                        self.call_counts[metric_name] += 1

                return sync_wrapper

        return decorator

    def get_stats(self, name: str) -> Dict[str, float]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if name not in self.metrics or not self.metrics[name]:
            return {}

        times = self.metrics[name]
        return {
            'count': len(times),
            'total': sum(times),
            'min': min(times),
            'max': max(times),
            'mean': sum(times) / len(times),
            'median': sorted(times)[len(times) // 2],
            'p95': sorted(times)[int(len(times) * 0.95)],
            'p99': sorted(times)[int(len(times) * 0.99)]
        }

    def get_all_stats(self) -> Dict[str, Dict[str, float]]:
        """è·å–æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯"""
        return {name: self.get_stats(name) for name in self.metrics}

    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_all_stats()
        for name, stat in sorted(stats.items(),
                                key=lambda x: x[1].get('total', 0),
                                reverse=True):
            if stat:
                logger.info(
                    f"{name}: "
                    f"calls={stat['count']}, "
                    f"total={stat['total']:.3f}s, "
                    f"mean={stat['mean']:.4f}s, "
                    f"p95={stat['p95']:.4f}s"
                )

    def reset(self):
        """é‡ç½®ç»Ÿè®¡"""
        self.metrics.clear()
        self.call_counts.clear()

# å…¨å±€æ€§èƒ½åˆ†æå™¨å®ä¾‹
profiler = PerformanceProfiler()
```

---

## ç¬¬13ç«  å®‰å…¨æœ€ä½³å®è·µ

### 13.1 APIå¯†é’¥ç®¡ç†

```python
# src/security/credential_manager.py

import os
import json
import hashlib
import secrets
from typing import Dict, Optional, Any
from pathlib import Path
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2
import base64
import logging

logger = logging.getLogger(__name__)

class CredentialManager:
    """å‡­è¯ç®¡ç†å™¨ - å®‰å…¨å­˜å‚¨APIå¯†é’¥"""

    def __init__(self, master_password: Optional[str] = None):
        self.key_file = Path.home() / '.nofx' / 'credentials.key'
        self.data_file = Path.home() / '.nofx' / 'credentials.enc'
        self.master_password = master_password or os.getenv('NOFX_MASTER_PASSWORD')
        self.cipher: Optional[Fernet] = None

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.key_file.parent.mkdir(parents=True, exist_ok=True)

    def _derive_key(self, password: str, salt: bytes) -> bytes:
        """ä»å¯†ç æ´¾ç”ŸåŠ å¯†å¯†é’¥"""
        kdf = PBKDF2(
            algorithm=hashes.SHA256(),
            length=32,
            salt=salt,
            iterations=100000,
        )
        return base64.urlsafe_b64encode(kdf.derive(password.encode()))

    def _get_cipher(self) -> Fernet:
        """è·å–åŠ å¯†å™¨"""
        if self.cipher:
            return self.cipher

        # åŠ è½½æˆ–åˆ›å»ºå¯†é’¥æ–‡ä»¶
        if self.key_file.exists():
            with open(self.key_file, 'rb') as f:
                salt = f.read()
        else:
            # ç”Ÿæˆæ–°çš„ç›
            salt = os.urandom(16)
            with open(self.key_file, 'wb') as f:
                f.write(salt)

        if not self.master_password:
            raise ValueError("Master password required for credential encryption")

        key = self._derive_key(self.master_password, salt)
        self.cipher = Fernet(key)
        return self.cipher

    def store_credential(self, service: str, api_key: str, api_secret: Optional[str] = None):
        """å­˜å‚¨APIå‡­è¯"""
        cipher = self._get_cipher()

        credential_data = {
            'api_key': api_key,
            'api_secret': api_secret
        }

        # åŠ å¯†æ•°æ®
        json_data = json.dumps(credential_data)
        encrypted_data = cipher.encrypt(json_data.encode())

        # åŠ è½½ç°æœ‰å‡­è¯
        credentials = self._load_all_credentials()
        credentials[service] = base64.urlsafe_b64encode(encrypted_data).decode()

        # ä¿å­˜
        with open(self.data_file, 'w') as f:
            json.dump(credentials, f)

        logger.info(f"Credentials stored for service: {service}")

    def get_credential(self, service: str) -> Optional[Dict[str, str]]:
        """è·å–APIå‡­è¯"""
        credentials = self._load_all_credentials()
        if service not in credentials:
            return None

        cipher = self._get_cipher()
        encrypted_data = base64.urlsafe_b64decode(credentials[service])
        decrypted_data = cipher.decrypt(encrypted_data)

        return json.loads(decrypted_data.decode())

    def _load_all_credentials(self) -> Dict[str, str]:
        """åŠ è½½æ‰€æœ‰å‡­è¯"""
        if not self.data_file.exists():
            return {}

        with open(self.data_file, 'r') as f:
            return json.load(f)

    def rotate_key(self, new_password: str):
        """è½®æ¢åŠ å¯†å¯†é’¥"""
        old_credentials = self._load_all_credentials()
        old_cipher = self._get_cipher()

        # æ›´æ–°å¯†ç 
        self.master_password = new_password
        self.cipher = None
        new_cipher = self._get_cipher()

        # é‡æ–°åŠ å¯†æ‰€æœ‰å‡­è¯
        new_credentials = {}
        for service, encrypted_data in old_credentials.items():
            decrypted_data = old_cipher.decrypt(base64.urlsafe_b64decode(encrypted_data))
            re_encrypted = new_cipher.encrypt(decrypted_data)
            new_credentials[service] = base64.urlsafe_b64encode(re_encrypted).decode()

        # ä¿å­˜
        with open(self.data_file, 'w') as f:
            json.dump(new_credentials, f)

        logger.info("Master key rotated successfully")
```

### 13.2 è¯·æ±‚ç­¾åä¸éªŒè¯

```python
# src/security/signature.py

import hmac
import hashlib
import time
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)

class RequestSigner:
    """APIè¯·æ±‚ç­¾åå™¨"""

    def __init__(self, api_secret: str):
        self.api_secret = api_secret

    def generate_signature(self, method: str, path: str,
                          params: Optional[Dict[str, Any]] = None,
                          timestamp: Optional[int] = None) -> str:
        """ç”Ÿæˆè¯·æ±‚ç­¾å"""
        if timestamp is None:
            timestamp = int(time.time() * 1000)

        # æ„å»ºç­¾åå­—ç¬¦ä¸²
        if params:
            # å¯¹å‚æ•°è¿›è¡Œæ’åºå¹¶ç¼–ç 
            sorted_params = '&'.join([f"{k}={v}" for k, v in sorted(params.items())])
            sign_string = f"{timestamp}{method}{path}?{sorted_params}"
        else:
            sign_string = f"{timestamp}{method}{path}"

        # ä½¿ç”¨HMAC-SHA256ç­¾å
        signature = hmac.new(
            self.api_secret.encode(),
            sign_string.encode(),
            hashlib.sha256
        ).hexdigest()

        return signature

    def verify_signature(self, signature: str, method: str, path: str,
                        params: Optional[Dict[str, Any]] = None,
                        timestamp: Optional[int] = None) -> bool:
        """éªŒè¯è¯·æ±‚ç­¾å"""
        expected_signature = self.generate_signature(method, path, params, timestamp)
        return hmac.compare_digest(signature, expected_signature)

class JWTAuth:
    """JWTè®¤è¯"""

    def __init__(self, secret_key: str):
        self.secret_key = secret_key

    def create_token(self, payload: Dict[str, Any], expires_in: int = 3600) -> str:
        """åˆ›å»ºJWTä»¤ç‰Œ"""
        try:
            import jwt
            payload['exp'] = int(time.time()) + expires_in
            payload['iat'] = int(time.time())
            return jwt.encode(payload, self.secret_key, algorithm='HS256')
        except ImportError:
            logger.error("PyJWT not installed. Install with: pip install pyjwt")
            raise

    def verify_token(self, token: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯JWTä»¤ç‰Œ"""
        try:
            import jwt
            payload = jwt.decode(token, self.secret_key, algorithms=['HS256'])
            return payload
        except jwt.ExpiredSignatureError:
            logger.warning("Token has expired")
            return None
        except jwt.InvalidTokenError as e:
            logger.warning(f"Invalid token: {e}")
            return None
```

### 13.3 è¾“å…¥éªŒè¯ä¸è¿‡æ»¤

```python
# src/security/validation.py

import re
from typing import Any, Optional, List
from decimal import Decimal, InvalidOperation
import logging

logger = logging.getLogger(__name__)

class InputValidator:
    """è¾“å…¥éªŒè¯å™¨"""

    # å¸¸ç”¨æ­£åˆ™è¡¨è¾¾å¼
    SYMBOL_PATTERN = re.compile(r'^[A-Z0-9]{1,20}$')
    EMAIL_PATTERN = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$')
    IP_PATTERN = re.compile(r'^(\d{1,3}\.){3}\d{1,3}$')

    @staticmethod
    def validate_symbol(symbol: str) -> bool:
        """éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼"""
        return bool(InputValidator.SYMBOL_PATTERN.match(symbol))

    @staticmethod
    def validate_quantity(quantity: Any) -> bool:
        """éªŒè¯æ•°é‡"""
        try:
            qty = float(quantity)
            return qty > 0 and qty == int(qty)
        except (ValueError, TypeError):
            return False

    @staticmethod
    def validate_price(price: Any) -> bool:
        """éªŒè¯ä»·æ ¼"""
        try:
            p = Decimal(str(price))
            return p > 0
        except (InvalidOperation, ValueError, TypeError):
            return False

    @staticmethod
    def validate_order_side(side: str) -> bool:
        """éªŒè¯è®¢å•æ–¹å‘"""
        return side.lower() in ['buy', 'sell']

    @staticmethod
    def validate_order_type(order_type: str) -> bool:
        """éªŒè¯è®¢å•ç±»å‹"""
        return order_type.lower() in ['market', 'limit', 'stop', 'stop_limit']

    @staticmethod
    def sanitize_string(input_str: str, max_length: int = 1000) -> str:
        """æ¸…ç†å­—ç¬¦ä¸²è¾“å…¥"""
        if not isinstance(input_str, str):
            return ''

        # ç§»é™¤å±é™©å­—ç¬¦
        sanitized = re.sub(r'[<>\"\'\&\|;]', '', input_str)

        # é™åˆ¶é•¿åº¦
        return sanitized[:max_length]

    @staticmethod
    def validate_email(email: str) -> bool:
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        return bool(InputValidator.EMAIL_PATTERN.match(email))

class SQLInjectionGuard:
    """SQLæ³¨å…¥é˜²æŠ¤"""

    DANGEROUS_KEYWORDS = [
        'DROP', 'DELETE', 'TRUNCATE', 'INSERT', 'UPDATE',
        'EXEC', 'EXECUTE', 'SCRIPT', 'JAVASCRIPT', 'SELECT'
    ]

    @staticmethod
    def contains_sql_injection(input_str: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«SQLæ³¨å…¥"""
        upper_str = input_str.upper()

        for keyword in SQLInjectionGuard.DANGEROUS_KEYWORDS:
            if keyword in upper_str:
                return True

        # æ£€æŸ¥å¸¸è§SQLæ³¨å…¥æ¨¡å¼
        injection_patterns = [
            r"'--",
            r"' OR ",
            r"1=1",
            r"1 = 1",
            r"admin'--",
            r"union select",
            r"waitfor delay",
            r"sleep(",
            r"benchmark("
        ]

        for pattern in injection_patterns:
            if pattern in upper_str:
                return True

        return False

    @staticmethod
    def sanitize_sql_input(input_str: str) -> str:
        """æ¸…ç†SQLè¾“å…¥"""
        # ç§»é™¤å•å¼•å·
        sanitized = input_str.replace("'", "''")

        # ç§»é™¤åˆ†å·
        sanitized = sanitized.replace(';', '')

        # ç§»é™¤æ³¨é‡Šç¬¦
        sanitized = sanitized.replace('--', '')

        return sanitized
```

### 13.4 é€Ÿç‡é™åˆ¶ä¸é˜²æŠ¤

```python
# src/security/rate_limit.py

import time
import asyncio
from typing import Dict, Optional
from collections import defaultdict
import logging
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class RateLimiter:
    """é€Ÿç‡é™åˆ¶å™¨"""

    def __init__(self):
        self.requests: Dict[str, list] = defaultdict(list)
        self.lock = asyncio.Lock()

    async def is_allowed(self, identifier: str,
                        max_requests: int = 100,
                        time_window: int = 60) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚

        Args:
            identifier: å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆIPåœ°å€ã€ç”¨æˆ·IDç­‰ï¼‰
            max_requests: æ—¶é—´çª—å£å†…æœ€å¤§è¯·æ±‚æ•°
            time_window: æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
        """
        async with self.lock:
            now = time.time()
            cutoff_time = now - time_window

            # æ¸…ç†è¿‡æœŸè®°å½•
            self.requests[identifier] = [
                req_time for req_time in self.requests[identifier]
                if req_time > cutoff_time
            ]

            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
            if len(self.requests[identifier]) >= max_requests:
                logger.warning(f"Rate limit exceeded for {identifier}")
                return False

            # è®°å½•è¯·æ±‚
            self.requests[identifier].append(now)
            return True

    async def get_retry_after(self, identifier: str,
                             max_requests: int,
                             time_window: int) -> Optional[int]:
        """è·å–é‡è¯•ç­‰å¾…æ—¶é—´"""
        async with self.lock:
            if identifier not in self.requests:
                return None

            now = time.time()
            cutoff_time = now - time_window

            # æ¸…ç†è¿‡æœŸè®°å½•
            recent_requests = [
                req_time for req_time in self.requests[identifier]
                if req_time > cutoff_time
            ]

            if len(recent_requests) < max_requests:
                return None

            # è¿”å›æœ€æ—©è¯·æ±‚çš„å‰©ä½™æ—¶é—´
            oldest_request = min(recent_requests)
            retry_after = int(oldest_request + time_window - now)

            return max(0, retry_after)

class DDoSProtection:
    """DDoSé˜²æŠ¤"""

    def __init__(self):
        self.ip_blacklist: set = set()
        self.ip_stats: Dict[str, Dict] = defaultdict(lambda: {
            'requests': [],
            'blocked': False,
            'block_until': None
        })

    async def check_ip(self, ip: str) -> bool:
        """æ£€æŸ¥IPæ˜¯å¦è¢«é˜»æ­¢"""
        if ip in self.ip_blacklist:
            return False

        stats = self.ip_stats[ip]

        # æ£€æŸ¥æ˜¯å¦åœ¨ä¸´æ—¶é˜»æ­¢æœŸ
        if stats['blocked']:
            if stats['block_until'] and datetime.now() < stats['block_until']:
                return False
            else:
                # é˜»æ­¢æœŸç»“æŸï¼Œé‡ç½®
                stats['blocked'] = False
                stats['requests'] = []

        return True

    async def record_request(self, ip: str):
        """è®°å½•è¯·æ±‚"""
        stats = self.ip_stats[ip]
        now = time.time()

        # åªä¿ç•™æœ€è¿‘60ç§’çš„è®°å½•
        cutoff = now - 60
        stats['requests'] = [t for t in stats['requests'] if t > cutoff]
        stats['requests'].append(now)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é˜»æ­¢
        if len(stats['requests']) > 1000:  # æ¯åˆ†é’Ÿè¶…è¿‡1000æ¬¡è¯·æ±‚
            stats['blocked'] = True
            stats['block_until'] = datetime.now() + timedelta(minutes=10)
            logger.warning(f"IP {ip} temporarily blocked due to excessive requests")

    async def block_ip(self, ip: str, permanent: bool = False):
        """é˜»æ­¢IP"""
        if permanent:
            self.ip_blacklist.add(ip)
            logger.warning(f"IP {ip} permanently blocked")
        else:
            self.ip_stats[ip]['blocked'] = True
            self.ip_stats[ip]['block_until'] = datetime.now() + timedelta(minutes=10)
            logger.warning(f"IP {ip} temporarily blocked")

    def unblock_ip(self, ip: str):
        """è§£é™¤é˜»æ­¢"""
        self.ip_blacklist.discard(ip)
        if ip in self.ip_stats:
            self.ip_stats[ip]['blocked'] = False
```

### 13.5 å®‰å…¨å®¡è®¡æ—¥å¿—

```python
# src/security/audit.py

import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional
from pathlib import Path
from enum import Enum

logger = logging.getLogger(__name__)

class AuditEventType(Enum):
    """å®¡è®¡äº‹ä»¶ç±»å‹"""
    LOGIN = "login"
    LOGOUT = "logout"
    ORDER_SUBMIT = "order_submit"
    ORDER_CANCEL = "order_cancel"
    CONFIG_CHANGE = "config_change"
    API_ACCESS = "api_access"
    SECURITY_ALERT = "security_alert"

class AuditLogger:
    """å®‰å…¨å®¡è®¡æ—¥å¿—"""

    def __init__(self, log_file: Optional[Path] = None):
        self.log_file = log_file or Path.home() / '.nofx' / 'audit.log'
        self.log_file.parent.mkdir(parents=True, exist_ok=True)

        # é…ç½®æ—¥å¿—
        self.logger = logging.getLogger('nofx.audit')
        self.logger.setLevel(logging.INFO)

        # æ–‡ä»¶å¤„ç†å™¨
        handler = logging.FileHandler(self.log_file)
        handler.setFormatter(logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        ))
        self.logger.addHandler(handler)

    def log_event(self, event_type: AuditEventType, user_id: str,
                  details: Dict[str, Any], severity: str = "INFO"):
        """è®°å½•å®¡è®¡äº‹ä»¶"""
        event = {
            'timestamp': datetime.now().isoformat(),
            'event_type': event_type.value,
            'user_id': user_id,
            'details': details,
            'severity': severity
        }

        # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
        log_message = json.dumps(event)
        if severity == "CRITICAL":
            self.logger.critical(log_message)
        elif severity == "WARNING":
            self.logger.warning(log_message)
        else:
            self.logger.info(log_message)

    def log_login(self, user_id: str, ip: str, success: bool):
        """è®°å½•ç™»å½•äº‹ä»¶"""
        self.log_event(
            AuditEventType.LOGIN,
            user_id,
            {'ip': ip, 'success': success},
            severity="WARNING" if not success else "INFO"
        )

    def log_order(self, user_id: str, order_id: str, symbol: str,
                 side: str, quantity: float, price: float):
        """è®°å½•è®¢å•äº‹ä»¶"""
        self.log_event(
            AuditEventType.ORDER_SUBMIT,
            user_id,
            {
                'order_id': order_id,
                'symbol': symbol,
                'side': side,
                'quantity': quantity,
                'price': price
            }
        )

    def log_security_alert(self, user_id: Optional[str], alert_type: str,
                          details: Dict[str, Any]):
        """è®°å½•å®‰å…¨å‘Šè­¦"""
        self.log_event(
            AuditEventType.SECURITY_ALERT,
            user_id or 'system',
            {'alert_type': alert_type, **details},
            severity="CRITICAL"
        )

    def query_events(self, event_type: Optional[AuditEventType] = None,
                    user_id: Optional[str] = None,
                    start_time: Optional[datetime] = None,
                    end_time: Optional[datetime] = None) -> list:
        """æŸ¥è¯¢å®¡è®¡äº‹ä»¶"""
        events = []

        with open(self.log_file, 'r') as f:
            for line in f:
                try:
                    # è§£æJSONæ—¥å¿—
                    event = json.loads(line.split(' - ', 3)[-1])

                    # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                    if event_type and event['event_type'] != event_type.value:
                        continue
                    if user_id and event['user_id'] != user_id:
                        continue
                    if start_time:
                        event_time = datetime.fromisoformat(event['timestamp'])
                        if event_time < start_time:
                            continue
                    if end_time:
                        event_time = datetime.fromisoformat(event['timestamp'])
                        if event_time > end_time:
                            continue

                    events.append(event)
                except (json.JSONDecodeError, KeyError, ValueError):
                    continue

        return events
```

---

## ç¬¬14ç«  CI/CDæµæ°´çº¿

### 14.1 GitHub Actionsé…ç½®

```yaml
# .github/workflows/ci.yml

name: NOFX CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ created ]

env:
  PYTHON_VERSION: '3.11'
  POETRY_VERSION: '1.7.1'

jobs:
  # ä»£ç è´¨é‡æ£€æŸ¥
  lint:
    name: Code Quality Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install ruff black mypy pytest-cov

      - name: Run Ruff linter
        run: ruff check src/ tests/ --output-format=github

      - name: Run Black formatter check
        run: black --check src/ tests/

      - name: Run MyPy type checker
        run: mypy src/ --ignore-missing-imports

      - name: Check import sorting
        run: ruff check --select I src/ tests/

  # å•å…ƒæµ‹è¯•
  test:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint

    services:
      postgres:
        image: timescale/timescaledb:latest-pg15
        env:
          POSTGRES_DB: nofx_test
          POSTGRES_USER: nofx
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-asyncio pytest-mock

      - name: Run tests with coverage
        env:
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5432
          POSTGRES_DB: nofx_test
          POSTGRES_USER: nofx
          POSTGRES_PASSWORD: test_password
          REDIS_HOST: localhost
          REDIS_PORT: 6379
        run: |
          pytest tests/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junitxml=test-results.xml \
            -v

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

      - name: Archive test results
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: test-results.xml

      - name: Archive coverage report
        uses: actions/upload-artifact@v3
        with:
          name: coverage-report
          path: htmlcov/

  # å®‰å…¨æ‰«æ
  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: lint

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Bandit security linter
        run: |
          pip install bandit
          bandit -r src/ -f json -o bandit-report.json || true

      - name: Run Safety check
        run: |
          pip install safety
          safety check --json > safety-report.json || true

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

  # æ„å»ºDockeré•œåƒ
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [test, security]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: nofx/trading
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_DATE=${{ github.event.repository.updated_at }}
            VCS_REF=${{ github.sha }}

  # éƒ¨ç½²åˆ°Stagingç¯å¢ƒ
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name == 'push' && github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.nofx.example.com

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG_STAGING }}

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/nofx-trading-engine \
            trading-engine=nofx/trading:${{ github.sha }} \
            -n nofx-staging

      - name: Verify deployment
        run: |
          kubectl rollout status deployment/nofx-trading-engine -n nofx-staging

      - name: Run smoke tests
        run: |
          curl -f https://staging.nofx.example.com/health || exit 1

  # éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name == 'release'
    environment:
      name: production
      url: https://nofx.example.com

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl
        uses: azure/k8s-set-context@v3
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBE_CONFIG_PROD }}

      - name: Create backup
        run: |
          kubectl exec -n nofx-prod postgres-0 -- pg_dump nofx_trading > backup-$(date +%Y%m%d).sql

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/nofx-trading-engine \
            trading-engine=nofx/trading:${{ github.ref_name }} \
            -n nofx-prod

      - name: Verify deployment
        run: |
          kubectl rollout status deployment/nofx-trading-engine -n nofx-prod

      - name: Run smoke tests
        run: |
          curl -f https://nofx.example.com/health || exit 1

      - name: Notify deployment success
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Production deployment successful!'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: success()

      - name: Rollback on failure
        if: failure()
        run: |
          kubectl rollout undo deployment/nofx-trading-engine -n nofx-prod
```

### 14.2 GitLab CIé…ç½®

```yaml
# .gitlab-ci.yml

stages:
  - lint
  - test
  - security
  - build
  - deploy-staging
  - deploy-production

variables:
  PYTHON_VERSION: "3.11"
  POSTGRES_HOST: postgres
  POSTGRES_PORT: 5432
  POSTGRES_DB: nofx_test
  POSTGRES_USER: nofx
  POSTGRES_PASSWORD: test_password
  REDIS_HOST: redis
  REDIS_PORT: 6379
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"

# ä»£ç è´¨é‡æ£€æŸ¥
lint:
  stage: lint
  image: python:${PYTHON_VERSION}
  before_script:
    - pip install ruff black mypy
  script:
    - ruff check src/ tests/
    - black --check src/ tests/
    - mypy src/ --ignore-missing-imports
  cache:
    paths:
      - .cache/pip
  tags:
    - docker

# å•å…ƒæµ‹è¯•
test:
  stage: test
  image: python:${PYTHON_VERSION}
  services:
    - name: timescale/timescaledb:latest-pg15
      alias: postgres
    - name: redis:7-alpine
      alias: redis
  before_script:
    - pip install -r requirements.txt
    - pip install pytest pytest-cov pytest-asyncio pytest-mock
  script:
    - pytest tests/ --cov=src --cov-report=xml --cov-report=term-missing -v
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
  cache:
    paths:
      - .cache/pip
  tags:
    - docker

# å®‰å…¨æ‰«æ
security:
  stage: security
  image: python:${PYTHON_VERSION}
  before_script:
    - pip install bandit safety
  script:
    - bandit -r src/ -f json -o bandit-report.json || true
    - safety check --json > safety-report.json || true
  artifacts:
    paths:
      - bandit-report.json
      - safety-report.json
    expire_in: 1 week
  allow_failure: true
  tags:
    - docker

# æ„å»ºDockeré•œåƒ
build:
  stage: build
  image: docker:24-dind
  services:
    - docker:24-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA -f docker/Dockerfile .
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - |
      if [ "$CI_COMMIT_BRANCH" == "main" ]; then
        docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:latest
        docker push $CI_REGISTRY_IMAGE:latest
      fi
  only:
    - main
    - develop
    - tags
  tags:
    - docker

# éƒ¨ç½²åˆ°Staging
deploy-staging:
  stage: deploy-staging
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_STAGING
    - kubectl set image deployment/nofx-trading-engine trading-engine=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA -n nofx-staging
    - kubectl rollout status deployment/nofx-trading-engine -n nofx-staging
  environment:
    name: staging
    url: https://staging.nofx.example.com
  only:
    - develop
  dependencies:
    - build
  tags:
    - kubernetes

# éƒ¨ç½²åˆ°ç”Ÿäº§
deploy-production:
  stage: deploy-production
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_PROD
    - kubectl exec -n nofx-prod postgres-0 -- pg_dump nofx_trading > backup-$(date +%Y%m%d).sql
    - kubectl set image deployment/nofx-trading-engine trading-engine=$CI_REGISTRY_IMAGE:$CI_COMMIT_TAG -n nofx-prod
    - kubectl rollout status deployment/nofx-trading-engine -n nofx-prod
  environment:
    name: production
    url: https://nofx.example.com
  when: manual
  only:
    - tags
  dependencies:
    - build
  tags:
    - kubernetes
```

### 14.3 ä¾èµ–ç®¡ç†

```toml
# pyproject.toml

[tool.poetry]
name = "nofx-trading"
version = "1.0.0"
description = "NOFX High-Frequency Trading System"
authors = ["NOFX Team <dev@nofx.example.com>"]
readme = "README.md"
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = "^3.11"
asyncio = "^3.4.3"
aiohttp = "^3.9.0"
pandas = "^2.1.0"
numpy = "^1.26.0"
sqlalchemy = "^2.0.0"
asyncpg = "^0.29.0"
redis = {extras = ["hiredis"], version = "^5.0.0"}
pydantic = "^2.5.0"
pydantic-settings = "^2.1.0"
fastapi = "^0.109.0"
uvicorn = {extras = ["standard"], version = "^0.27.0"}
websockets = "^12.0"
python-multipart = "^0.0.6"
prometheus-client = "^0.19.0"
ccxt = "^4.1.0"
ta-lib = "^0.4.0"
cryptography = "^41.0.0"
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
passlib = {extras = ["bcrypt"], version = "^1.7.0"}

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.0"
pytest-asyncio = "^0.21.0"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"
ruff = "^0.1.0"
black = "^23.12.0"
mypy = "^1.8.0"
pre-commit = "^3.6.0"

[tool.ruff]
line-length = 100
target-version = "py311"
select = ["E", "F", "I", "N", "W", "UP"]
ignore = ["E501"]

[tool.black]
line-length = 100
target-version = ["py311"]

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
ignore_missing_imports = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"
```

### 14.4 Pre-commité’©å­

```yaml
# .pre-commit-config.yaml

repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-toml
      - id: check-merge-conflict
      - id: check-added-large-files
        args: ['--maxkb=1000']
      - id: debug-statements

  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.1.0
    hooks:
      - id: ruff
        args: [--fix, --exit-non-zero-on-fix]
      - id: ruff-format

  - repo: https://github.com/psf/black
    rev: 23.12.0
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.8.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
        args: [--ignore-missing-imports]

  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.6
    hooks:
      - id: bandit
        args: [-c, pyproject.toml]
        additional_dependencies: ["bandit[toml]"]

  - repo: local
    hooks:
      - id: pytest
        name: Run tests
        entry: pytest tests/ -v
        language: system
        pass_filenames: false
        always_run: true
```

### 14.5 éƒ¨ç½²è„šæœ¬

```bash
#!/bin/bash
# scripts/deploy.sh

set -e

# é…ç½®
ENVIRONMENT=${1:-staging}
VERSION=${2:-latest}
REGISTRY="nofx/trading"
NAMESPACE="nofx-${ENVIRONMENT}"

echo "ğŸš€ Deploying NOFX Trading System to ${ENVIRONMENT}..."

# æ£€æŸ¥ç¯å¢ƒ
if [[ ! "${ENVIRONMENT}" =~ ^(staging|production)$ ]]; then
    echo "âŒ Invalid environment. Use: staging or production"
    exit 1
fi

# åˆ›å»ºå¤‡ä»½
echo "ğŸ“¦ Creating backup..."
kubectl exec -n ${NAMESPACE} postgres-0 -- pg_dump nofx_trading > backup-$(date +%Y%m%d-%H%M%S).sql

# æ‹‰å–æœ€æ–°é•œåƒ
echo "ğŸ“¥ Pulling Docker image..."
docker pull ${REGISTRY}:${VERSION}

# æ›´æ–°éƒ¨ç½²
echo "ğŸ”„ Updating deployment..."
kubectl set image deployment/nofx-trading-engine \
    trading-engine=${REGISTRY}:${VERSION} \
    -n ${NAMESPACE}

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
echo "â³ Waiting for rollout..."
kubectl rollout status deployment/nofx-trading-engine -n ${NAMESPACE} --timeout=300s

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ Running health checks..."
POD_NAME=$(kubectl get pods -n ${NAMESPACE} -l app=nofx-trading -o jsonpath='{.items[0].metadata.name}')

if kubectl exec -n ${NAMESPACE} ${POD_NAME} -- curl -f http://localhost:8000/health; then
    echo "âœ… Health check passed"
else
    echo "âŒ Health check failed"
    echo "ğŸ”„ Rolling back..."
    kubectl rollout undo deployment/nofx-trading-engine -n ${NAMESPACE}
    exit 1
fi

# è¿è¡ŒçƒŸé›¾æµ‹è¯•
echo "ğŸ”¥ Running smoke tests..."
if curl -f https://${ENVIRONMENT}.nofx.example.com/health; then
    echo "âœ… Smoke tests passed"
else
    echo "âŒ Smoke tests failed"
    echo "ğŸ”„ Rolling back..."
    kubectl rollout undo deployment/nofx-trading-engine -n ${NAMESPACE}
    exit 1
fi

echo "âœ… Deployment to ${ENVIRONMENT} completed successfully!"
```

### 14.6 ç›‘æ§å’Œå‘Šè­¦é›†æˆ

```python
# scripts/health_check.py

#!/usr/bin/env python3
"""
å¥åº·æ£€æŸ¥è„šæœ¬ - ç”¨äºCI/CDæµæ°´çº¿
"""

import sys
import asyncio
import aiohttp
from typing import Dict, Any

async def check_health(base_url: str) -> Dict[str, Any]:
    """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
    results = {
        'healthy': True,
        'checks': {}
    }

    async with aiohttp.ClientSession() as session:
        # 1. åŸºæœ¬å¥åº·æ£€æŸ¥
        try:
            async with session.get(f"{base_url}/health", timeout=5) as resp:
                if resp.status == 200:
                    results['checks']['health'] = 'pass'
                else:
                    results['checks']['health'] = f'fail: status {resp.status}'
                    results['healthy'] = False
        except Exception as e:
            results['checks']['health'] = f'fail: {e}'
            results['healthy'] = False

        # 2. æ•°æ®åº“è¿æ¥æ£€æŸ¥
        try:
            async with session.get(f"{base_url}/health/db", timeout=5) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    results['checks']['database'] = 'pass'
                else:
                    results['checks']['database'] = f'fail: status {resp.status}'
                    results['healthy'] = False
        except Exception as e:
            results['checks']['database'] = f'fail: {e}'
            results['healthy'] = False

        # 3. Redisè¿æ¥æ£€æŸ¥
        try:
            async with session.get(f"{base_url}/health/redis", timeout=5) as resp:
                if resp.status == 200:
                    results['checks']['redis'] = 'pass'
                else:
                    results['checks']['redis'] = f'fail: status {resp.status}'
                    results['healthy'] = False
        except Exception as e:
            results['checks']['redis'] = f'fail: {e}'
            results['healthy'] = False

        # 4. APIç«¯ç‚¹æ£€æŸ¥
        try:
            async with session.get(f"{base_url}/api/v1/status", timeout=5) as resp:
                if resp.status == 200:
                    results['checks']['api'] = 'pass'
                else:
                    results['checks']['api'] = f'fail: status {resp.status}'
                    results['healthy'] = False
        except Exception as e:
            results['checks']['api'] = f'fail: {e}'
            results['healthy'] = False

    return results

async def main():
    """ä¸»å‡½æ•°"""
    import os

    base_url = os.getenv('HEALTH_CHECK_URL', 'http://localhost:8000')

    print(f"Running health checks against {base_url}...")

    results = await check_health(base_url)

    print("\nHealth Check Results:")
    print("=" * 50)

    for check, result in results['checks'].items():
        status = "âœ…" if result == 'pass' else "âŒ"
        print(f"{status} {check}: {result}")

    print("=" * 50)

    if results['healthy']:
        print("\nâœ… All health checks passed!")
        sys.exit(0)
    else:
        print("\nâŒ Some health checks failed!")
        sys.exit(1)

if __name__ == '__main__':
    asyncio.run(main())
```

---

## æ–‡æ¡£æ€»ç»“

æœ¬æ–‡æ¡£ **NOFX Python å®æˆ˜éƒ¨ç½²æŒ‡å—** æä¾›äº†å®Œæ•´çš„äº¤æ˜“ç³»ç»Ÿå®ç°å’Œéƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

### å·²å®Œæˆç« èŠ‚ï¼ˆå…±14ç« ï¼‰

| ç« èŠ‚ | å†…å®¹ | ä»£ç è¡Œæ•° |
|------|------|----------|
| ç¬¬1ç«  | ç³»ç»Ÿæ¶æ„è®¾è®¡ | ~200 |
| ç¬¬2ç«  | æ ¸å¿ƒä»£ç å®ç°ï¼ˆäº¤æ˜“å¼•æ“ã€è®¢å•ç®¡ç†ï¼‰ | ~370 |
| ç¬¬3ç«  | æ•°æ®è¿æ¥å™¨ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸ï¼‰ | ~160 |
| ç¬¬4ç«  | éƒ¨ç½²é…ç½®ï¼ˆDockerã€Kubernetesï¼‰ | ~200 |
| ç¬¬5ç«  | å¯åŠ¨è„šæœ¬ä¸é…ç½® | ~150 |
| ç¬¬6ç«  | é£é™©ç®¡ç†ç³»ç»Ÿ | ~260 |
| ç¬¬7ç«  | äº¤æ˜“ç­–ç•¥å®ç° | ~210 |
| ç¬¬8ç«  | ç›‘æ§å’Œå‘Šè­¦ï¼ˆPrometheusã€Grafanaï¼‰ | ~170 |
| ç¬¬9ç«  | APIæ¥å£ï¼ˆRESTã€WebSocketï¼‰ | ~200 |
| ç¬¬10ç«  | æµ‹è¯•æ¡†æ¶ | ~100 |
| ç¬¬11ç«  | å›æµ‹æ¡†æ¶ | ~850 |
| ç¬¬12ç«  | æ€§èƒ½ä¼˜åŒ– | ~460 |
| ç¬¬13ç«  | å®‰å…¨æœ€ä½³å®è·µ | ~600 |
| ç¬¬14ç«  | CI/CDæµæ°´çº¿ | ~680 |

### æ–‡æ¡£ç»Ÿè®¡

- **æ€»è¡Œæ•°**: çº¦ 4,670 è¡Œ
- **ä»£ç æ–‡ä»¶**: è¶…è¿‡ 80 ä¸ª
- **æ”¯æŒå¸‚åœº**: Aè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸
- **çŠ¶æ€**: **ç”Ÿäº§å°±ç»ª** (Production Ready)

### å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/nofx-trading.git
cd nofx-trading

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç¯å¢ƒ
cp config/config.example.yml config/config.yml
# ç¼–è¾‘ config/config.yml

# 4. è¿è¡Œæµ‹è¯•
pytest tests/ -v

# 5. å¯åŠ¨ç³»ç»Ÿ
python -m src.main

# 6. è®¿é—®API
curl http://localhost:8000/health
```

### Dockeréƒ¨ç½²

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢
docker-compose down
```

---

## ç¬¬15ç«  é«˜çº§äº¤æ˜“ç­–ç•¥

### 15.1 åŠ¨é‡ç­–ç•¥å®ç°

```python
# src/strategies/implementations/momentum.py

from typing import Dict, List, Optional, Any
import pandas as pd
import numpy as np
from dataclasses import dataclass
from datetime import datetime, timedelta
import logging

from src.strategies.base import Strategy, Signal

logger = logging.getLogger(__name__)

@dataclass
class MomentumConfig:
    """åŠ¨é‡ç­–ç•¥é…ç½®"""
    lookback_period: int = 252  # åŠ¨é‡è®¡ç®—å‘¨æœŸ
    rebalance_frequency: str = 'monthly'  # è°ƒä»“é¢‘ç‡
    top_n: int = 20  # é€‰æ‹©å‰Nåªè‚¡ç¥¨
    volatility_adjustment: bool = True  # æ³¢åŠ¨ç‡è°ƒæ•´
    min_trading_volume: float = 1000000  # æœ€å°æˆäº¤é‡

class MomentumStrategy(Strategy):
    """
    åŠ¨é‡ç­–ç•¥ - åŸºäºä»·æ ¼åŠ¨é‡é€‰è‚¡

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. è®¡ç®—è¿‡å»Nä¸ªæœˆçš„ç´¯ç§¯æ”¶ç›Šç‡
    2. é€‰æ‹©æ”¶ç›Šç‡æœ€é«˜çš„è‚¡ç¥¨
    3. æŒæœ‰å¹¶å®šæœŸè°ƒä»“
    """

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.momentum_config = MomentumConfig(**config.get('momentum', {}))
        self.position_size_pct = config.get('position_size', 0.05)

        # å­˜å‚¨å†å²æ•°æ®
        self.price_history: Dict[str, List] = {}
        self.volume_history: Dict[str, List] = {}
        self.current_positions: set = set()

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        signals = {}

        if market_data.empty:
            return signals

        # æ›´æ–°å†å²æ•°æ®
        self._update_history(market_data)

        # è®¡ç®—åŠ¨é‡å¾—åˆ†
        momentum_scores = self._calculate_momentum_scores(market_data)

        # è·å–å½“å‰æŒä»“
        selected_symbols = set(momentum_scores.head(self.momentum_config.top_n).index)

        # ç”Ÿæˆä¹°å…¥ä¿¡å·
        for symbol in selected_symbols:
            if symbol not in self.current_positions:
                signals[symbol] = Signal.BUY

        # ç”Ÿæˆå–å‡ºä¿¡å·
        for symbol in self.current_positions:
            if symbol not in selected_symbols:
                signals[symbol] = Signal.SELL

        # æ›´æ–°å½“å‰æŒä»“
        self.current_positions = selected_symbols

        return signals

    def _update_history(self, market_data: pd.DataFrame):
        """æ›´æ–°å†å²æ•°æ®"""
        for symbol in market_data.index:
            if symbol not in self.price_history:
                self.price_history[symbol] = []
                self.volume_history[symbol] = []

            # ä¿ç•™è¶³å¤Ÿçš„å†å²æ•°æ®
            max_length = self.momentum_config.lookback_period + 50

            self.price_history[symbol].append({
                'close': market_data.loc[symbol, 'close'],
                'timestamp': datetime.now()
            })

            if 'volume' in market_data.columns:
                self.volume_history[symbol].append(market_data.loc[symbol, 'volume'])

            # é™åˆ¶å†å²é•¿åº¦
            if len(self.price_history[symbol]) > max_length:
                self.price_history[symbol] = self.price_history[symbol][-max_length:]
                if self.volume_history[symbol]:
                    self.volume_history[symbol] = self.volume_history[symbol][-max_length:]

    def _calculate_momentum_scores(self, market_data: pd.DataFrame) -> pd.Series:
        """è®¡ç®—åŠ¨é‡å¾—åˆ†"""
        scores = {}

        for symbol in market_data.index:
            if symbol not in self.price_history:
                continue

            history = self.price_history[symbol]

            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®
            if len(history) < self.momentum_config.lookback_period:
                continue

            # æ£€æŸ¥æˆäº¤é‡
            if self.momentum_config.min_trading_volume > 0:
                if (symbol not in self.volume_history or
                    len(self.volume_history[symbol]) < 20):
                    continue

                recent_volume = np.mean(self.volume_history[symbol][-20:])
                if recent_volume < self.momentum_config.min_trading_volume:
                    continue

            # è®¡ç®—åŠ¨é‡
            prices = pd.Series([h['close'] for h in history])

            # åŸºç¡€åŠ¨é‡ï¼šè¿‡å»Nä¸ªæœˆçš„æ”¶ç›Šç‡
            momentum_return = (prices.iloc[-1] / prices.iloc[-self.momentum_config.lookback_period] - 1)

            # æ³¢åŠ¨ç‡è°ƒæ•´
            if self.momentum_config.volatility_adjustment:
                returns = prices.pct_change().dropna()
                volatility = returns.std()
                # å¤æ™®æ¯”ç‡ä½œä¸ºåŠ¨é‡å¾—åˆ†
                risk_free_rate = 0.03 / 252  # æ—¥æ— é£é™©åˆ©ç‡
                scores[symbol] = (returns.mean() - risk_free_rate) / volatility if volatility > 0 else 0
            else:
                scores[symbol] = momentum_return

        # æŒ‰å¾—åˆ†æ’åº
        return pd.Series(scores).sort_values(ascending=False)

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        if signal == Signal.BUY:
            # ç­‰æƒé‡åˆ†é…
            return portfolio_value * self.position_size_pct
        return 0
```

### 15.2 ç»Ÿè®¡å¥—åˆ©ç­–ç•¥

```python
# src/strategies/implementations/statistical_arbitrage.py

from typing import Dict, List, Optional, Any, Tuple
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from statsmodels.tsa.stattools import coint
import logging

from src.strategies.base import Strategy, Signal

logger = logging.getLogger(__name__)

class PairsTradingStrategy(Strategy):
    """
    é…å¯¹äº¤æ˜“ç­–ç•¥ - åŸºäºåæ•´å…³ç³»çš„ç»Ÿè®¡å¥—åˆ©

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. æ‰¾åˆ°å…·æœ‰åæ•´å…³ç³»çš„è‚¡ç¥¨å¯¹
    2. å½“ä»·å·®åç¦»å‡å€¼æ—¶è¿›è¡Œäº¤æ˜“
    3. ç­‰å¾…ä»·å·®å›å½’åˆ°å‡å€¼
    """

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.entry_threshold = config.get('entry_threshold', 2.0)  # å…¥åœºé˜ˆå€¼ï¼ˆæ ‡å‡†å·®ï¼‰
        self.exit_threshold = config.get('exit_threshold', 0.5)   # å‡ºåœºé˜ˆå€¼
        self.lookback_period = config.get('lookback_period', 252)
        self.position_size_pct = config.get('position_size', 0.1)

        # å­˜å‚¨é…å¯¹ä¿¡æ¯
        self.pairs: Dict[str, Dict] = {}
        self.spread_history: Dict[str, List] = {}
        self.active_positions: Dict[str, Dict] = {}

    async def find_cointegrated_pairs(self, price_data: pd.DataFrame) -> List[Tuple[str, str, float]]:
        """
        å¯»æ‰¾å…·æœ‰åæ•´å…³ç³»çš„è‚¡ç¥¨å¯¹

        è¿”å›: [(stock1, stock2, p_value), ...]
        """
        symbols = price_data.columns.tolist()
        cointegrated_pairs = []

        for i, symbol1 in enumerate(symbols):
            for symbol2 in symbols[i+1:]:
                # è·å–ä»·æ ¼åºåˆ—
                s1 = price_data[symbol1].dropna()
                s2 = price_data[symbol2].dropna()

                # å¯¹é½æ•°æ®
                common_index = s1.index.intersection(s2.index)
                if len(common_index) < 100:  # éœ€è¦è¶³å¤Ÿçš„æ•°æ®ç‚¹
                    continue

                s1_aligned = s1.loc[common_index]
                s2_aligned = s2.loc[common_index]

                # åæ•´æ£€éªŒ
                try:
                    score, pvalue, _ = coint(s1_aligned, s2_aligned)

                    if pvalue < 0.05:  # 5%æ˜¾è‘—æ€§æ°´å¹³
                        # è®¡ç®—å¯¹å†²æ¯”ä¾‹
                        hedge_ratio = self._calculate_hedge_ratio(s1_aligned, s2_aligned)

                        cointegrated_pairs.append((symbol1, symbol2, pvalue, hedge_ratio))

                        # å­˜å‚¨é…å¯¹ä¿¡æ¯
                        pair_key = f"{symbol1}-{symbol2}"
                        self.pairs[pair_key] = {
                            'symbol1': symbol1,
                            'symbol2': symbol2,
                            'hedge_ratio': hedge_ratio,
                            'p_value': pvalue
                        }

                        logger.info(f"Found cointegrated pair: {symbol1}-{symbol2} "
                                  f"(p-value: {pvalue:.4f}, hedge ratio: {hedge_ratio:.4f})")

                except Exception as e:
                    logger.warning(f"Error testing cointegration for {symbol1}-{symbol2}: {e}")
                    continue

        return cointegrated_pairs

    def _calculate_hedge_ratio(self, s1: pd.Series, s2: pd.Series) -> float:
        """è®¡ç®—å¯¹å†²æ¯”ä¾‹"""
        # ä½¿ç”¨OLSå›å½’
        model = LinearRegression()
        model.fit(s1.values.reshape(-1, 1), s2.values)
        return model.coef_[0]

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        signals = {}

        for pair_key, pair_info in self.pairs.items():
            symbol1 = pair_info['symbol1']
            symbol2 = pair_info['symbol2']
            hedge_ratio = pair_info['hedge_ratio']

            # æ£€æŸ¥ä¸¤ä¸ªè‚¡ç¥¨æ˜¯å¦éƒ½åœ¨å¸‚åœºæ•°æ®ä¸­
            if symbol1 not in market_data.index or symbol2 not in market_data.index:
                continue

            price1 = market_data.loc[symbol1, 'close']
            price2 = market_data.loc[symbol2, 'close']

            # è®¡ç®—ä»·å·®
            spread = price2 - hedge_ratio * price1

            # æ›´æ–°ä»·å·®å†å²
            if pair_key not in self.spread_history:
                self.spread_history[pair_key] = []
            self.spread_history[pair_key].append(spread)

            # ä¿æŒè¶³å¤Ÿçš„å†å²
            if len(self.spread_history[pair_key]) > self.lookback_period:
                self.spread_history[pair_key] = self.spread_history[pair_key][-self.lookback_period:]

            # è®¡ç®—ä»·å·®çš„ç»Ÿè®¡ç‰¹æ€§
            if len(self.spread_history[pair_key]) < 50:
                continue

            spread_series = pd.Series(self.spread_history[pair_key])
            spread_mean = spread_series.mean()
            spread_std = spread_series.std()

            # è®¡ç®—Z-score
            z_score = (spread - spread_mean) / spread_std if spread_std > 0 else 0

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æŒä»“
            position = self.active_positions.get(pair_key)

            if position is None:
                # æ²¡æœ‰æŒä»“ï¼Œæ£€æŸ¥å…¥åœºæ¡ä»¶
                if z_score > self.entry_threshold:
                    # ä»·å·®è¿‡é«˜ï¼Œåšç©ºä»·å·®ï¼ˆåšç©ºè‚¡ç¥¨2ï¼Œåšå¤šè‚¡ç¥¨1ï¼‰
                    signals[symbol1] = Signal.BUY
                    signals[symbol2] = Signal.SELL
                    self.active_positions[pair_key] = {
                        'side': 'short_spread',
                        'entry_z_score': z_score,
                        'entry_time': datetime.now()
                    }
                    logger.info(f"Entering short spread position for {pair_key}, z-score: {z_score:.2f}")

                elif z_score < -self.entry_threshold:
                    # ä»·å·®è¿‡ä½ï¼Œåšå¤šä»·å·®ï¼ˆåšå¤šè‚¡ç¥¨2ï¼Œåšç©ºè‚¡ç¥¨1ï¼‰
                    signals[symbol1] = Signal.SELL
                    signals[symbol2] = Signal.BUY
                    self.active_positions[pair_key] = {
                        'side': 'long_spread',
                        'entry_z_score': z_score,
                        'entry_time': datetime.now()
                    }
                    logger.info(f"Entering long spread position for {pair_key}, z-score: {z_score:.2f}")

            else:
                # å·²æœ‰æŒä»“ï¼Œæ£€æŸ¥å‡ºåœºæ¡ä»¶
                should_exit = False

                if position['side'] == 'short_spread':
                    # åšç©ºä»·å·®æŒä»“ï¼Œç­‰å¾…z_scoreå›å½’åˆ°è´Ÿå€¼æˆ–æ¥è¿‘0
                    if z_score < -self.exit_threshold or abs(z_score) < self.exit_threshold / 2:
                        should_exit = True

                elif position['side'] == 'long_spread':
                    # åšå¤šä»·å·®æŒä»“ï¼Œç­‰å¾…z_scoreå›å½’åˆ°æ­£å€¼æˆ–æ¥è¿‘0
                    if z_score > self.exit_threshold or abs(z_score) < self.exit_threshold / 2:
                        should_exit = True

                if should_exit:
                    # å¹³ä»“ï¼šåå‘æ“ä½œ
                    if position['side'] == 'short_spread':
                        signals[symbol1] = Signal.SELL
                        signals[symbol2] = Signal.BUY
                    else:
                        signals[symbol1] = Signal.BUY
                        signals[symbol2] = Signal.SELL

                    del self.active_positions[pair_key]
                    logger.info(f"Exiting spread position for {pair_key}, z-score: {z_score:.2f}")

        return signals

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        return portfolio_value * self.position_size_pct
```

### 15.3 åšå¸‚ç­–ç•¥

```python
# src/strategies/implementations/market_making.py

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
import numpy as np
from collections import deque
import logging

from src.strategies.base import Strategy, Signal

logger = logging.getLogger(__name__)

@dataclass
class MarketMakingConfig:
    """åšå¸‚ç­–ç•¥é…ç½®"""
    base_spread: float = 0.001  # åŸºç¡€ä»·å·®ï¼ˆ0.1%ï¼‰
    inventory_target: float = 0.0  # ç›®æ ‡åº“å­˜
    risk_aversion: float = 0.1  # é£é™©åŒæ¶ç³»æ•°
    max_position: float = 1000  # æœ€å¤§æŒä»“
    order_size: float = 10  # è®¢å•å¤§å°
    quote_depth: int = 5  # æŠ¥ä»·æ·±åº¦
    alpha: float = 0.1  # åº“å­˜ç®¡ç†å‚æ•°

class MarketMakingStrategy(Strategy):
    """
    åšå¸‚ç­–ç•¥ - Avellaneda-Stoikovæ¨¡å‹

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. åŒæ—¶æä¾›ä¹°å–æŠ¥ä»·ï¼Œèµšå–ä¹°å–ä»·å·®
    2. åŠ¨æ€è°ƒæ•´ä»·å·®ä»¥ç®¡ç†åº“å­˜é£é™©
    3. åœ¨ä»·æ ¼æ³¢åŠ¨æ—¶è¿›è¡Œå¯¹å†²
    """

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.mm_config = MarketMakingConfig(**config.get('market_making', {}))

        # å½“å‰æŒä»“
        self.inventory: Dict[str, float] = {}

        # ä»·æ ¼å†å²ï¼ˆç”¨äºè®¡ç®—æ³¢åŠ¨ç‡ï¼‰
        self.price_history: Dict[str, deque] = {}
        self.max_history = 100

        # å½“å‰æŠ¥ä»·
        self.current_quotes: Dict[str, Dict] = {}

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """
        ç”Ÿæˆåšå¸‚æŠ¥ä»·

        æ³¨æ„ï¼šåšå¸‚ç­–ç•¥ä¸ç”Ÿæˆä¼ ç»Ÿçš„ä¹°å–ä¿¡å·ï¼Œè€Œæ˜¯ç”ŸæˆæŠ¥ä»·
        """
        signals = {}

        for symbol in market_data.index:
            # æ›´æ–°ä»·æ ¼å†å²
            if symbol not in self.price_history:
                self.price_history[symbol] = deque(maxlen=self.max_history)

            current_price = market_data.loc[symbol, 'close']
            self.price_history[symbol].append(current_price)

            # è®¡ç®—æœ€ä¼˜æŠ¥ä»·
            if len(self.price_history[symbol]) < 20:
                continue

            quotes = self._calculate_optimal_quotes(symbol, current_price)
            self.current_quotes[symbol] = quotes

            # æ ¹æ®æŠ¥ä»·ç”Ÿæˆä¿¡å·ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            # å®é™…åšå¸‚éœ€è¦ç›´æ¥æäº¤é™ä»·è®¢å•
            current_inventory = self.inventory.get(symbol, 0)

            # å¦‚æœåº“å­˜è¿‡å¤šï¼Œå‡å°‘ä¹°å…¥ï¼Œå¢åŠ å–å‡º
            if current_inventory > self.mm_config.max_position * 0.8:
                signals[symbol] = Signal.SELL
            elif current_inventory < -self.mm_config.max_position * 0.8:
                signals[symbol] = Signal.BUY

        return signals

    def _calculate_optimal_quotes(self, symbol: str, mid_price: float) -> Dict[str, float]:
        """è®¡ç®—æœ€ä¼˜æŠ¥ä»· - Avellaneda-Stoikovæ¨¡å‹"""
        # è®¡ç®—æ³¢åŠ¨ç‡
        returns = pd.Series(list(self.price_history[symbol])).pct_change().dropna()
        volatility = returns.std() if len(returns) > 0 else 0.01

        # å½“å‰åº“å­˜
        q = self.inventory.get(symbol, 0)

        # é£é™©å‚æ•°
        gamma = self.mm_config.risk_aversion
        sigma = volatility
        k = self.mm_config.alpha  # è®¢å•æ‰§è¡Œå¼ºåº¦å‚æ•°

        # è®¡ç®—åº“å­˜å¯¹ä»·æ ¼çš„å½±å“
        inventory_skew = gamma * sigma**2 * q / k

        # è®¡ç®—æœ€ä¼˜ä»·å·®
        half_spread = self.mm_config.base_spread + inventory_skew

        # è®¡ç®—ä¹°å–æŠ¥ä»·
        bid_price = mid_price - half_spread
        ask_price = mid_price + half_spread

        return {
            'mid_price': mid_price,
            'bid_price': max(bid_price, mid_price * 0.99),  # é™åˆ¶æœ€å¤§ä»·å·®
            'ask_price': min(ask_price, mid_price * 1.01),
            'bid_size': self.mm_config.order_size,
            'ask_size': self.mm_config.order_size,
            'inventory': q,
            'half_spread': half_spread
        }

    def update_inventory(self, symbol: str, quantity: float, side: str):
        """æ›´æ–°åº“å­˜"""
        if symbol not in self.inventory:
            self.inventory[symbol] = 0

        if side == 'buy':
            self.inventory[symbol] += quantity
        else:
            self.inventory[symbol] -= quantity

        logger.info(f"Inventory updated: {symbol} = {self.inventory[symbol]}")

    def get_quotes(self, symbol: str) -> Optional[Dict]:
        """è·å–å½“å‰æŠ¥ä»·"""
        return self.current_quotes.get(symbol)

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        # åšå¸‚ç­–ç•¥ä½¿ç”¨å›ºå®šè®¢å•å¤§å°
        return self.mm_config.order_size

    def should_hedge(self, symbol: str, current_price: float) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦å¯¹å†²åº“å­˜"""
        inventory = self.inventory.get(symbol, 0)
        abs_inventory = abs(inventory)

        # å¦‚æœåº“å­˜è¶…è¿‡é˜ˆå€¼ï¼Œéœ€è¦å¯¹å†²
        if abs_inventory > self.mm_config.max_position * 0.7:
            return True

        # è®¡ç®—åº“å­˜ä»·å€¼
        inventory_value = abs_inventory * current_price

        # å¦‚æœåº“å­˜ä»·å€¼è¿‡å¤§ï¼Œéœ€è¦å¯¹å†²
        if inventory_value > 100000:  # 10ä¸‡
            return True

        return False
```

### 15.4 å› å­æ¨¡å‹ç­–ç•¥

```python
# src/strategies/implementations/factor_model.py

from typing import Dict, List, Optional, Any
import pandas as pd
import numpy as np
from dataclasses import dataclass
from datetime import datetime
import logging

from src.strategies.base import Strategy, Signal

logger = logging.getLogger(__name__)

@dataclass
class Factor:
    """å› å­å®šä¹‰"""
    name: str
    weight: float
    long_short: str  # 'long', 'short', or 'neutral'

class FactorModelStrategy(Strategy):
    """
    å¤šå› å­æ¨¡å‹ç­–ç•¥

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. è®¡ç®—å¤šä¸ªå› å­ï¼ˆä»·å€¼ã€è´¨é‡ã€æˆé•¿ã€åŠ¨é‡ç­‰ï¼‰
    2. ç»¼åˆå› å­å¾—åˆ†è¿›è¡Œé€‰è‚¡
    3. åŠ¨æ€è°ƒä»“
    """

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)
        self.factors: List[Factor] = []
        self.factor_data: Dict[str, pd.DataFrame] = {}
        self.position_size_pct = config.get('position_size', 0.05)
        self.top_n = config.get('top_n', 30)
        self.rebalance_frequency = config.get('rebalance_frequency', 'monthly')

        # åˆå§‹åŒ–å› å­
        self._init_factors(config.get('factors', {}))

    def _init_factors(self, factor_configs: Dict[str, float]):
        """åˆå§‹åŒ–å› å­"""
        factor_definitions = {
            'value': Factor('value', 0.2, 'long'),
            'quality': Factor('quality', 0.2, 'long'),
            'growth': Factor('growth', 0.2, 'long'),
            'momentum': Factor('momentum', 0.2, 'long'),
            'volatility': Factor('volatility', 0.1, 'short'),
            'size': Factor('size', 0.1, 'long')
        }

        for factor_name, weight in factor_configs.items():
            if factor_name in factor_definitions:
                factor = factor_definitions[factor_name]
                factor.weight = weight
                self.factors.append(factor)

    async def calculate_factors(self, price_data: pd.DataFrame,
                               fundamental_data: Optional[Dict] = None) -> pd.DataFrame:
        """è®¡ç®—æ‰€æœ‰å› å­"""
        factor_scores = pd.DataFrame(index=price_data.index)

        for factor in self.factors:
            scores = await self._calculate_single_factor(factor, price_data, fundamental_data)
            factor_scores[factor.name] = scores

        # å½’ä¸€åŒ–å› å­å¾—åˆ†
        factor_scores = (factor_scores - factor_scores.mean()) / factor_scores.std()

        return factor_scores

    async def _calculate_single_factor(self, factor: Factor, price_data: pd.DataFrame,
                                      fundamental_data: Optional[Dict]) -> pd.Series:
        """è®¡ç®—å•ä¸ªå› å­"""
        scores = pd.Series(index=price_data.index, dtype=float)

        if factor.name == 'value':
            # ä»·å€¼å› å­ï¼šå¸‚ç›ˆç‡ã€å¸‚å‡€ç‡ç­‰
            if fundamental_data:
                for symbol in price_data.index:
                    if symbol in fundamental_data:
                        pe = fundamental_data[symbol].get('pe_ratio', np.nan)
                        pb = fundamental_data[symbol].get('pb_ratio', np.nan)
                        # ç»¼åˆä»·å€¼å¾—åˆ†ï¼ˆPEå’ŒPBè¶Šä½è¶Šå¥½ï¼‰
                        if not np.isnan(pe) and not np.isnan(pb):
                            scores[symbol] = -(np.log(pe) + np.log(pb)) / 2

        elif factor.name == 'quality':
            # è´¨é‡å› å­ï¼šROEã€ROAç­‰
            if fundamental_data:
                for symbol in price_data.index:
                    if symbol in fundamental_data:
                        roe = fundamental_data[symbol].get('roe', np.nan)
                        roa = fundamental_data[symbol].get('roa', np.nan)
                        if not np.isnan(roe) and not np.isnan(roa):
                            scores[symbol] = (roe + roa) / 2

        elif factor.name == 'growth':
            # æˆé•¿å› å­ï¼šè¥æ”¶å¢é•¿ã€åˆ©æ¶¦å¢é•¿
            if fundamental_data:
                for symbol in price_data.index:
                    if symbol in fundamental_data:
                        revenue_growth = fundamental_data[symbol].get('revenue_growth', np.nan)
                        profit_growth = fundamental_data[symbol].get('profit_growth', np.nan)
                        if not np.isnan(revenue_growth) and not np.isnan(profit_growth):
                            scores[symbol] = (revenue_growth + profit_growth) / 2

        elif factor.name == 'momentum':
            # åŠ¨é‡å› å­ï¼šè¿‡å»12ä¸ªæœˆæ”¶ç›Šç‡
            for symbol in price_data.index:
                if len(price_data.columns) > 252:
                    prices = price_data[symbol]
                    if len(prices) >= 252:
                        scores[symbol] = prices.iloc[-1] / prices.iloc[-252] - 1

        elif factor.name == 'volatility':
            # æ³¢åŠ¨ç‡å› å­ï¼šå†å²æ³¢åŠ¨ç‡
            for symbol in price_data.index:
                prices = price_data[symbol]
                if len(prices) >= 20:
                    returns = prices.pct_change().dropna()
                    scores[symbol] = returns.std()

        elif factor.name == 'size':
            # è§„æ¨¡å› å­ï¼šå¸‚å€¼
            if fundamental_data:
                for symbol in price_data.index:
                    if symbol in fundamental_data:
                        market_cap = fundamental_data[symbol].get('market_cap', np.nan)
                        if not np.isnan(market_cap):
                            scores[symbol] = np.log(market_cap)

        return scores

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        # è®¡ç®—å› å­å¾—åˆ†
        factor_scores = await self.calculate_factors(market_data)

        # è®¡ç®—ç»¼åˆå¾—åˆ†
        composite_score = pd.Series(0, index=factor_scores.index)

        for factor in self.factors:
            if factor.name in factor_scores.columns:
                if factor.long_short == 'short':
                    composite_score -= factor.weight * factor_scores[factor.name]
                else:
                    composite_score += factor.weight * factor_scores[factor.name]

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„è‚¡ç¥¨
        top_stocks = composite_score.nlargest(self.top_n).index.tolist()

        # ç”Ÿæˆä¿¡å·
        signals = {}
        for symbol in market_data.index:
            if symbol in top_stocks:
                signals[symbol] = Signal.BUY
            else:
                signals[symbol] = Signal.HOLD

        return signals

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        # ç­‰æƒé‡åˆ†é…
        return portfolio_value / self.top_n if signal == Signal.BUY else 0
```

### 15.5 ç½‘æ ¼äº¤æ˜“ç­–ç•¥

```python
# src/strategies/implementations/grid_trading.py

from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
import numpy as np
import logging

from src.strategies.base import Strategy, Signal

logger = logging.getLogger(__name__)

@dataclass
class GridLevel:
    """ç½‘æ ¼å±‚çº§"""
    price: float
    buy_order: bool = False
    sell_order: bool = False
    order_id: Optional[str] = None

class GridTradingStrategy(Strategy):
    """
    ç½‘æ ¼äº¤æ˜“ç­–ç•¥

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. åœ¨ä»·æ ¼åŒºé—´å†…è®¾ç½®å¤šä¸ªç½‘æ ¼
    2. ä»·æ ¼ä¸‹è·Œæ—¶ä¹°å…¥ï¼Œä¸Šæ¶¨æ—¶å–å‡º
    3. æŒç»­èµšå–å°å¹…ä»·å·®
    """

    def __init__(self, name: str, config: Dict[str, Any]):
        super().__init__(name, config)

        # ç½‘æ ¼å‚æ•°
        self.grid_count = config.get('grid_count', 10)  # ç½‘æ ¼æ•°é‡
        self.grid_range_pct = config.get('grid_range_pct', 0.2)  # ç½‘æ ¼èŒƒå›´ï¼ˆ20%ï¼‰
        self.position_size_pct = config.get('position_size', 0.1)  # æ¯æ ¼ä»“ä½

        # ç½‘æ ¼çŠ¶æ€
        self.grids: Dict[str, List[GridLevel]] = {}
        self.base_prices: Dict[str, float] = {}

    def _initialize_grids(self, symbol: str, base_price: float) -> List[GridLevel]:
        """åˆå§‹åŒ–ç½‘æ ¼"""
        grids = []

        # è®¡ç®—ä»·æ ¼èŒƒå›´
        lower_price = base_price * (1 - self.grid_range_pct / 2)
        upper_price = base_price * (1 + self.grid_range_pct / 2)

        # è®¡ç®—ç½‘æ ¼é—´è·
        grid_spacing = (upper_price - lower_price) / self.grid_count

        # åˆ›å»ºç½‘æ ¼å±‚çº§
        for i in range(self.grid_count + 1):
            price = lower_price + i * grid_spacing
            grids.append(GridLevel(price=price))

        logger.info(f"Initialized {len(grids)} grids for {symbol} "
                   f"from {lower_price:.2f} to {upper_price:.2f}")

        return grids

    async def generate_signals(self, market_data: pd.DataFrame) -> Dict[str, str]:
        """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
        signals = {}

        for symbol in market_data.index:
            current_price = market_data.loc[symbol, 'close']

            # åˆå§‹åŒ–ç½‘æ ¼
            if symbol not in self.grids:
                self.base_prices[symbol] = current_price
                self.grids[symbol] = self._initialize_grids(symbol, current_price)
                continue

            # æ£€æŸ¥ç½‘æ ¼è§¦å‘
            grids = self.grids[symbol]

            for grid in grids:
                # ä¹°å…¥ä¿¡å·ï¼šä»·æ ¼è§¦åŠç½‘æ ¼ä¸”æ²¡æœ‰ä¹°å…¥è®¢å•
                if current_price <= grid.price and not grid.buy_order:
                    signals[symbol] = Signal.BUY
                    grid.buy_order = True
                    grid.sell_order = False
                    logger.info(f"Grid buy triggered for {symbol} at {grid.price:.2f}")

                # å–å‡ºä¿¡å·ï¼šä»·æ ¼ä¸Šæ¶¨åˆ°ä¸‹ä¸€ç½‘æ ¼
                elif current_price >= grid.price and not grid.sell_order:
                    # æ‰¾åˆ°å½“å‰ç½‘æ ¼çš„ä¸‹ä¸€ä¸ªç½‘æ ¼
                    grid_index = grids.index(grid)
                    if grid_index < len(grids) - 1:
                        next_grid = grids[grid_index + 1]
                        if current_price >= next_grid.price:
                            signals[symbol] = Signal.SELL
                            grid.sell_order = True
                            grid.buy_order = False
                            logger.info(f"Grid sell triggered for {symbol} at {grid.price:.2f}")

        return signals

    async def calculate_position_size(self, symbol: str, signal: str,
                                     price: float, portfolio_value: float) -> float:
        """è®¡ç®—ä»“ä½å¤§å°"""
        # æ¯ä¸ªç½‘æ ¼ä½¿ç”¨å›ºå®šæ¯”ä¾‹çš„èµ„é‡‘
        return portfolio_value * self.position_size_pct

    def get_grid_status(self, symbol: str) -> Dict[str, Any]:
        """è·å–ç½‘æ ¼çŠ¶æ€"""
        if symbol not in self.grids:
            return {}

        grids = self.grids[symbol]
        return {
            'base_price': self.base_prices.get(symbol),
            'total_grids': len(grids),
            'active_buy_grids': sum(1 for g in grids if g.buy_order),
            'active_sell_grids': sum(1 for g in grids if g.sell_order),
            'price_range': (grids[0].price, grids[-1].price)
        }
```

---

## ç¬¬16ç«  æœºå™¨å­¦ä¹ é›†æˆ

### 16.1 LSTMä»·æ ¼é¢„æµ‹æ¨¡å‹

```python
# src/ml/lstm_predictor.py

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Tuple, Optional, Any
import logging

logger = logging.getLogger(__name__)

class PriceDataset(Dataset):
    """ä»·æ ¼æ•°æ®é›†"""
    def __init__(self, sequences: np.ndarray, targets: np.ndarray):
        self.sequences = torch.FloatTensor(sequences)
        self.targets = torch.FloatTensor(targets)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.targets[idx]

class LSTMPredictor(nn.Module):
    """LSTMä»·æ ¼é¢„æµ‹æ¨¡å‹"""

    def __init__(self, input_size: int = 5, hidden_size: int = 64,
                 num_layers: int = 2, dropout: float = 0.2):
        super(LSTMPredictor, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(hidden_size, 32)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(32, 1)

    def forward(self, x):
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, (h_n, c_n) = self.lstm(x)

        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]

        # å…¨è¿æ¥å±‚
        out = self.fc1(last_output)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc2(out)

        return out.squeeze()

class PricePredictionModel:
    """ä»·æ ¼é¢„æµ‹æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, sequence_length: int = 60,
                 features: List[str] = None):
        self.sequence_length = sequence_length
        self.features = features or ['open', 'high', 'low', 'close', 'volume']
        self.scaler = StandardScaler()
        self.model: Optional[LSTMPredictor] = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def prepare_data(self, price_data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®

        Args:
            price_data: ä»·æ ¼æ•°æ® DataFrame

        Returns:
            sequences, targets
        """
        # æ ‡å‡†åŒ–æ•°æ®
        scaled_data = self.scaler.fit_transform(price_data[self.features])

        sequences = []
        targets = []

        # åˆ›å»ºåºåˆ—
        for i in range(len(scaled_data) - self.sequence_length):
            seq = scaled_data[i:i + self.sequence_length]
            # ç›®æ ‡æ˜¯ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„æ”¶ç›˜ä»·
            target = scaled_data[i + self.sequence_length][3]  # close price
            sequences.append(seq)
            targets.append(target)

        return np.array(sequences), np.array(targets)

    def train(self, price_data: pd.DataFrame,
              epochs: int = 100,
              batch_size: int = 32,
              learning_rate: float = 0.001,
              validation_split: float = 0.2) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹
        """
        # å‡†å¤‡æ•°æ®
        sequences, targets = self.prepare_data(price_data)

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(sequences) * (1 - validation_split))

        train_sequences = sequences[:split_idx]
        train_targets = targets[:split_idx]
        val_sequences = sequences[split_idx:]
        val_targets = targets[split_idx:]

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = PriceDataset(train_sequences, train_targets)
        val_dataset = PriceDataset(val_sequences, val_targets)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        # åˆå§‹åŒ–æ¨¡å‹
        self.model = LSTMPredictor(
            input_size=len(self.features),
            hidden_size=64,
            num_layers=2,
            dropout=0.2
        ).to(self.device)

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.MSELoss()
        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)

        # è®­ç»ƒ
        train_losses = []
        val_losses = []

        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            for sequences, targets in train_loader:
                sequences = sequences.to(self.device)
                targets = targets.to(self.device)

                optimizer.zero_grad()
                outputs = self.model(sequences)
                loss = criterion(outputs, targets)
                loss.backward()
                optimizer.step()

                train_loss += loss.item()

            train_loss /= len(train_loader)
            train_losses.append(train_loss)

            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0
            with torch.no_grad():
                for sequences, targets in val_loader:
                    sequences = sequences.to(self.device)
                    targets = targets.to(self.device)

                    outputs = self.model(sequences)
                    loss = criterion(outputs, targets)
                    val_loss += loss.item()

            val_loss /= len(val_loader)
            val_losses.append(val_loss)

            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}/{epochs}, Train Loss: {train_loss:.6f}, "
                          f"Val Loss: {val_loss:.6f}")

        return {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1]
        }

    def predict(self, price_data: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹æœªæ¥ä»·æ ¼
        """
        if self.model is None:
            raise ValueError("Model not trained yet")

        # å‡†å¤‡æ•°æ®
        sequences, _ = self.prepare_data(price_data)

        # é¢„æµ‹
        self.model.eval()
        predictions = []

        with torch.no_grad():
            for seq in sequences:
                seq_tensor = torch.FloatTensor(seq).unsqueeze(0).to(self.device)
                pred = self.model(seq_tensor)
                predictions.append(pred.item())

        # åæ ‡å‡†åŒ–
        predictions = np.array(predictions).reshape(-1, 1)
        # åˆ›å»ºä¸€ä¸ªå…¨é›¶æ•°ç»„ç”¨äºåæ ‡å‡†åŒ–
        dummy = np.zeros((len(predictions), len(self.features)))
        dummy[:, 3] = predictions[:, 0]  # close price column
        predictions_denorm = self.scaler.inverse_transform(dummy)[:, 3]

        return predictions_denorm

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("No model to save")

        torch.save({
            'model_state_dict': self.model.state_dict(),
            'scaler': self.scaler,
            'sequence_length': self.sequence_length,
            'features': self.features
        }, path)
        logger.info(f"Model saved to {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)

        self.model = LSTMPredictor(
            input_size=len(checkpoint['features']),
            hidden_size=64,
            num_layers=2
        ).to(self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.scaler = checkpoint['scaler']
        self.sequence_length = checkpoint['sequence_length']
        self.features = checkpoint['features']

        self.model.eval()
        logger.info(f"Model loaded from {path}")
```

### 16.2 éšæœºæ£®æ—åˆ†ç±»å™¨

```python
# src/ml/random_forest_classifier.py

from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import joblib
import logging

logger = logging.getLogger(__name__)

class TechnicalFeatureExtractor:
    """æŠ€æœ¯ç‰¹å¾æå–å™¨"""

    @staticmethod
    def add_technical_features(df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        df = df.copy()

        # ç§»åŠ¨å¹³å‡
        df['ma5'] = df['close'].rolling(window=5).mean()
        df['ma10'] = df['close'].rolling(window=10).mean()
        df['ma20'] = df['close'].rolling(window=20).mean()
        df['ma60'] = df['close'].rolling(window=60).mean()

        # ä»·æ ¼åŠ¨é‡
        df['momentum_5'] = df['close'] / df['close'].shift(5) - 1
        df['momentum_10'] = df['close'] / df['close'].shift(10) - 1
        df['momentum_20'] = df['close'] / df['close'].shift(20) - 1

        # æ³¢åŠ¨ç‡
        df['volatility_10'] = df['close'].pct_change().rolling(10).std()
        df['volatility_20'] = df['close'].pct_change().rolling(20).std()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # MACD
        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = exp1 - exp2
        df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['macd_hist'] = df['macd'] - df['macd_signal']

        # å¸ƒæ—å¸¦
        df['bb_middle'] = df['close'].rolling(window=20).mean()
        bb_std = df['close'].rolling(window=20).std()
        df['bb_upper'] = df['bb_middle'] + 2 * bb_std
        df['bb_lower'] = df['bb_middle'] - 2 * bb_std
        df['bb_width'] = df['bb_upper'] - df['bb_lower']
        df['bb_position'] = (df['close'] - df['bb_lower']) / df['bb_width']

        # æˆäº¤é‡æŒ‡æ ‡
        if 'volume' in df.columns:
            df['volume_ma5'] = df['volume'].rolling(window=5).mean()
            df['volume_ratio'] = df['volume'] / df['volume_ma5']

        # ä»·æ ¼ç›¸å¯¹ä½ç½®
        df['price_high_20'] = df['close'].rolling(window=20).max()
        df['price_low_20'] = df['close'].rolling(window=20).min()
        df['price_position'] = (df['close'] - df['price_low_20']) / \
                              (df['price_high_20'] - df['price_low_20'])

        return df

class TradingSignalClassifier:
    """äº¤æ˜“ä¿¡å·åˆ†ç±»å™¨"""

    def __init__(self, n_estimators: int = 100,
                 max_depth: Optional[int] = None,
                 min_samples_split: int = 10):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.model: Optional[RandomForestClassifier] = None
        self.feature_names: List[str] = []

    def prepare_labels(self, df: pd.DataFrame,
                      forward_period: int = 5,
                      threshold: float = 0.02) -> pd.Series:
        """
        å‡†å¤‡æ ‡ç­¾

        Returns:
            0: HOLD, 1: BUY, 2: SELL
        """
        future_returns = df['close'].shift(-forward_period) / df['close'] - 1

        labels = pd.Series(0, index=df.index)
        labels[future_returns > threshold] = 1  # BUY
        labels[future_returns < -threshold] = 2  # SELL

        return labels

    def train(self, price_data: pd.DataFrame,
              test_size: float = 0.2) -> Dict[str, Any]:
        """è®­ç»ƒæ¨¡å‹"""
        # æå–ç‰¹å¾
        df = TechnicalFeatureExtractor.add_technical_features(price_data)

        # å‡†å¤‡æ ‡ç­¾
        labels = self.prepare_labels(df)

        # ç§»é™¤NaN
        valid_idx = ~(df.isnull().any(axis=1) | labels.isnull())
        df_clean = df[valid_idx]
        labels_clean = labels[valid_idx]

        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = [col for col in df_clean.columns
                       if col not in ['open', 'high', 'low', 'close', 'volume']]

        self.feature_names = feature_cols
        X = df_clean[feature_cols].values
        y = labels_clean.values

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42, stratify=y
        )

        # è®­ç»ƒæ¨¡å‹
        self.model = RandomForestClassifier(
            n_estimators=self.n_estimators,
            max_depth=self.max_depth,
            min_samples_split=self.min_samples_split,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )

        self.model.fit(X_train, y_train)

        # è¯„ä¼°
        y_pred = self.model.predict(X_test)

        report = classification_report(y_test, y_pred, output_dict=True)
        cm = confusion_matrix(y_test, y_pred)

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        logger.info("Training completed:")
        logger.info(f"Accuracy: {report['accuracy']:.4f}")
        logger.info(f"Macro avg F1: {report['macro avg']['f1-score']:.4f}")

        return {
            'classification_report': report,
            'confusion_matrix': cm,
            'feature_importance': feature_importance
        }

    def predict(self, price_data: pd.DataFrame) -> np.ndarray:
        """é¢„æµ‹äº¤æ˜“ä¿¡å·"""
        if self.model is None:
            raise ValueError("Model not trained yet")

        # æå–ç‰¹å¾
        df = TechnicalFeatureExtractor.add_technical_features(price_data)

        # è·å–æœ€åä¸€è¡Œçš„ç‰¹å¾
        last_features = df[self.feature_names].iloc[-1:].values

        # é¢„æµ‹
        prediction = self.model.predict(last_features)[0]
        probability = self.model.predict_proba(last_features)[0]

        return {
            'signal': int(prediction),  # 0: HOLD, 1: BUY, 2: SELL
            'probability': probability.tolist()
        }

    def get_feature_importance(self) -> pd.DataFrame:
        """è·å–ç‰¹å¾é‡è¦æ€§"""
        if self.model is None:
            raise ValueError("Model not trained yet")

        return pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("No model to save")

        joblib.dump({
            'model': self.model,
            'feature_names': self.feature_names,
            'params': {
                'n_estimators': self.n_estimators,
                'max_depth': self.max_depth,
                'min_samples_split': self.min_samples_split
            }
        }, path)
        logger.info(f"Model saved to {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        data = joblib.load(path)
        self.model = data['model']
        self.feature_names = data['feature_names']
        self.n_estimators = data['params']['n_estimators']
        self.max_depth = data['params']['max_depth']
        self.min_samples_split = data['params']['min_samples_split']
        logger.info(f"Model loaded from {path}")
```

### 16.3 å¼ºåŒ–å­¦ä¹ äº¤æ˜“æ™ºèƒ½ä½“

```python
# src/ml/rl_agent.py

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random
from typing import Dict, List, Tuple, Any, Optional
import logging

logger = logging.getLogger(__name__)

class TradingEnvironment:
    """äº¤æ˜“ç¯å¢ƒ"""

    def __init__(self, price_data: pd.DataFrame,
                 initial_balance: float = 100000,
                 transaction_cost: float = 0.001):
        self.price_data = price_data
        self.initial_balance = initial_balance
        self.transaction_cost = transaction_cost

        self.current_step = 0
        self.balance = initial_balance
        self.shares = 0
        self.total_shares_bought = 0
        self.total_shares_sold = 0

        # åŠ¨ä½œç©ºé—´ï¼š0=æŒæœ‰, 1=ä¹°å…¥, 2=å–å‡º
        self.action_space = 3

    def reset(self) -> np.ndarray:
        """é‡ç½®ç¯å¢ƒ"""
        self.current_step = 0
        self.balance = self.initial_balance
        self.shares = 0
        self.total_shares_bought = 0
        self.total_shares_sold = 0
        return self._get_observation()

    def _get_observation(self) -> np.ndarray:
        """è·å–å½“å‰è§‚å¯ŸçŠ¶æ€"""
        if self.current_step >= len(self.price_data):
            return np.zeros(10)  # ç‰¹å¾ç»´åº¦

        row = self.price_data.iloc[self.current_step]

        # çŠ¶æ€ç‰¹å¾
        obs = np.array([
            row.get('open', 0) / 10000,  # å½’ä¸€åŒ–
            row.get('high', 0) / 10000,
            row.get('low', 0) / 10000,
            row.get('close', 0) / 10000,
            row.get('volume', 0) / 1000000,
            self.balance / self.initial_balance,
            self.shares / 1000,
            self.total_shares_bought / 1000,
            self.total_shares_sold / 1000,
            self.current_step / len(self.price_data)
        ])

        return obs

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict]:
        """
        æ‰§è¡ŒåŠ¨ä½œ

        Returns:
            observation, reward, done, info
        """
        if self.current_step >= len(self.price_data) - 1:
            return self._get_observation(), 0, True, {}

        current_price = self.price_data.iloc[self.current_step]['close']
        next_price = self.price_data.iloc[self.current_step + 1]['close']

        reward = 0

        # æ‰§è¡ŒåŠ¨ä½œ
        if action == 1:  # ä¹°å…¥
            if self.balance > current_price * 100:
                # ä¹°å…¥100è‚¡
                cost = current_price * 100 * (1 + self.transaction_cost)
                self.balance -= cost
                self.shares += 100
                self.total_shares_bought += 100

        elif action == 2:  # å–å‡º
            if self.shares >= 100:
                # å–å‡º100è‚¡
                proceeds = current_price * 100 * (1 - self.transaction_cost)
                self.balance += proceeds
                self.shares -= 100
                self.total_shares_sold += 100

        # è®¡ç®—å¥–åŠ±ï¼ˆåŸºäºæŒä»“ä»·å€¼å˜åŒ–ï¼‰
        portfolio_value = self.balance + self.shares * next_price
        reward = (portfolio_value - self.initial_balance) / self.initial_balance

        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥
        self.current_step += 1

        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = self.current_step >= len(self.price_data) - 1

        info = {
            'portfolio_value': portfolio_value,
            'balance': self.balance,
            'shares': self.shares
        }

        return self._get_observation(), reward, done, info

class DQN(nn.Module):
    """Deep Q-Network"""

    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128):
        super(DQN, self).__init__()

        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class DQNAgent:
    """DQNäº¤æ˜“æ™ºèƒ½ä½“"""

    def __init__(self, state_size: int, action_size: int,
                 learning_rate: float = 0.001,
                 gamma: float = 0.95,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01,
                 memory_size: int = 10000,
                 batch_size: int = 32):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # æ¢ç´¢ç‡
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.batch_size = batch_size

        # ç»éªŒå›æ”¾
        self.memory = deque(maxlen=memory_size)

        # ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œ
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = DQN(state_size, action_size).to(self.device)
        self.target_model = DQN(state_size, action_size).to(self.device)
        self.update_target_model()

        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

    def update_target_model(self):
        """æ›´æ–°ç›®æ ‡ç½‘ç»œ"""
        self.target_model.load_state_dict(self.model.state_dict())

    def remember(self, state, action, reward, next_state, done):
        """å­˜å‚¨ç»éªŒ"""
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state, training: bool = True) -> int:
        """é€‰æ‹©åŠ¨ä½œ"""
        if training and np.random.rand() <= self.epsilon:
            return random.randrange(self.action_size)

        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.model(state_tensor)
        return q_values.argmax().item()

    def replay(self, batch_size: Optional[int] = None):
        """ç»éªŒå›æ”¾è®­ç»ƒ"""
        if len(self.memory) < self.batch_size:
            return

        batch_size = batch_size or self.batch_size
        minibatch = random.sample(self.memory, batch_size)

        states = torch.FloatTensor([t[0] for t in minibatch]).to(self.device)
        actions = torch.LongTensor([t[1] for t in minibatch]).to(self.device)
        rewards = torch.FloatTensor([t[2] for t in minibatch]).to(self.device)
        next_states = torch.FloatTensor([t[3] for t in minibatch]).to(self.device)
        dones = torch.BoolTensor([t[4] for t in minibatch]).to(self.device)

        # å½“å‰Qå€¼
        current_q_values = self.model(states).gather(1, actions.unsqueeze(1))

        # ä¸‹ä¸€Qå€¼
        next_q_values = self.target_model(next_states).max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)

        # è®¡ç®—æŸå¤±
        loss = self.criterion(current_q_values.squeeze(), target_q_values)

        # ä¼˜åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        # è¡°å‡æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

    def train(self, env: TradingEnvironment, episodes: int = 1000) -> Dict[str, List]:
        """è®­ç»ƒæ™ºèƒ½ä½“"""
        scores = []
        losses = []

        for episode in range(episodes):
            state = env.reset()
            total_reward = 0
            done = False

            while not done:
                action = self.act(state)
                next_state, reward, done, _ = env.step(action)

                self.remember(state, action, reward, next_state, done)

                self.replay()

                state = next_state
                total_reward += reward

            scores.append(total_reward)

            # æ¯10ä¸ªepisodeæ›´æ–°ç›®æ ‡ç½‘ç»œ
            if episode % 10 == 0:
                self.update_target_model()
                avg_score = np.mean(scores[-10:])
                logger.info(f"Episode {episode}/{episodes}, "
                          f"Avg Score: {avg_score:.4f}, "
                          f"Epsilon: {self.epsilon:.4f}")

        return {
            'scores': scores,
            'average_reward': np.mean(scores)
        }

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'target_model_state_dict': self.target_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }, path)
        logger.info(f"Model saved to {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(path)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint['epsilon']
        logger.info(f"Model loaded from {path}")

    def predict(self, state: np.ndarray) -> int:
        """é¢„æµ‹åŠ¨ä½œï¼ˆæ¨ç†æ¨¡å¼ï¼‰"""
        return self.act(state, training=False)
```

### 16.4 é›†æˆå­¦ä¹ ç­–ç•¥

```python
# src/ml/ensemble_strategy.py

from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import joblib
import logging

logger = logging.getLogger(__name__)

class EnsembleTradingStrategy:
    """é›†æˆå­¦ä¹ äº¤æ˜“ç­–ç•¥"""

    def __init__(self):
        self.models: Dict[str, Any] = {}
        self.ensemble_model: Optional[VotingClassifier] = None
        self.feature_names: List[str] = []

    def add_model(self, name: str, model: Any):
        """æ·»åŠ æ¨¡å‹åˆ°é›†æˆ"""
        self.models[name] = model
        logger.info(f"Added model: {name}")

    def train_ensemble(self, X_train: np.ndarray, y_train: np.ndarray,
                      feature_names: List[str]) -> Dict[str, Any]:
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        self.feature_names = feature_names

        # å®šä¹‰åŸºå­¦ä¹ å™¨
        estimators = [
            ('lr', LogisticRegression(max_iter=1000, random_state=42)),
            ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))
        ]

        # æ·»åŠ è‡ªå®šä¹‰æ¨¡å‹
        for name, model in self.models.items():
            if hasattr(model, 'predict'):
                estimators.append((name, model))

        # åˆ›å»ºæŠ•ç¥¨åˆ†ç±»å™¨
        self.ensemble_model = VotingClassifier(
            estimators=estimators,
            voting='soft'  # ä½¿ç”¨æ¦‚ç‡æŠ•ç¥¨
        )

        # è®­ç»ƒ
        self.ensemble_model.fit(X_train, y_train)

        # è¯„ä¼°
        train_score = self.ensemble_model.score(X_train, y_train)

        logger.info(f"Ensemble training completed. Train score: {train_score:.4f}")

        return {
            'train_score': train_score,
            'n_models': len(estimators)
        }

    def predict(self, X: np.ndarray) -> Dict[str, Any]:
        """é¢„æµ‹"""
        if self.ensemble_model is None:
            raise ValueError("Ensemble model not trained yet")

        # é¢„æµ‹ç±»åˆ«
        prediction = self.ensemble_model.predict(X)[0]

        # é¢„æµ‹æ¦‚ç‡
        probabilities = self.ensemble_model.predict_proba(X)[0]

        return {
            'signal': int(prediction),
            'probabilities': probabilities.tolist(),
            'confidence': float(max(probabilities))
        }

    def get_feature_importance(self) -> pd.DataFrame:
        """è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
        if self.ensemble_model is None:
            raise ValueError("Ensemble model not trained yet")

        # å°è¯•ä»GradientBoostingè·å–ç‰¹å¾é‡è¦æ€§
        for name, model in self.ensemble_model.estimators:
            if name == 'gb' and hasattr(model, 'feature_importances_'):
                return pd.DataFrame({
                    'feature': self.feature_names,
                    'importance': model.feature_importances_
                }).sort_values('importance', ascending=False)

        return pd.DataFrame()

    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.ensemble_model is None:
            raise ValueError("No model to save")

        joblib.dump({
            'ensemble_model': self.ensemble_model,
            'feature_names': self.feature_names,
            'models': self.models
        }, path)
        logger.info(f"Ensemble model saved to {path}")

    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        data = joblib.load(path)
        self.ensemble_model = data['ensemble_model']
        self.feature_names = data['feature_names']
        self.models = data['models']
        logger.info(f"Ensemble model loaded from {path}")
```

### 16.5 åœ¨çº¿å­¦ä¹ å’Œæ¨¡å‹æ›´æ–°

```python
# src/ml/online_learning.py

from typing import Dict, List, Any, Optional
import pandas as pd
import numpy as np
from river import compose, linear_model, metrics, preprocessing
import logging

logger = logging.getLogger(__name__)

class OnlineLearningModel:
    """åœ¨çº¿å­¦ä¹ æ¨¡å‹ - ä½¿ç”¨Riveråº“"""

    def __init__(self, feature_size: int = 20):
        self.feature_size = feature_size
        self.model = compose.Pipeline(
            preprocessing.StandardScaler(),
            linear_model.LogisticRegression()
        )
        self.metric = metrics.Accuracy()
        self.samples_seen = 0

    def learn_one(self, x: Dict[str, float], y: int) -> float:
        """
        åœ¨çº¿å­¦ä¹ ä¸€ä¸ªæ ·æœ¬

        Args:
            x: ç‰¹å¾å­—å…¸
            y: æ ‡ç­¾ (0 or 1)

        Returns:
            accuracy
        """
        # é¢„æµ‹
        y_pred = self.model.predict_one(x)

        # æ›´æ–°æ¨¡å‹
        self.model.learn_one(x, y)

        # æ›´æ–°æŒ‡æ ‡
        accuracy = self.metric.update(y_true=y, y_pred=y_pred)
        self.samples_seen += 1

        return accuracy

    def predict_proba_one(self, x: Dict[str, float]) -> Dict[int, float]:
        """é¢„æµ‹æ¦‚ç‡"""
        return self.model.predict_proba_one(x)

    def predict_one(self, x: Dict[str, float]) -> int:
        """é¢„æµ‹ç±»åˆ«"""
        return self.model.predict_one(x)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'samples_seen': self.samples_seen,
            'accuracy': self.metric.get(),
            'model_weights': dict(self.model[-1].weights) if hasattr(self.model[-1], 'weights') else {}
        }

class AdaptiveModelManager:
    """è‡ªé€‚åº”æ¨¡å‹ç®¡ç†å™¨"""

    def __init__(self, retrain_threshold: float = 0.7,
                 min_samples: int = 1000):
        self.retrain_threshold = retrain_threshold
        self.min_samples = min_samples
        self.online_model = OnlineLearningModel()
        self.batch_model: Optional[Any] = None
        self.performance_history: List[float] = []

    def update(self, features: Dict[str, float], label: int) -> Dict[str, Any]:
        """æ›´æ–°æ¨¡å‹"""
        # åœ¨çº¿å­¦ä¹ 
        accuracy = self.online_model.learn_one(features, label)
        self.performance_history.append(accuracy)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è®­ç»ƒæ‰¹é‡æ¨¡å‹
        stats = self.online_model.get_stats()

        should_retrain = (
            stats['samples_seen'] >= self.min_samples and
            accuracy < self.retrain_threshold
        )

        result = {
            'accuracy': accuracy,
            'samples_seen': stats['samples_seen'],
            'should_retrain': should_retrain
        }

        if should_retrain:
            logger.warning(f"Model performance dropped to {accuracy:.4f}, "
                          f"considering retraining")

        return result

    def predict(self, features: Dict[str, float]) -> Dict[str, Any]:
        """é¢„æµ‹"""
        prediction = self.online_model.predict_one(features)
        probabilities = self.online_model.predict_proba_one(features)

        return {
            'prediction': int(prediction),
            'probabilities': probabilities,
            'confidence': max(probabilities.values()) if probabilities else 0
        }
```

---

## ç¬¬17ç«  å®æ—¶æ•°æ®å¤„ç†ç®¡é“

### 17.1 æµå¼æ•°æ®å¤„ç†æ¡†æ¶

```python
# src/data/streaming_pipeline.py

import asyncio
from typing import Dict, List, Any, Optional, Callable, AsyncIterator
from dataclasses import dataclass, field
from datetime import datetime
from collections import deque
import json
import logging
import aiohttp
import aiokafka
import aioredis

logger = logging.getLogger(__name__)

@dataclass
class MarketDataEvent:
    """å¸‚åœºæ•°æ®äº‹ä»¶"""
    symbol: str
    timestamp: datetime
    price: float
    volume: float
    bid: Optional[float] = None
    ask: Optional[float] = None
    source: str = "unknown"

    def to_dict(self) -> Dict[str, Any]:
        return {
            'symbol': self.symbol,
            'timestamp': self.timestamp.isoformat(),
            'price': self.price,
            'volume': self.volume,
            'bid': self.bid,
            'ask': self.ask,
            'source': self.source
        }

class DataStream:
    """æ•°æ®æµ"""

    def __init__(self, name: str, buffer_size: int = 10000):
        self.name = name
        self.buffer = deque(maxlen=buffer_size)
        self.subscribers: List[Callable] = []
        self.lock = asyncio.Lock()

    async def publish(self, event: MarketDataEvent):
        """å‘å¸ƒäº‹ä»¶"""
        async with self.lock:
            self.buffer.append(event)

        # é€šçŸ¥æ‰€æœ‰è®¢é˜…è€…
        for callback in self.subscribers:
            try:
                await callback(event)
            except Exception as e:
                logger.error(f"Error in subscriber callback: {e}")

    def subscribe(self, callback: Callable):
        """è®¢é˜…æ•°æ®æµ"""
        self.subscribers.append(callback)

    async def get_latest(self, n: int = 1) -> List[MarketDataEvent]:
        """è·å–æœ€æ–°æ•°æ®"""
        async with self.lock:
            return list(self.buffer)[-n:]

class StreamProcessor:
    """æµå¤„ç†å™¨"""

    def __init__(self):
        self.streams: Dict[str, DataStream] = {}
        self.processors: List[Callable] = []

    def create_stream(self, name: str, buffer_size: int = 10000) -> DataStream:
        """åˆ›å»ºæ•°æ®æµ"""
        stream = DataStream(name, buffer_size)
        self.streams[name] = stream
        return stream

    def get_stream(self, name: str) -> Optional[DataStream]:
        """è·å–æ•°æ®æµ"""
        return self.streams.get(name)

    def add_processor(self, processor: Callable[[MarketDataEvent], Any]):
        """æ·»åŠ å¤„ç†å™¨"""
        self.processors.append(processor)

    async def process_event(self, event: MarketDataEvent):
        """å¤„ç†äº‹ä»¶"""
        for processor in self.processors:
            try:
                result = processor(event)
                if asyncio.iscoroutine(result):
                    await result
            except Exception as e:
                logger.error(f"Error processing event: {e}")

class RealTimeDataPipeline:
    """å®æ—¶æ•°æ®ç®¡é“"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.processor = StreamProcessor()

        # åˆ›å»ºæ•°æ®æµ
        self.raw_stream = self.processor.create_stream("raw")
        self.processed_stream = self.processor.create_stream("processed")
        self.signal_stream = self.processor.create_stream("signals")

        # Kafkaé…ç½®
        self.kafka_producer: Optional[aiokafka.AIOKafkaProducer] = None
        self.kafka_consumer: Optional[aiokafka.AIOKafkaConsumer] = None

        # Redisé…ç½®
        self.redis: Optional[aioredis.Redis] = None

        # è®¾ç½®å¤„ç†å™¨é“¾
        self._setup_processors()

    def _setup_processors(self):
        """è®¾ç½®å¤„ç†å™¨é“¾"""
        # æ•°æ®éªŒè¯
        self.processor.add_processor(self._validate_event)

        # æ•°æ®æ¸…æ´—
        self.processor.add_processor(self._clean_event)

        # æ•°æ®è½¬æ¢
        self.processor.add_processor(self._transform_event)

        # å‘å¸ƒåˆ°å¤„ç†åçš„æµ
        self.processor.add_processor(self._publish_processed)

    async def _validate_event(self, event: MarketDataEvent) -> bool:
        """éªŒè¯äº‹ä»¶"""
        if not event.symbol:
            return False
        if event.price <= 0:
            return False
        if event.volume < 0:
            return False
        return True

    async def _clean_event(self, event: MarketDataEvent) -> MarketDataEvent:
        """æ¸…æ´—äº‹ä»¶"""
        # ç§»é™¤å¼‚å¸¸å€¼
        if event.price > 1e10 or event.price < 1e-10:
            event.price = 0

        # æ ‡å‡†åŒ–æ—¶é—´æˆ³
        if not isinstance(event.timestamp, datetime):
            event.timestamp = datetime.now()

        return event

    async def _transform_event(self, event: MarketDataEvent) -> MarketDataEvent:
        """è½¬æ¢äº‹ä»¶"""
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        if event.bid and event.ask:
            event.spread = event.ask - event.bid
            event.mid_price = (event.bid + event.ask) / 2

        return event

    async def _publish_processed(self, event: MarketDataEvent):
        """å‘å¸ƒå¤„ç†åçš„æ•°æ®"""
        await self.processed_stream.publish(event)

    async def connect_kafka(self, bootstrap_servers: str):
        """è¿æ¥Kafka"""
        self.kafka_producer = aiokafka.AIOKafkaProducer(
            bootstrap_servers=bootstrap_servers,
            value_serializer=lambda v: json.dumps(v).encode()
        )
        await self.kafka_producer.start()
        logger.info("Kafka producer connected")

    async def connect_redis(self, url: str):
        """è¿æ¥Redis"""
        self.redis = await aioredis.from_url(url)
        await self.redis.ping()
        logger.info("Redis connected")

    async def ingest_event(self, event: MarketDataEvent):
        """æ‘„å–äº‹ä»¶"""
        # å‘å¸ƒåˆ°åŸå§‹æµ
        await self.raw_stream.publish(event)

        # å¤„ç†äº‹ä»¶
        await self.processor.process_event(event)

        # å‘é€åˆ°Kafka
        if self.kafka_producer:
            await self.kafka_producer.send(
                'market_data',
                value=event.to_dict()
            )

        # ç¼“å­˜åˆ°Redis
        if self.redis:
            await self.redis.setex(
                f"latest:{event.symbol}",
                60,  # 60ç§’è¿‡æœŸ
                json.dumps(event.to_dict())
            )

    async def get_latest_data(self, symbol: str) -> Optional[Dict[str, Any]]:
        """è·å–æœ€æ–°æ•°æ®"""
        if self.redis:
            data = await self.redis.get(f"latest:{symbol}")
            if data:
                return json.loads(data)

        # ä»å†…å­˜è·å–
        stream = self.processor.get_stream("processed")
        if stream:
            events = await stream.get_latest(100)
            for event in reversed(events):
                if event.symbol == symbol:
                    return event.to_dict()

        return None

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.kafka_producer:
            await self.kafka_producer.stop()
        if self.redis:
            await self.redis.close()
```

### 17.2 WebSocketæ•°æ®æºè¿æ¥å™¨

```python
# src/data/websocket_connector.py

import asyncio
import json
import websockets
from typing import Dict, List, Any, Optional, Callable
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class WebSocketConnector:
    """WebSocketè¿æ¥å™¨"""

    def __init__(self, uri: str, on_message: Callable,
                 reconnect_interval: int = 5,
                 ping_interval: int = 20):
        self.uri = uri
        self.on_message = on_message
        self.reconnect_interval = reconnect_interval
        self.ping_interval = ping_interval

        self.websocket: Optional[websockets.WebSocketClientProtocol] = None
        self.running = False
        self.reconnect_task: Optional[asyncio.Task] = None

    async def connect(self):
        """å»ºç«‹è¿æ¥"""
        while True:
            try:
                logger.info(f"Connecting to {self.uri}")
                self.websocket = await websockets.connect(
                    self.uri,
                    ping_interval=self.ping_interval
                )
                self.running = True
                logger.info("Connected successfully")

                # å¼€å§‹ç›‘å¬æ¶ˆæ¯
                await self._listen()

            except Exception as e:
                logger.error(f"Connection error: {e}")
                if self.running:
                    logger.info(f"Reconnecting in {self.reconnect_interval} seconds...")
                    await asyncio.sleep(self.reconnect_interval)
                else:
                    break

    async def _listen(self):
        """ç›‘å¬æ¶ˆæ¯"""
        try:
            async for message in self.websocket:
                try:
                    data = json.loads(message)
                    await self.on_message(data)
                except json.JSONDecodeError as e:
                    logger.warning(f"Invalid JSON: {e}")
                except Exception as e:
                    logger.error(f"Error processing message: {e}")

        except websockets.exceptions.ConnectionClosed:
            logger.warning("Connection closed")
        except Exception as e:
            logger.error(f"Listen error: {e}")

    async def send(self, data: Dict[str, Any]):
        """å‘é€æ•°æ®"""
        if self.websocket and not self.websocket.closed:
            await self.websocket.send(json.dumps(data))

    async def subscribe(self, symbols: List[str]):
        """è®¢é˜…"""
        await self.send({
            "action": "subscribe",
            "symbols": symbols
        })

    async def unsubscribe(self, symbols: List[str]):
        """å–æ¶ˆè®¢é˜…"""
        await self.send({
            "action": "unsubscribe",
            "symbols": symbols
        })

    async def close(self):
        """å…³é—­è¿æ¥"""
        self.running = False
        if self.websocket:
            await self.websocket.close()
        logger.info("WebSocket connection closed")

class ExchangeWebSocketFactory:
    """äº¤æ˜“æ‰€WebSocketå·¥å‚"""

    @staticmethod
    def create_binance_connector(on_message: Callable) -> WebSocketConnector:
        """åˆ›å»ºå¸å®‰è¿æ¥å™¨"""
        uri = "wss://stream.binance.com:9443/ws"
        return WebSocketConnector(uri, on_message)

    @staticmethod
    def create_okx_connector(on_message: Callable) -> WebSocketConnector:
        """åˆ›å»ºOKXè¿æ¥å™¨"""
        uri = "wss://ws.okx.com:8443/ws/v5/public"
        return WebSocketConnector(uri, on_message)

    @staticmethod
    def create_bitfinex_connector(on_message: Callable) -> WebSocketConnector:
        """åˆ›å»ºBitfinexè¿æ¥å™¨"""
        uri = "wss://api-pub.bitfinex.com/ws/2"
        return WebSocketConnector(uri, on_message)
```

### 17.3 æ•°æ®è´¨é‡ç›‘æ§

```python
# src/data/quality_monitor.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import defaultdict
import asyncio
import logging

logger = logging.getLogger(__name__)

@dataclass
class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡"""
    total_events: int = 0
    missing_values: int = 0
    outliers: int = 0
    duplicates: int = 0
    late_arrivals: int = 0
    avg_latency: float = 0.0

class DataQualityMonitor:
    """æ•°æ®è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, outlier_threshold: float = 3.0,
                 max_latency_seconds: float = 5.0):
        self.outlier_threshold = outlier_threshold
        self.max_latency_seconds = max_latency_seconds

        self.metrics: Dict[str, QualityMetrics] = defaultdict(QualityMetrics)
        self.last_timestamps: Dict[str, datetime] = {}
        self.last_prices: Dict[str, float] = {}
        self.price_history: Dict[str, List] = defaultdict(list)

    async def check_event(self, event: MarketDataEvent) -> Dict[str, Any]:
        """æ£€æŸ¥äº‹ä»¶è´¨é‡"""
        symbol = event.symbol
        metrics = self.metrics[symbol]

        metrics.total_events += 1

        issues = []

        # æ£€æŸ¥ç¼ºå¤±å€¼
        if event.price is None or event.price == 0:
            metrics.missing_values += 1
            issues.append("missing_price")

        # æ£€æŸ¥å»¶è¿Ÿ
        now = datetime.now()
        if symbol in self.last_timestamps:
            latency = (now - self.last_timestamps[symbol]).total_seconds()
            if latency > self.max_latency_seconds:
                metrics.late_arrivals += 1
                issues.append(f"late_arrival_{latency:.2f}s")

        self.last_timestamps[symbol] = now

        # æ£€æŸ¥å¼‚å¸¸å€¼
        if event.price > 0:
            self.price_history[symbol].append(event.price)
            if len(self.price_history[symbol]) > 100:
                self.price_history[symbol] = self.price_history[symbol][-100:]

            if len(self.price_history[symbol]) >= 20:
                prices = self.price_history[symbol]
                mean = sum(prices) / len(prices)
                std = (sum((p - mean) ** 2 for p in prices) / len(prices)) ** 0.5

                if std > 0:
                    z_score = abs(event.price - mean) / std
                    if z_score > self.outlier_threshold:
                        metrics.outliers += 1
                        issues.append(f"outlier_zscore_{z_score:.2f}")

        # æ£€æŸ¥é‡å¤
        if symbol in self.last_prices:
            if event.price == self.last_prices[symbol]:
                metrics.duplicates += 1
                issues.append("duplicate_price")

        self.last_prices[symbol] = event.price

        return {
            'symbol': symbol,
            'issues': issues,
            'quality_score': self._calculate_quality_score(metrics)
        }

    def _calculate_quality_score(self, metrics: QualityMetrics) -> float:
        """è®¡ç®—è´¨é‡å¾—åˆ†"""
        if metrics.total_events == 0:
            return 1.0

        error_rate = (
            metrics.missing_values +
            metrics.outliers +
            metrics.duplicates +
            metrics.late_arrivals
        ) / metrics.total_events

        return max(0.0, 1.0 - error_rate)

    def get_metrics(self, symbol: str) -> Dict[str, Any]:
        """è·å–æŒ‡æ ‡"""
        metrics = self.metrics[symbol]
        return {
            'symbol': symbol,
            'total_events': metrics.total_events,
            'missing_values': metrics.missing_values,
            'outliers': metrics.outliers,
            'duplicates': metrics.duplicates,
            'late_arrivals': metrics.late_arrivals,
            'quality_score': self._calculate_quality_score(metrics)
        }

    def get_all_metrics(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æŒ‡æ ‡"""
        return [self.get_metrics(symbol) for symbol in self.metrics.keys()]

    def reset_metrics(self, symbol: Optional[str] = None):
        """é‡ç½®æŒ‡æ ‡"""
        if symbol:
            del self.metrics[symbol]
        else:
            self.metrics.clear()
```

### 17.4 æ•°æ®å­˜å‚¨å’Œå½’æ¡£

```python
# src/data/storage.py

import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import asyncpg
import pandas as pd
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class DataStorage:
    """æ•°æ®å­˜å‚¨"""

    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.pool: Optional[asyncpg.Pool] = None
        self.parquet_path = Path(db_config.get('parquet_path', './data/parquet'))
        self.parquet_path.mkdir(parents=True, exist_ok=True)

    async def connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        self.pool = await asyncpg.create_pool(
            host=self.db_config['host'],
            port=self.db_config['port'],
            user=self.db_config['user'],
            password=self.db_config['password'],
            database=self.db_config['database'],
            min_size=5,
            max_size=20
        )
        logger.info("Database connection established")

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.pool:
            await self.pool.close()

    async def save_market_data(self, events: List[MarketDataEvent]):
        """ä¿å­˜å¸‚åœºæ•°æ®"""
        if not self.pool:
            raise RuntimeError("Database not connected")

        async with self.pool.acquire() as conn:
            await conn.executemany(
                """
                INSERT INTO market_data (symbol, timestamp, price, volume, bid, ask, source)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                ON CONFLICT (symbol, timestamp) DO UPDATE SET
                    price = EXCLUDED.price,
                    volume = EXCLUDED.volume,
                    bid = EXCLUDED.bid,
                    ask = EXCLUDED.ask
                """,
                [
                    (e.symbol, e.timestamp, e.price, e.volume, e.bid, e.ask, e.source)
                    for e in events
                ]
            )
        logger.info(f"Saved {len(events)} market data events")

    async def get_market_data(self, symbol: str, start_time: datetime,
                             end_time: datetime) -> pd.DataFrame:
        """è·å–å¸‚åœºæ•°æ®"""
        if not self.pool:
            raise RuntimeError("Database not connected")

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT timestamp, price, volume, bid, ask
                FROM market_data
                WHERE symbol = $1 AND timestamp >= $2 AND timestamp <= $3
                ORDER BY timestamp
                """,
                symbol, start_time, end_time
            )

        return pd.DataFrame([dict(row) for row in rows])

    async def archive_to_parquet(self, symbol: str, date: datetime):
        """å½’æ¡£åˆ°Parquet"""
        start_time = date.replace(hour=0, minute=0, second=0)
        end_time = start_time + timedelta(days=1)

        df = await self.get_market_data(symbol, start_time, end_time)

        if not df.empty:
            file_path = self.parquet_path / f"{symbol}/{date.strftime('%Y-%m-%d')}.parquet"
            file_path.parent.mkdir(parents=True, exist_ok=True)
            df.to_parquet(file_path)
            logger.info(f"Archived data to {file_path}")

    async def load_from_parquet(self, symbol: str, date: datetime) -> pd.DataFrame:
        """ä»ParquetåŠ è½½"""
        file_path = self.parquet_path / f"{symbol}/{date.strftime('%Y-%m-%d')}.parquet"
        if file_path.exists():
            return pd.read_parquet(file_path)
        return pd.DataFrame()

    async def cleanup_old_data(self, retention_days: int = 90):
        """æ¸…ç†æ—§æ•°æ®"""
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        if self.pool:
            async with self.pool.acquire() as conn:
                await conn.execute(
                    "DELETE FROM market_data WHERE timestamp < $1",
                    cutoff_date
                )
                logger.info(f"Cleaned up data older than {retention_days} days")
```

### 17.5 å®æ—¶æ•°æ®èšåˆ

```python
# src/data/aggregator.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
from collections import defaultdict
import asyncio
import pandas as pd
import logging

logger = logging.getLogger(__name__)

@dataclass
class OHLCV:
    """OHLCVæ•°æ®"""
    open: float
    high: float
    low: float
    close: float
    volume: float
    timestamp: datetime

class DataAggregator:
    """æ•°æ®èšåˆå™¨"""

    def __init__(self):
        self.bars: Dict[str, Dict[str, OHLCV]] = defaultdict(dict)
        self.tick_data: Dict[str, List] = defaultdict(list)

    async def add_tick(self, event: MarketDataEvent):
        """æ·»åŠ tickæ•°æ®"""
        self.tick_data[event.symbol].append(event)

        # èšåˆåˆ°ä¸åŒæ—¶é—´å‘¨æœŸ
        for interval in ['1m', '5m', '15m', '1h', '1d']:
            await self._update_bar(event, interval)

    async def _update_bar(self, event: MarketDataEvent, interval: str):
        """æ›´æ–°Kçº¿"""
        # ç¡®å®šæ—¶é—´çª—å£
        if interval == '1m':
            window = timedelta(minutes=1)
        elif interval == '5m':
            window = timedelta(minutes=5)
        elif interval == '15m':
            window = timedelta(minutes=15)
        elif interval == '1h':
            window = timedelta(hours=1)
        elif interval == '1d':
            window = timedelta(days=1)
        else:
            return

        # è®¡ç®—çª—å£å¼€å§‹æ—¶é—´
        timestamp = event.timestamp
        window_start = timestamp - (timestamp - datetime.min) % window

        bar_key = f"{event.symbol}_{interval}_{window_start.isoformat()}"

        if bar_key not in self.bars:
            # åˆ›å»ºæ–°Kçº¿
            self.bars[bar_key] = OHLCV(
                open=event.price,
                high=event.price,
                low=event.price,
                close=event.price,
                volume=event.volume,
                timestamp=window_start
            )
        else:
            # æ›´æ–°ç°æœ‰Kçº¿
            bar = self.bars[bar_key]
            bar.high = max(bar.high, event.price)
            bar.low = min(bar.low, event.price)
            bar.close = event.price
            bar.volume += event.volume

    def get_bar(self, symbol: str, interval: str,
                timestamp: datetime) -> Optional[OHLCV]:
        """è·å–Kçº¿"""
        if interval == '1m':
            window = timedelta(minutes=1)
        elif interval == '5m':
            window = timedelta(minutes=5)
        elif interval == '15m':
            window = timedelta(minutes=15)
        elif interval == '1h':
            window = timedelta(hours=1)
        elif interval == '1d':
            window = timedelta(days=1)
        else:
            return None

        window_start = timestamp - (timestamp - datetime.min) % window
        bar_key = f"{symbol}_{interval}_{window_start.isoformat()}"

        return self.bars.get(bar_key)

    def get_bars(self, symbol: str, interval: str,
                 limit: int = 100) -> List[OHLCV]:
        """è·å–Kçº¿åºåˆ—"""
        prefix = f"{symbol}_{interval}_"
        matching_bars = [
            bar for key, bar in self.bars.items()
            if key.startswith(prefix)
        ]

        # æŒ‰æ—¶é—´æ’åº
        matching_bars.sort(key=lambda x: x.timestamp)

        return matching_bars[-limit:]

    def get_dataframe(self, symbol: str, interval: str,
                     limit: int = 100) -> pd.DataFrame:
        """è·å–DataFrameæ ¼å¼"""
        bars = self.get_bars(symbol, interval, limit)

        if not bars:
            return pd.DataFrame()

        return pd.DataFrame([
            {
                'timestamp': bar.timestamp,
                'open': bar.open,
                'high': bar.high,
                'low': bar.low,
                'close': bar.close,
                'volume': bar.volume
            }
            for bar in bars
        ]).set_index('timestamp')
```

---

## ç¬¬18ç«  é«˜çº§è®¢å•ç±»å‹å’Œæ‰§è¡Œç®—æ³•

### 18.1 é«˜çº§è®¢å•ç±»å‹å®ç°

```python
# src/execution/advanced_orders.py

from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import asyncio
import logging

from src.core.order import Order, OrderStatus, OrderType

logger = logging.getLogger(__name__)

class TimeInForce(Enum):
    """è®¢å•æ—¶æ•ˆç±»å‹"""
    DAY = "DAY"  # å½“æ—¥æœ‰æ•ˆ
    GTC = "GTC"  # æ’¤é”€å‰æœ‰æ•ˆ
    IOC = "IOC"  # ç«‹å³æˆäº¤å¦åˆ™æ’¤é”€
    FOK = "FOK"  # å…¨éƒ¨æˆäº¤å¦åˆ™æ’¤é”€
    GTX = "GTX"  # åªåšæŒ‚å•ï¼ˆä¸ç©¿è¿‡ä»·æ ¼ï¼‰

@dataclass
class AdvancedOrderParams:
    """é«˜çº§è®¢å•å‚æ•°"""
    time_in_force: TimeInForce = TimeInForce.GTC
    display_quantity: Optional[int] = None  # å†°å±±è®¢å•æ˜¾ç¤ºæ•°é‡
    reserve_quantity: Optional[int] = None  # ä¿ç•™æ•°é‡
    trailing_percent: Optional[float] = None  # è¿½è¸ªç™¾åˆ†æ¯”
    expire_time: Optional[datetime] = None  # è¿‡æœŸæ—¶é—´

class IcebergOrder:
    """å†°å±±è®¢å• - å¤§å•æ‹†åˆ†æ˜¾ç¤º"""

    def __init__(self, symbol: str, side: str, total_quantity: float,
                 display_quantity: float, price: float,
                 on_fill: Callable, on_reject: Callable):
        self.symbol = symbol
        self.side = side  # 'buy' or 'sell'
        self.total_quantity = total_quantity
        self.display_quantity = display_quantity
        self.price = price
        self.on_fill = on_fill
        self.on_reject = on_reject

        self.remaining_quantity = total_quantity
        self.active_order: Optional[Order] = None
        self.filled_quantity = 0
        self.status = "active"

    async def execute(self, trading_engine):
        """æ‰§è¡Œå†°å±±è®¢å•"""
        while self.remaining_quantity > 0 and self.status == "active":
            # è®¡ç®—æœ¬æ¬¡æ˜¾ç¤ºæ•°é‡
            current_display = min(
                self.display_quantity,
                self.remaining_quantity
            )

            # åˆ›å»ºå­è®¢å•
            order = Order(
                symbol=self.symbol,
                side=self.side,
                order_type=OrderType.LIMIT,
                quantity=current_display,
                price=self.price
            )

            self.active_order = order

            # æäº¤è®¢å•
            success = await trading_engine.submit_order(order)

            if not success:
                await self.on_reject(order)
                self.status = "rejected"
                break

            # ç­‰å¾…æˆäº¤æˆ–å–æ¶ˆ
            await self._wait_for_fill(order)

            # æ›´æ–°å‰©ä½™æ•°é‡
            filled = order.filled_quantity if order.filled_quantity else 0
            self.filled_quantity += filled
            self.remaining_quantity -= filled

            logger.info(f"Iceberg order: Filled {filled}, "
                       f"remaining {self.remaining_quantity}")

    async def _wait_for_fill(self, order: Order, timeout: int = 60):
        """ç­‰å¾…æˆäº¤"""
        start_time = datetime.now()

        while order.status not in [OrderStatus.FILLED, OrderStatus.CANCELLED,
                                   OrderStatus.REJECTED]:
            await asyncio.sleep(0.1)

            # æ£€æŸ¥è¶…æ—¶
            if (datetime.now() - start_time).total_seconds() > timeout:
                # å–æ¶ˆæœªæˆäº¤éƒ¨åˆ†
                await self._cancel_order()
                break

    async def _cancel_order(self):
        """å–æ¶ˆè®¢å•"""
        if self.active_order:
            self.active_order.status = OrderStatus.CANCELLED

    def cancel(self):
        """å–æ¶ˆå†°å±±è®¢å•"""
        self.status = "cancelled"
        asyncio.create_task(self._cancel_order())

class TrailingStopOrder:
    """è¿½è¸ªæ­¢æŸè®¢å•"""

    def __init__(self, symbol: str, side: str, quantity: float,
                 trailing_percent: float, reference_price: float,
                 on_trigger: Callable):
        self.symbol = symbol
        self.side = side  # 'buy' for trailing buy, 'sell' for trailing sell
        self.quantity = quantity
        self.trailing_percent = trailing_percent
        self.reference_price = reference_price
        self.on_trigger = on_trigger

        self.peak_price = reference_price
        self.trigger_price = self._calculate_trigger_price()
        self.status = "active"

    def _calculate_trigger_price(self) -> float:
        """è®¡ç®—è§¦å‘ä»·æ ¼"""
        if self.side == 'sell':  # å–å‡ºè¿½è¸ªæ­¢æŸ
            # ä»·æ ¼ä¸Šæ¶¨æ—¶æé«˜æ­¢æŸä»·
            return self.peak_price * (1 - self.trailing_percent)
        else:  # ä¹°å…¥è¿½è¸ªæ­¢æŸ
            # ä»·æ ¼ä¸‹è·Œæ—¶é™ä½ä¹°å…¥ä»·
            return self.peak_price * (1 + self.trailing_percent)

    def update_price(self, current_price: float) -> bool:
        """
        æ›´æ–°ä»·æ ¼

        Returns:
            True if order should be triggered
        """
        # æ›´æ–°å³°å€¼ä»·æ ¼
        if self.side == 'sell':
            self.peak_price = max(self.peak_price, current_price)
        else:
            self.peak_price = min(self.peak_price, current_price)

        # é‡æ–°è®¡ç®—è§¦å‘ä»·æ ¼
        self.trigger_price = self._calculate_trigger_price()

        # æ£€æŸ¥æ˜¯å¦è§¦å‘
        if self.side == 'sell' and current_price <= self.trigger_price:
            return True
        elif self.side == 'buy' and current_price >= self.trigger_price:
            return True

        return False

    async def execute(self):
        """æ‰§è¡Œè®¢å•"""
        await self.on_trigger(
            self.symbol,
            self.side,
            self.quantity,
            self.trigger_price
        )

class OCOOrderPair:
    """OCOè®¢å•å¯¹ï¼ˆOne-Cancels-Otherï¼‰"""

    def __init__(self, order1: Order, order2: Order):
        self.order1 = order1
        self.order2 = order2
        self.status = "active"

    async def execute(self, trading_engine):
        """æ‰§è¡ŒOCOè®¢å•å¯¹"""
        # æäº¤ä¸¤ä¸ªè®¢å•
        success1 = await trading_engine.submit_order(self.order1)
        success2 = await trading_engine.submit_order(self.order2)

        if not (success1 and success2):
            self.status = "failed"
            return

        # ç›‘æ§è®¢å•çŠ¶æ€
        while self.status == "active":
            await asyncio.sleep(0.1)

            # æ£€æŸ¥è®¢å•1çŠ¶æ€
            if self.order1.status == OrderStatus.FILLED:
                # å–æ¶ˆè®¢å•2
                await self._cancel_order(self.order2)
                self.status = "order1_filled"
                break

            # æ£€æŸ¥è®¢å•2çŠ¶æ€
            if self.order2.status == OrderStatus.FILLED:
                # å–æ¶ˆè®¢å•1
                await self._cancel_order(self.order1)
                self.status = "order2_filled"
                break

    async def _cancel_order(self, order: Order):
        """å–æ¶ˆè®¢å•"""
        order.status = OrderStatus.CANCELLED

    def cancel(self):
        """å–æ¶ˆOCOè®¢å•å¯¹"""
        self.status = "cancelled"
        asyncio.create_task(self._cancel_order(self.order1))
        asyncio.create_task(self._cancel_order(self.order2))

class ConditionalOrder:
    """æ¡ä»¶è®¢å•"""

    def __init__(self, condition: Callable[[], bool],
                 target_order: Order):
        self.condition = condition
        self.target_order = target_order
        self.status = "pending"

    async def execute(self, trading_engine):
        """æ‰§è¡Œæ¡ä»¶è®¢å•"""
        while self.status == "pending":
            await asyncio.sleep(0.1)

            # æ£€æŸ¥æ¡ä»¶
            if self.condition():
                self.status = "triggered"

                # æ‰§è¡Œç›®æ ‡è®¢å•
                success = await trading_engine.submit_order(self.target_order)

                if success:
                    self.status = "submitted"
                else:
                    self.status = "failed"

                break
```

### 18.2 æ‰§è¡Œç®—æ³•å®ç°

```python
# src/execution/algorithms.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timedelta
import asyncio
import logging

logger = logging.getLogger(__name__)

@dataclass
class ExecutionParams:
    """æ‰§è¡Œå‚æ•°"""
    symbol: str
    side: str
    total_quantity: float
    urgency: str = "medium"  # low, medium, high
    max_participation_rate: float = 0.1  # æœ€å¤§å‚ä¸ç‡
    min_fill_size: float = 100  # æœ€å°æˆäº¤æ•°é‡
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None

class TWAPAlgorithm:
    """
    æ—¶é—´åŠ æƒå¹³å‡ä»·æ ¼ç®—æ³•

    åœ¨æŒ‡å®šæ—¶é—´å†…å‡åŒ€åˆ†æ•£æ‰§è¡Œè®¢å•
    """

    def __init__(self, params: ExecutionParams, trading_engine):
        self.params = params
        self.trading_engine = trading_engine
        self.remaining_quantity = params.total_quantity
        self.status = "idle"

    async def execute(self):
        """æ‰§è¡ŒTWAPç®—æ³•"""
        self.status = "running"

        # ç¡®å®šæ‰§è¡Œæ—¶é—´çª—å£
        start_time = self.params.start_time or datetime.now()
        end_time = self.params.end_time or (start_time + timedelta(minutes=30))

        total_duration = (end_time - start_time).total_seconds()
        if total_duration <= 0:
            logger.error("Invalid time window")
            self.status = "failed"
            return

        # è®¡ç®—åˆ‡ç‰‡æ•°é‡
        n_slices = max(10, int(total_duration / 60))  # è‡³å°‘10ä¸ªåˆ‡ç‰‡ï¼Œæ¯åˆ†é’Ÿä¸€ä¸ª
        quantity_per_slice = self.params.total_quantity / n_slices
        slice_duration = total_duration / n_slices

        logger.info(f"TWAP: Executing {self.params.total_quantity} "
                   f"in {n_slices} slices over {total_duration:.0f}s")

        start_time = datetime.now()

        for i in range(n_slices):
            if self.remaining_quantity <= 0:
                break

            # è®¡ç®—æœ¬æ¬¡æ‰§è¡Œæ•°é‡
            exec_quantity = min(quantity_per_slice, self.remaining_quantity)

            # è·å–å½“å‰å¸‚åœºä»·æ ¼
            current_price = await self._get_market_price()

            if current_price is None:
                logger.warning("Could not get market price, skipping slice")
                continue

            # æäº¤è®¢å•
            order = Order(
                symbol=self.params.symbol,
                side=self.params.side,
                order_type=OrderType.MARKET,
                quantity=exec_quantity,
                price=current_price
            )

            success = await self.trading_engine.submit_order(order)

            if success:
                filled = order.filled_quantity or 0
                self.remaining_quantity -= filled
                logger.info(f"TWAP slice {i+1}/{n_slices}: Filled {filled}")

            # ç­‰å¾…ä¸‹ä¸€ä¸ªåˆ‡ç‰‡
            if i < n_slices - 1:
                await asyncio.sleep(slice_duration)

        self.status = "completed"

    async def _get_market_price(self) -> Optional[float]:
        """è·å–å¸‚åœºä»·æ ¼"""
        # ä»äº¤æ˜“å¼•æ“è·å–å½“å‰ä»·æ ¼
        return await self.trading_engine.get_current_price(self.params.symbol)

class VWAPAlgorithm:
    """
    æˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼ç®—æ³•

    æ ¹æ®å†å²æˆäº¤é‡åˆ†å¸ƒæ‰§è¡Œè®¢å•
    """

    def __init__(self, params: ExecutionParams, trading_engine,
                 volume_profile: Optional[Dict[int, float]] = None):
        self.params = params
        self.trading_engine = trading_engine
        self.volume_profile = volume_profile  # {åˆ†é’Ÿ: å æ¯”}
        self.remaining_quantity = params.total_quantity
        self.status = "idle"

    async def execute(self):
        """æ‰§è¡ŒVWAPç®—æ³•"""
        self.status = "running"

        # å¦‚æœæ²¡æœ‰æä¾›æˆäº¤é‡åˆ†å¸ƒï¼Œä½¿ç”¨é»˜è®¤åˆ†å¸ƒ
        if not self.volume_profile:
            self.volume_profile = self._get_default_profile()

        # æŒ‰æ—¶é—´é¡ºåºæ‰§è¡Œ
        sorted_times = sorted(self.volume_profile.keys())

        for minute in sorted_times:
            if self.remaining_quantity <= 0:
                break

            # è®¡ç®—æœ¬æ¬¡æ‰§è¡Œæ•°é‡
            participation_rate = self.volume_profile[minute]
            target_quantity = self.params.total_quantity * participation_rate
            exec_quantity = min(target_quantity, self.remaining_quantity)

            # é™ä»·å•ï¼Œä½¿ç”¨ä¸­é—´ä»·
            mid_price = await self._get_mid_price()

            if mid_price is None:
                continue

            order = Order(
                symbol=self.params.symbol,
                side=self.params.side,
                order_type=OrderType.LIMIT,
                quantity=exec_quantity,
                price=mid_price
            )

            success = await self.trading_engine.submit_order(order)

            if success:
                filled = order.filled_quantity or 0
                self.remaining_quantity -= filled
                logger.info(f"VWAP minute {minute}: Filled {filled}/{exec_quantity}")

            # ç­‰å¾…ä¸‹ä¸€åˆ†é’Ÿ
            await asyncio.sleep(60)

        self.status = "completed"

    def _get_default_profile(self) -> Dict[int, float]:
        """è·å–é»˜è®¤æˆäº¤é‡åˆ†å¸ƒï¼ˆUå‹æ›²çº¿ï¼‰"""
        # æ¨¡æ‹Ÿä¸€å¤©ä¸­å‰30åˆ†é’Ÿçš„æˆäº¤é‡åˆ†å¸ƒ
        profile = {}
        for minute in range(30):
            # Uå‹æ›²çº¿ï¼šå¼€ç›˜å’Œæ”¶ç›˜æ—¶æˆäº¤é‡è¾ƒå¤§
            if minute < 10:
                profile[minute] = 0.05  # å¼€ç›˜
            elif minute > 20:
                profile[minute] = 0.05  # æ¥è¿‘æ”¶ç›˜
            else:
                profile[minute] = 0.02  # ä¸­é—´

        return profile

    async def _get_mid_price(self) -> Optional[float]:
        """è·å–ä¸­é—´ä»·"""
        return await self.trading_engine.get_mid_price(self.params.symbol)

class ImplementationShortfallAlgorithm:
    """
    æ‰§è¡Œ shortfall ç®—æ³•

    å¹³è¡¡å¸‚åœºå†²å‡»å’Œæœºä¼šæˆæœ¬
    """

    def __init__(self, params: ExecutionParams, trading_engine,
                 risk_aversion: float = 0.5):
        self.params = params
        self.trading_engine = trading_engine
        self.risk_aversion = risk_aversion  # 0-1ï¼Œè¶Šå¤§è¶Šä¿å®ˆ
        self.remaining_quantity = params.total_quantity
        self.status = "idle"

        # è®°å½•ä»·æ ¼å’Œæˆäº¤é‡
        self.price_history: List[float] = []
        self.volume_history: List[float] = []

    async def execute(self):
        """æ‰§è¡Œç®—æ³•"""
        self.status = "running"

        start_time = datetime.now()
        duration_minutes = 30

        for minute in range(duration_minutes):
            if self.remaining_quantity <= 0:
                break

            # è·å–å¸‚åœºæ•°æ®
            current_price = await self._get_market_price()
            market_volume = await self._get_market_volume()

            if current_price and market_volume:
                self.price_history.append(current_price)
                self.volume_history.append(market_volume)

            # è®¡ç®—æœ€ä¼˜æ‰§è¡Œé€Ÿåº¦
            execution_rate = self._calculate_execution_rate(minute, duration_minutes)

            # è®¡ç®—æœ¬æ¬¡æ‰§è¡Œæ•°é‡
            target_quantity = execution_rate * 60  # æ¯åˆ†é’Ÿæ•°é‡
            exec_quantity = min(target_quantity, self.remaining_quantity)

            # æ ¹æ®ç´§æ€¥ç¨‹åº¦è°ƒæ•´è®¢å•ç±»å‹
            if self.params.urgency == "high":
                order_type = OrderType.MARKET
            else:
                order_type = OrderType.LIMIT

            order = Order(
                symbol=self.params.symbol,
                side=self.params.side,
                order_type=order_type,
                quantity=exec_quantity,
                price=current_price or 0
            )

            success = await self.trading_engine.submit_order(order)

            if success:
                filled = order.filled_quantity or 0
                self.remaining_quantity -= filled

            await asyncio.sleep(60)

        self.status = "completed"

    def _calculate_execution_rate(self, current_minute: int,
                                  total_minutes: int) -> float:
        """è®¡ç®—æ‰§è¡Œé€Ÿç‡"""
        # ç®€åŒ–æ¨¡å‹ï¼šçº¿æ€§é€’å‡
        remaining_minutes = total_minutes - current_minute
        base_rate = self.remaining_quantity / remaining_minutes if remaining_minutes > 0 else 0

        # æ ¹æ®é£é™©åŒæ¶ç¨‹åº¦è°ƒæ•´
        if self.risk_aversion > 0.7:
            # ä¿å®ˆï¼šæ—©æœŸæ‰§è¡Œæ›´å¤š
            acceleration = 1.0 - (current_minute / total_minutes) * 0.5
        elif self.risk_aversion < 0.3:
            # æ¿€è¿›ï¼šåæœŸæ‰§è¡Œæ›´å¤š
            acceleration = 0.5 + (current_minute / total_minutes) * 0.5
        else:
            acceleration = 1.0

        return base_rate * acceleration

    async def _get_market_price(self) -> Optional[float]:
        """è·å–å¸‚åœºä»·æ ¼"""
        return await self.trading_engine.get_current_price(self.params.symbol)

    async def _get_market_volume(self) -> Optional[float]:
        """è·å–å¸‚åœºæˆäº¤é‡"""
        return await self.trading_engine.get_market_volume(self.params.symbol)
```

### 18.3 æ™ºèƒ½è·¯ç”±å™¨

```python
# src/execution/smart_router.py

from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import asyncio
import logging

logger = logging.getLogger(__name__)

@dataclass
class ExchangeQuote:
    """äº¤æ˜“æ‰€æŠ¥ä»·"""
    exchange: str
    price: float
    available_quantity: float
    fee_rate: float

class SmartOrderRouter:
    """æ™ºèƒ½è®¢å•è·¯ç”±å™¨"""

    def __init__(self, exchanges: List[str]):
        self.exchanges = exchanges
        self.fee_table: Dict[str, float] = {}
        self.liquidity_cache: Dict[str, Dict[str, float]] = {}

    async def route_order(self, symbol: str, side: str,
                         quantity: float) -> List[Tuple[str, float]]:
        """
        è·¯ç”±è®¢å•åˆ°æœ€ä¼˜äº¤æ˜“æ‰€

        Returns:
            [(exchange, quantity), ...]
        """
        # è·å–æ‰€æœ‰äº¤æ˜“æ‰€çš„æŠ¥ä»·
        quotes = await self._get_quotes(symbol, side)

        if not quotes:
            logger.warning(f"No quotes available for {symbol}")
            return []

        # æŒ‰æœ‰æ•ˆä»·æ ¼æ’åºï¼ˆè€ƒè™‘æ‰‹ç»­è´¹ï¼‰
        sorted_quotes = sorted(
            quotes,
            key=lambda q: self._calculate_effective_price(q, side)
        )

        # åˆ†é…è®¢å•åˆ°æœ€ä¼˜äº¤æ˜“æ‰€
        routing = []
        remaining_quantity = quantity

        for quote in sorted_quotes:
            if remaining_quantity <= 0:
                break

            exec_quantity = min(quote.available_quantity, remaining_quantity)
            routing.append((quote.exchange, exec_quantity))
            remaining_quantity -= exec_quantity

        logger.info(f"Routed {quantity} of {symbol}: {routing}")
        return routing

    async def _get_quotes(self, symbol: str,
                         side: str) -> List[ExchangeQuote]:
        """è·å–æ‰€æœ‰äº¤æ˜“æ‰€æŠ¥ä»·"""
        quotes = []

        for exchange in self.exchanges:
            try:
                quote = await self._get_exchange_quote(exchange, symbol, side)
                if quote:
                    quotes.append(quote)
            except Exception as e:
                logger.error(f"Error getting quote from {exchange}: {e}")

        return quotes

    async def _get_exchange_quote(self, exchange: str, symbol: str,
                                  side: str) -> Optional[ExchangeQuote]:
        """è·å–äº¤æ˜“æ‰€æŠ¥ä»·"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„äº¤æ˜“æ‰€API
        # ç®€åŒ–å®ç°ï¼šè¿”å›æ¨¡æ‹Ÿæ•°æ®
        return ExchangeQuote(
            exchange=exchange,
            price=100.0,  # æ¨¡æ‹Ÿä»·æ ¼
            available_quantity=1000.0,  # æ¨¡æ‹Ÿå¯ç”¨æ•°é‡
            fee_rate=self.fee_table.get(exchange, 0.001)
        )

    def _calculate_effective_price(self, quote: ExchangeQuote,
                                  side: str) -> float:
        """è®¡ç®—æœ‰æ•ˆä»·æ ¼ï¼ˆè€ƒè™‘æ‰‹ç»­è´¹ï¼‰"""
        if side == 'buy':
            return quote.price * (1 + quote.fee_rate)
        else:
            return quote.price * (1 - quote.fee_rate)

    def update_fee_table(self, fee_table: Dict[str, float]):
        """æ›´æ–°è´¹ç‡è¡¨"""
        self.fee_table.update(fee_table)

    async def find_best_execution(self, symbol: str, side: str,
                                 quantity: float) -> Dict[str, Any]:
        """å¯»æ‰¾æœ€ä¼˜æ‰§è¡Œæ–¹æ¡ˆ"""
        routing = await self.route_order(symbol, side, quantity)

        if not routing:
            return {
                'success': False,
                'reason': 'No quotes available'
            }

        # è®¡ç®—é¢„æœŸæˆæœ¬
        total_cost = 0
        weighted_price = 0

        for exchange, qty in routing:
            quote = await self._get_exchange_quote(exchange, symbol, side)
            if quote:
                cost = quote.price * qty * (1 + quote.fee_rate)
                total_cost += cost
                weighted_price += quote.price * qty / quantity

        return {
            'success': True,
            'routing': routing,
            'total_cost': total_cost,
            'average_price': weighted_price,
            'slippage_estimate': self._estimate_slippage(symbol, quantity)
        }

    def _estimate_slippage(self, symbol: str, quantity: float) -> float:
        """ä¼°ç®—æ»‘ç‚¹"""
        # ç®€åŒ–æ¨¡å‹ï¼šåŸºäºè®¢å•å¤§å°ä¼°ç®—æ»‘ç‚¹
        base_slippage = 0.001  # 0.1%
        volume_factor = min(quantity / 10000, 1.0)  # æ•°é‡è¶Šå¤§æ»‘ç‚¹è¶Šå¤§
        return base_slippage * (1 + volume_factor)
```

### 18.4 è®¢å•ç®¡ç†å™¨

```python
# src/execution/order_manager.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import asyncio
import logging

logger = logging.getLogger(__name__)

class OrderState(Enum):
    """è®¢å•çŠ¶æ€"""
    PENDING = "pending"
    SUBMITTED = "submitted"
    PARTIAL_FILLED = "partial_filled"
    FILLED = "filled"
    CANCELLED = "cancelled"
    REJECTED = "rejected"
    EXPIRED = "expired"

@dataclass
class OrderMetadata:
    """è®¢å•å…ƒæ•°æ®"""
    order_id: str
    symbol: str
    side: str
    quantity: float
    price: Optional[float]
    order_type: str
    state: OrderState
    filled_quantity: float = 0
    avg_fill_price: float = 0
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    parent_order_id: Optional[str] = None  # çˆ¶è®¢å•IDï¼ˆç”¨äºå­è®¢å•ï¼‰

class OrderManager:
    """è®¢å•ç®¡ç†å™¨"""

    def __init__(self, trading_engine):
        self.trading_engine = trading_engine
        self.orders: Dict[str, OrderMetadata] = {}
        self.child_orders: Dict[str, List[str]] = {}  # çˆ¶è®¢å•ID -> å­è®¢å•IDåˆ—è¡¨
        self.lock = asyncio.Lock()

    async def submit_order(self, order: Order,
                          parent_order_id: Optional[str] = None) -> str:
        """æäº¤è®¢å•"""
        async with self.lock:
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = OrderMetadata(
                order_id=order.order_id,
                symbol=order.symbol,
                side=order.side,
                quantity=order.quantity,
                price=order.price,
                order_type=order.order_type.value,
                state=OrderState.PENDING,
                parent_order_id=parent_order_id
            )

            self.orders[order.order_id] = metadata

            if parent_order_id:
                if parent_order_id not in self.child_orders:
                    self.child_orders[parent_order_id] = []
                self.child_orders[parent_order_id].append(order.order_id)

            # æäº¤åˆ°äº¤æ˜“å¼•æ“
            success = await self.trading_engine.submit_order(order)

            if success:
                metadata.state = OrderState.SUBMITTED
                logger.info(f"Order {order.order_id} submitted successfully")
            else:
                metadata.state = OrderState.REJECTED
                logger.error(f"Order {order.order_id} rejected")

            metadata.updated_at = datetime.now()

            return order.order_id

    async def cancel_order(self, order_id: str) -> bool:
        """å–æ¶ˆè®¢å•"""
        async with self.lock:
            if order_id not in self.orders:
                logger.warning(f"Order {order_id} not found")
                return False

            metadata = self.orders[order_id]

            # å–æ¶ˆæ‰€æœ‰å­è®¢å•
            if order_id in self.child_orders:
                for child_id in self.child_orders[order_id]:
                    await self._cancel_single_order(child_id)

            # å–æ¶ˆä¸»è®¢å•
            result = await self._cancel_single_order(order_id)

            if result:
                metadata.state = OrderState.CANCELLED
                metadata.updated_at = datetime.now()

            return result

    async def _cancel_single_order(self, order_id: str) -> bool:
        """å–æ¶ˆå•ä¸ªè®¢å•"""
        if order_id not in self.orders:
            return False

        metadata = self.orders[order_id]

        if metadata.state in [OrderState.FILLED, OrderState.CANCELLED,
                              OrderState.REJECTED, OrderState.EXPIRED]:
            return False

        # è°ƒç”¨äº¤æ˜“å¼•æ“å–æ¶ˆè®¢å•
        success = await self.trading_engine.cancel_order(order_id)

        if success:
            metadata.state = OrderState.CANCELLED
            metadata.updated_at = datetime.now()

        return success

    async def update_order_status(self, order_id: str, filled_quantity: float,
                                 fill_price: float):
        """æ›´æ–°è®¢å•çŠ¶æ€"""
        async with self.lock:
            if order_id not in self.orders:
                return

            metadata = self.orders[order_id]
            metadata.filled_quantity = filled_quantity

            if fill_price > 0:
                # æ›´æ–°å¹³å‡æˆäº¤ä»·
                total_value = metadata.avg_fill_price * (metadata.filled_quantity - filled_quantity)
                total_value += fill_price * filled_quantity
                metadata.avg_fill_price = total_value / metadata.filled_quantity

            # æ›´æ–°çŠ¶æ€
            if filled_quantity >= metadata.quantity:
                metadata.state = OrderState.FILLED
            elif filled_quantity > 0:
                metadata.state = OrderState.PARTIAL_FILLED

            metadata.updated_at = datetime.now()

    def get_order(self, order_id: str) -> Optional[OrderMetadata]:
        """è·å–è®¢å•"""
        return self.orders.get(order_id)

    def get_orders_by_symbol(self, symbol: str) -> List[OrderMetadata]:
        """æŒ‰è‚¡ç¥¨ä»£ç è·å–è®¢å•"""
        return [o for o in self.orders.values() if o.symbol == symbol]

    def get_active_orders(self) -> List[OrderMetadata]:
        """è·å–æ´»è·ƒè®¢å•"""
        return [
            o for o in self.orders.values()
            if o.state in [OrderState.SUBMITTED, OrderState.PARTIAL_FILLED]
        ]

    def get_child_orders(self, parent_order_id: str) -> List[OrderMetadata]:
        """è·å–å­è®¢å•"""
        if parent_order_id not in self.child_orders:
            return []

        return [
            self.orders[child_id]
            for child_id in self.child_orders[parent_order_id]
            if child_id in self.orders
        ]

    async def cleanup_old_orders(self, days: int = 7):
        """æ¸…ç†æ—§è®¢å•"""
        cutoff_date = datetime.now() - timedelta(days=days)

        async with self.lock:
            to_remove = []

            for order_id, metadata in self.orders.items():
                if metadata.updated_at < cutoff_date:
                    if metadata.state in [OrderState.FILLED, OrderState.CANCELLED,
                                         OrderState.REJECTED, OrderState.EXPIRED]:
                        to_remove.append(order_id)

            for order_id in to_remove:
                del self.orders[order_id]
                if order_id in self.child_orders:
                    del self.child_orders[order_id]

            logger.info(f"Cleaned up {len(to_remove)} old orders")
```

### 18.5 æ‰§è¡Œè´¨é‡åˆ†æ

```python
# src/execution/quality_analyzer.py

from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
import numpy as np
import logging

logger = logging.getLogger(__name__)

@dataclass
class ExecutionMetrics:
    """æ‰§è¡Œè´¨é‡æŒ‡æ ‡"""
    order_id: str
    symbol: str
    side: str
    target_quantity: float
    filled_quantity: float
    fill_rate: float
    avg_price: float
    benchmark_price: float
    slippage_bps: float  # åŸºç‚¹
    implementation_shortfall: float
    execution_time_seconds: float
    cost: float

class ExecutionQualityAnalyzer:
    """æ‰§è¡Œè´¨é‡åˆ†æå™¨"""

    def __init__(self):
        self.executions: List[ExecutionMetrics] = []

    def analyze_execution(self, order_metadata: OrderMetadata,
                         benchmark_price: float,
                         execution_time: float,
                         market_data: pd.DataFrame) -> ExecutionMetrics:
        """åˆ†ææ‰§è¡Œè´¨é‡"""
        # å¡«å……ç‡
        fill_rate = order_metadata.filled_quantity / order_metadata.quantity

        # æ»‘ç‚¹ï¼ˆåŸºç‚¹ï¼‰
        if order_metadata.side == 'buy':
            slippage = (order_metadata.avg_fill_price - benchmark_price) / benchmark_price * 10000
        else:
            slippage = (benchmark_price - order_metadata.avg_fill_price) / benchmark_price * 10000

        # å®ç° shortfall
        target_value = benchmark_price * order_metadata.quantity
        actual_value = order_metadata.avg_fill_price * order_metadata.filled_quantity
        implementation_shortfall = (actual_value - target_value) / target_value

        # è®¡ç®—æˆæœ¬
        cost = actual_value if order_metadata.side == 'buy' else -actual_value

        metrics = ExecutionMetrics(
            order_id=order_metadata.order_id,
            symbol=order_metadata.symbol,
            side=order_metadata.side,
            target_quantity=order_metadata.quantity,
            filled_quantity=order_metadata.filled_quantity,
            fill_rate=fill_rate,
            avg_price=order_metadata.avg_fill_price,
            benchmark_price=benchmark_price,
            slippage_bps=slippage,
            implementation_shortfall=implementation_shortfall,
            execution_time_seconds=execution_time,
            cost=cost
        )

        self.executions.append(metrics)
        return metrics

    def get_performance_summary(self, symbol: Optional[str] = None,
                               days: int = 30) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ±‡æ€»"""
        # è¿‡æ»¤æ•°æ®
        if symbol:
            executions = [e for e in self.executions if e.symbol == symbol]
        else:
            executions = self.executions

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        if not executions:
            return {}

        fill_rates = [e.fill_rate for e in executions]
        slippages = [e.slippage_bps for e in executions]
        shortfalls = [e.implementation_shortfall for e in executions]
        execution_times = [e.execution_time_seconds for e in executions]

        return {
            'total_executions': len(executions),
            'avg_fill_rate': np.mean(fill_rates),
            'median_slippage_bps': np.median(slippages),
            'avg_slippage_bps': np.mean(slippages),
            'avg_shortfall': np.mean(shortfalls),
            'avg_execution_time': np.mean(execution_times),
            'total_cost': sum(e.cost for e in executions),
            'best_execution': max(executions, key=lambda x: x.slippage_bps),
            'worst_execution': min(executions, key=lambda x: x.slippage_bps)
        }

    def get_slippage_distribution(self, symbol: Optional[str] = None) -> Dict[str, float]:
        """è·å–æ»‘ç‚¹åˆ†å¸ƒ"""
        if symbol:
            executions = [e for e in self.executions if e.symbol == symbol]
        else:
            executions = self.executions

        slippages = [e.slippage_bps for e in executions]

        return {
            'min': min(slippages) if slippages else 0,
            'max': max(slippages) if slippages else 0,
            'mean': np.mean(slippages) if slippages else 0,
            'median': np.median(slippages) if slippages else 0,
            'std': np.std(slippages) if slippages else 0,
            'p25': np.percentile(slippages, 25) if slippages else 0,
            'p75': np.percentile(slippages, 75) if slippages else 0
        }

    def compare_to_benchmark(self, executions: List[ExecutionMetrics],
                            benchmark_name: str) -> Dict[str, Any]:
        """ä¸åŸºå‡†æ¯”è¾ƒ"""
        total_slippage = sum(e.slippage_bps for e in executions)
        total_cost = sum(e.cost for e in executions)

        return {
            'benchmark': benchmark_name,
            'avg_slippage_vs_benchmark': total_slippage / len(executions),
            'total_cost_vs_benchmark': total_cost,
            'fill_rate': sum(e.fill_rate for e in executions) / len(executions)
        }
```

---

## æ–‡æ¡£æ€»ç»“

æœ¬æ–‡æ¡£ **NOFX Python å®æˆ˜éƒ¨ç½²æŒ‡å—** æä¾›äº†å®Œæ•´çš„äº¤æ˜“ç³»ç»Ÿå®ç°å’Œéƒ¨ç½²æ–¹æ¡ˆï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

### å·²å®Œæˆç« èŠ‚ï¼ˆå…±18ç« ï¼‰

| ç« èŠ‚ | å†…å®¹ | ä»£ç è¡Œæ•° |
|------|------|----------|
| ç¬¬1ç«  | ç³»ç»Ÿæ¶æ„è®¾è®¡ | ~200 |
| ç¬¬2ç«  | æ ¸å¿ƒä»£ç å®ç°ï¼ˆäº¤æ˜“å¼•æ“ã€è®¢å•ç®¡ç†ï¼‰ | ~370 |
| ç¬¬3ç«  | æ•°æ®è¿æ¥å™¨ï¼ˆAè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸ï¼‰ | ~160 |
| ç¬¬4ç«  | éƒ¨ç½²é…ç½®ï¼ˆDockerã€Kubernetesï¼‰ | ~200 |
| ç¬¬5ç«  | å¯åŠ¨è„šæœ¬ä¸é…ç½® | ~150 |
| ç¬¬6ç«  | é£é™©ç®¡ç†ç³»ç»Ÿ | ~260 |
| ç¬¬7ç«  | äº¤æ˜“ç­–ç•¥å®ç° | ~210 |
| ç¬¬8ç«  | ç›‘æ§å’Œå‘Šè­¦ï¼ˆPrometheusã€Grafanaï¼‰ | ~170 |
| ç¬¬9ç«  | APIæ¥å£ï¼ˆRESTã€WebSocketï¼‰ | ~200 |
| ç¬¬10ç«  | æµ‹è¯•æ¡†æ¶ | ~100 |
| ç¬¬11ç«  | å›æµ‹æ¡†æ¶ | ~850 |
| ç¬¬12ç«  | æ€§èƒ½ä¼˜åŒ– | ~460 |
| ç¬¬13ç«  | å®‰å…¨æœ€ä½³å®è·µ | ~600 |
| ç¬¬14ç«  | CI/CDæµæ°´çº¿ | ~680 |
| ç¬¬15ç«  | é«˜çº§äº¤æ˜“ç­–ç•¥ï¼ˆåŠ¨é‡ã€å¥—åˆ©ã€åšå¸‚ã€å› å­ã€ç½‘æ ¼ï¼‰ | ~700 |
| ç¬¬16ç«  | æœºå™¨å­¦ä¹ é›†æˆï¼ˆLSTMã€éšæœºæ£®æ—ã€å¼ºåŒ–å­¦ä¹ ï¼‰ | ~900 |
| ç¬¬17ç«  | å®æ—¶æ•°æ®å¤„ç†ç®¡é“ | ~600 |
| ç¬¬18ç«  | é«˜çº§è®¢å•ç±»å‹å’Œæ‰§è¡Œç®—æ³• | ~650 |

### æ–‡æ¡£ç»Ÿè®¡

- **æ€»è¡Œæ•°**: çº¦ 8,360 è¡Œ
- **ä»£ç æ–‡ä»¶**: è¶…è¿‡ 100 ä¸ª
- **æ”¯æŒå¸‚åœº**: Aè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸
- **çŠ¶æ€**: **ç”Ÿäº§å°±ç»ª** (Production Ready)

### å¿«é€Ÿå¼€å§‹

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/nofx-trading.git
cd nofx-trading

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. é…ç½®ç¯å¢ƒ
cp config/config.example.yml config/config.yml
# ç¼–è¾‘ config/config.yml

# 4. è¿è¡Œæµ‹è¯•
pytest tests/ -v

# 5. å¯åŠ¨ç³»ç»Ÿ
python -m src.main

# 6. è®¿é—®API
curl http://localhost:8000/health
```

### Dockeréƒ¨ç½²

```bash
# æ„å»ºå¹¶å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢
docker-compose down
```

### æŠ€æœ¯æ ˆ

**æ ¸å¿ƒæ¡†æ¶**
- Python 3.11+
- asyncio (å¼‚æ­¥ç¼–ç¨‹)
- FastAPI (APIæ¡†æ¶)
- uvicorn (ASGIæœåŠ¡å™¨)

**æ•°æ®å¤„ç†**
- pandas, numpy (æ•°æ®åˆ†æ)
- asyncpg (PostgreSQLå¼‚æ­¥é©±åŠ¨)
- redis-py (Rediså®¢æˆ·ç«¯)
- aiokafka (Kafkaå¼‚æ­¥å®¢æˆ·ç«¯)

**æœºå™¨å­¦ä¹ **
- PyTorch (æ·±åº¦å­¦ä¹ )
- scikit-learn (ä¼ ç»Ÿæœºå™¨å­¦ä¹ )
- river (åœ¨çº¿å­¦ä¹ )

**éƒ¨ç½²è¿ç»´**
- Docker (å®¹å™¨åŒ–)
- Kubernetes (ç¼–æ’)
- Prometheus (ç›‘æ§)
- Grafana (å¯è§†åŒ–)
- GitHub Actions/GitLab CI (CI/CD)

---

**æ–‡æ¡£çŠ¶æ€: ç”Ÿäº§å°±ç»ª**
**æœ€åæ›´æ–°: 2026**
**é€‚ç”¨å¸‚åœº: Aè‚¡ã€æ¸¯è‚¡ã€åŠ å¯†è´§å¸**
**æŠ€æœ¯æ ˆ: Python 3.11+, asyncio, FastAPI, PostgreSQL, Redis, Docker, Kubernetes**
