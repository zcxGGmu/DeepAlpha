# NOFX Pythoné‡æ„æŠ€æœ¯æ–¹æ¡ˆ - éƒ¨ç½²è¿ç»´è¡¥å……ç¯‡

> æœ¬æ–‡æ¡£æ˜¯ã€ŠNOFX_Python_é‡æ„æŠ€æœ¯æ–¹æ¡ˆ_Aè‚¡æ¸¯è‚¡ã€‹ç³»åˆ—çš„ç¬¬å…­éƒ¨åˆ†
> è¦†ç›–ç¬¬21-25ç« ï¼šCI/CDã€å®¹å™¨åŒ–éƒ¨ç½²ã€æ•°æ®è¿ç§»ã€å‰ç«¯é«˜çº§æ¨¡å¼ã€ç”Ÿäº§è¿ç»´

---

## ç¬¬21ç«  CI/CDæµæ°´çº¿é…ç½®

### 21.1 GitHub Actionså®Œæ•´é…ç½®

#### 21.1.1 ä¸»æµæ°´çº¿é…ç½®

```yaml
# .github/workflows/ci-cd-pipeline.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop, 'feature/**']
  pull_request:
    branches: [main, develop]
  release:
    types: [published]
  workflow_dispatch:
    inputs:
      environment:
        description: 'éƒ¨ç½²ç¯å¢ƒ'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # ========================================
  # ä»£ç è´¨é‡æ£€æŸ¥
  # ========================================
  code-quality:
    name: Code Quality Check
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # å®Œæ•´å†å²ç”¨äºSonarQube

      - name: è®¾ç½®Pythonç¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          python -m pip install --upgrade pip
          pip install ruff black isort mypy pylint bandit
          pip install -r requirements-dev.txt

      - name: Ruffä»£ç æ£€æŸ¥
        run: |
          ruff check . --output-format=github
        continue-on-error: false

      - name: Blackæ ¼å¼æ£€æŸ¥
        run: |
          black --check --diff .
        continue-on-error: false

      - name: isortå¯¼å…¥æ’åºæ£€æŸ¥
        run: |
          isort --check-only --diff .
        continue-on-error: false

      - name: MyPyç±»å‹æ£€æŸ¥
        run: |
          mypy src/ --config-file pyproject.toml
        continue-on-error: true

      - name: Pylintä»£ç è¯„åˆ†
        run: |
          pylint src/ --rcfile .pylintrc --fail-under=8.0
        continue-on-error: true

      - name: Banditå®‰å…¨æ‰«æ
        run: |
          bandit -r src/ -f json -o bandit-report.json
        continue-on-error: true

      - name: ä¸Šä¼ BanditæŠ¥å‘Š
        uses: actions/upload-artifact@v4
        with:
          name: bandit-security-report
          path: bandit-report.json
          retention-days: 30

  # ========================================
  # å•å…ƒæµ‹è¯•
  # ========================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality
    timeout-minutes: 20

    strategy:
      matrix:
        python-version: ['3.11', '3.12']
      fail-fast: false

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: è¿è¡Œå•å…ƒæµ‹è¯•ï¼ˆå¸¦è¦†ç›–ç‡ï¼‰
        run: |
          pytest tests/unit/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junitxml=test-results.xml \
            -v \
            --tb=short
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0

      - name: ä¸Šä¼ è¦†ç›–ç‡åˆ°Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unit-tests
          name: codecov-${{ matrix.python-version }}
          fail_ci_if_error: false

      - name: ä¸Šä¼ æµ‹è¯•æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        with:
          name: test-results-py${{ matrix.python-version }}
          path: |
            test-results.xml
            htmlcov/
            .coverage
          retention-days: 30

      - name: è¦†ç›–ç‡æ£€æŸ¥
        run: |
          coverage report --fail-under=80

  # ========================================
  # é›†æˆæµ‹è¯•
  # ========================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    timeout-minutes: 30

    services:
      postgres:
        image: postgres:16-alpine
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®Pythonç¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: æ•°æ®åº“è¿ç§»
        run: |
          alembic upgrade head
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db

      - name: è¿è¡Œé›†æˆæµ‹è¯•
        run: |
          pytest tests/integration/ \
            --cov=src \
            --cov-report=xml \
            --cov-append \
            --junitxml=integration-test-results.xml \
            -v \
            --tb=short
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379/0

      - name: ä¸Šä¼ é›†æˆæµ‹è¯•æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: integration-test-results.xml
          retention-days: 30

  # ========================================
  # ç«¯åˆ°ç«¯æµ‹è¯•
  # ========================================
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    timeout-minutes: 45

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®Pythonç¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: è®¾ç½®Node.jsç¯å¢ƒ
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: å®‰è£…Pythonä¾èµ–
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: å®‰è£…å‰ç«¯ä¾èµ–
        run: |
          cd frontend
          npm ci

      - name: æ„å»ºå‰ç«¯
        run: |
          cd frontend
          npm run build

      - name: å¯åŠ¨æµ‹è¯•æœåŠ¡å™¨
        run: |
          uvicorn src.main:app --host 0.0.0.0 --port 8000 &
          sleep 10
        env:
          DATABASE_URL: sqlite:///./test.db
          REDIS_URL: redis://localhost:6379/0

      - name: å®‰è£…Playwright
        run: |
          npm init -y
          npm install -D @playwright/test
          npx playwright install --with-deps

      - name: è¿è¡ŒE2Eæµ‹è¯•
        run: |
          npx playwright test
        working-directory: tests/e2e

      - name: ä¸Šä¼ PlaywrightæŠ¥å‘Š
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: playwright-report
          path: tests/e2e/playwright-report/
          retention-days: 30

  # ========================================
  # æ€§èƒ½æµ‹è¯•
  # ========================================
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    timeout-minutes: 60

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®Pythonç¯å¢ƒ
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          pip install -r requirements.txt
          pip install locust pytest-benchmark

      - name: è¿è¡Œè´Ÿè½½æµ‹è¯•
        run: |
          locust -f tests/performance/locustfile.py \
            --headless \
            --users 100 \
            --spawn-rate 10 \
            --run-time 5m \
            --host http://localhost:8000 \
            --html performance-report.html \
            --csv performance
        continue-on-error: true

      - name: è¿è¡ŒåŸºå‡†æµ‹è¯•
        run: |
          pytest tests/performance/benchmarks.py \
            --benchmark-only \
            --benchmark-json=benchmark-results.json
        continue-on-error: true

      - name: ä¸Šä¼ æ€§èƒ½æµ‹è¯•æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        with:
          name: performance-reports
          path: |
            performance-report.html
            benchmark-results.json
          retention-days: 30

  # ========================================
  # å®‰å…¨æ‰«æ
  # ========================================
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: code-quality
    timeout-minutes: 15

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è¿è¡ŒTrivyæ¼æ´æ‰«æ
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'

      - name: ä¾èµ–å®‰å…¨æ£€æŸ¥
        run: |
          pip install safety
          safety check --json > safety-report.json || true
        continue-on-error: true

      - name: ä»£ç æ¼æ´æ‰«æ
        uses: github/codeql-action/analyze@v3
        with:
          languages: python, javascript
          category: "/language:python"

      - name: ä¸Šä¼ å®‰å…¨æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        with:
          name: security-reports
          path: |
            trivy-results.sarif
            safety-report.json
          retention-days: 90

  # ========================================
  # æ„å»ºDockeré•œåƒ
  # ========================================
  build-image:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-scan]
    if: github.event_name != 'pull_request'
    timeout-minutes: 30
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: ç™»å½•åˆ°Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: æå–é•œåƒå…ƒæ•°æ®
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: æ„å»ºå¹¶æ¨é€é•œåƒ
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILD_DATE=${{ github.event.head_commit.timestamp }}
            VCS_REF=${{ github.sha }}
            VERSION=${{ steps.meta.outputs.version }}

  # ========================================
  # éƒ¨ç½²åˆ°Staging
  # ========================================
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build-image, e2e-tests]
    if: github.ref == 'refs/heads/develop'
    environment:
      name: staging
      url: https://staging.deepalpha.example.com

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: é…ç½®Kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING }}" | base64 -d > ~/.kube/config

      - name: éƒ¨ç½²åˆ°Kubernetes
        run: |
          kubectl set image deployment/deepalpha-api \
            deepalpha-api=${{ needs.build-image.outputs.image-tag }} \
            -n deepalpha-staging

          kubectl rollout status deployment/deepalpha-api \
            -n deepalpha-staging \
            --timeout=5m

      - name: è¿è¡Œæ•°æ®åº“è¿ç§»
        run: |
          kubectl exec -n deepalpha-staging \
            deployment/deepalpha-api \
            -- alembic upgrade head

      - name: å¥åº·æ£€æŸ¥
        run: |
          kubectl wait --for=condition=ready pod \
            -l app=deepalpha-api \
            -n deepalpha-staging \
            --timeout=60s

          curl -f https://staging.deepalpha.example.com/health || exit 1

      - name: é€šçŸ¥Slack
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "ğŸš€ éƒ¨ç½²æˆåŠŸ",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*DeepAlphaå·²éƒ¨ç½²åˆ°Stagingç¯å¢ƒ*\nâ€¢ åˆ†æ”¯: `${{ github.ref }}`\nâ€¢ æäº¤: `${{ github.sha }}`\nâ€¢ ä½œè€…: `${{ github.actor }}`\nâ€¢ é•œåƒ: `${{ needs.build-image.outputs.image-tag }}`"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}

  # ========================================
  # éƒ¨ç½²åˆ°Production
  # ========================================
  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [build-image, e2e-tests]
    if: github.event_name == 'release'
    environment:
      name: production
      url: https://deepalpha.example.com

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½®kubectl
        uses: azure/setup-kubectl@v4

      - name: é…ç½®Kubeconfig
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG_PRODUCTION }}" | base64 -d > ~/.kube/config

      - name: åˆ›å»ºGitæ ‡ç­¾
        run: |
          git tag -a v${{ github.event.release.tag_name }} -m "Release v${{ github.event.release.tag_name }}"
          git push origin v${{ github.event.release.tag_name }}

      - name: è“ç»¿éƒ¨ç½² - åˆ‡æ¢æµé‡
        run: |
          # éƒ¨ç½²åˆ°Greenç¯å¢ƒ
          helm upgrade --install deepalpha-green ./helm/deepalpha \
            --namespace deepalpha-production \
            --set image.tag=${{ github.event.release.tag_name }} \
            --set environment=production \
            --values helm/deepalpha/values-production.yaml \
            --wait \
            --timeout 10m

          # å¥åº·æ£€æŸ¥
          kubectl wait --for=condition=ready pod \
            -l app=deepalpha,environment=green \
            -n deepalpha-production \
            --timeout=120s

          # é‡‘ä¸é›€å‘å¸ƒï¼š10%æµé‡åˆ°Green
          kubectl patch service deepalpha-api \
            -n deepalpha-production \
            -p '{"spec":{"selector":{"version":"green"}}}' \
            --type=merge

          sleep 60  # è§‚å¯ŸæœŸ

          # 100%æµé‡åˆ°Green
          kubectl patch service deepalpha-api \
            -n deepalpha-production \
            -p '{"spec":{"selector":{"version":"green"}}}' \
            --type=merge

          # æ¸…ç†Blueç¯å¢ƒ
          helm uninstall deepalpha-blue -n deepalpha-production || true

      - name: è¿è¡Œæ•°æ®åº“è¿ç§»
        run: |
          kubectl exec -n deepalpha-production \
            deployment/deepalpha-api \
            -- alembic upgrade head

      - name: éªŒè¯éƒ¨ç½²
        run: |
          # APIå¥åº·æ£€æŸ¥
          curl -f https://deepalpha.example.com/health || exit 1

          # å…³é”®ç«¯ç‚¹æ£€æŸ¥
          curl -f https://deepalpha.example.com/api/v1/traders || exit 1

          # ç›‘æ§æŒ‡æ ‡æ£€æŸ¥
          curl -f https://deepalpha.example.com/metrics || exit 1

      - name: åˆ›å»ºéƒ¨ç½²å›æ»šä»»åŠ¡
        if: failure()
        run: |
          echo "éƒ¨ç½²å¤±è´¥ï¼Œè§¦å‘å›æ»šæµç¨‹"
          # è‡ªåŠ¨å›æ»šåˆ°ä¸Šä¸€ä¸ªç¨³å®šç‰ˆæœ¬
          helm rollback deepalpha-green -n deepalpha-production

      - name: é€šçŸ¥ç”Ÿäº§éƒ¨ç½²
        if: success()
        uses: slackapi/slack-github-action@v1
        with:
          payload: |
            {
              "text": "ğŸ‰ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æˆåŠŸ",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*DeepAlpha v${{ github.event.release.tag_name }}å·²éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ*\nâ€¢ å‘å¸ƒ: <${{ github.event.release.html_url }}|${{ github.event.release.name }}>\nâ€¢ æäº¤: `${{ github.sha }}`\nâ€¢ ä½œè€…: `${{ github.actor }}`"
                  }
                }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_PRODUCTION }}

  # ========================================
  # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
  # ========================================
  test-report:
    name: Generate Test Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests]
    if: always()

    steps:
      - name: ä¸‹è½½æ‰€æœ‰æµ‹è¯•æŠ¥å‘Š
        uses: actions/download-artifact@v4

      - name: å‘å¸ƒæµ‹è¯•æŠ¥å‘Š
        uses: mikepenz/action-junit-report@v4
        with:
          report_paths: '**/test-results.xml'
          check_name: æµ‹è¯•ç»“æœæ±‡æ€»
          detailed_summary: true
          include_passed: true

  # ========================================
  # å‘å¸ƒGitHub Release
  # ========================================
  release:
    name: Create Release
    runs-on: ubuntu-latest
    needs: [build-image]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    permissions:
      contents: write

    steps:
      - name: Checkoutä»£ç 
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ç”Ÿæˆå˜æ›´æ—¥å¿—
        id: changelog
        uses: conventional-changelog/conventional-changelog-action@v5
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          output-file: 'CHANGELOG.md'

      - name: åˆ›å»ºRelease
        uses: softprops/action-gh-release@v1
        with:
          body_path: CHANGELOG.md
          draft: false
          prerelease: false
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

#### 21.1.2 Dockerfileå¤šé˜¶æ®µæ„å»º

```dockerfile
# Dockerfile
# å¤šé˜¶æ®µæ„å»ºï¼Œä¼˜åŒ–é•œåƒå¤§å°å’Œå®‰å…¨æ€§

# ============================================
# Stage 1: Base Builder
# ============================================
FROM python:3.11-slim as base-builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /build

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    libpq-dev \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶requirementsæ–‡ä»¶
COPY requirements.txt requirements-dev.txt ./

# ============================================
# Stage 2: Dependencies Builder
# ============================================
FROM base-builder as dependencies-builder

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# å‡çº§pipå’Œå®‰è£…æ„å»ºå·¥å…·
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# å®‰è£…ç”Ÿäº§ä¾èµ–ï¼ˆåˆ†ç¦»ç¼–è¯‘å’Œè¿è¡Œæ—¶ä¾èµ–ï¼‰
RUN pip install --no-cache-dir --no-deps -r requirements.txt

# ============================================
# Stage 3: Test Runner
# ============================================
FROM dependencies-builder as test-runner

# å®‰è£…æµ‹è¯•ä¾èµ–
RUN pip install --no-cache-dir -r requirements-dev.txt

# å¤åˆ¶æºä»£ç 
COPY . .

# è¿è¡Œæµ‹è¯•
RUN pytest tests/unit/ --cov=src --cov-report=term || echo "Tests completed with status: $?"

# ============================================
# Stage 4: Production Image
# ============================================
FROM python:3.11-slim as production

# è·å–æ„å»ºå‚æ•°
ARG BUILD_DATE
ARG VCS_REF
ARG VERSION=0.0.0

# æ·»åŠ æ ‡ç­¾
LABEL org.opencontainers.image.created="${BUILD_DATE}" \
      org.opencontainers.image.revision="${VCS_REF}" \
      org.opencontainers.image.version="${VERSION}" \
      org.opencontainers.image.title="DeepAlpha Trading System" \
      org.opencontainers.image.description="AI-powered trading system for A-shares and HK stocks" \
      org.opencontainers.image.vendor="DeepAlpha" \
      org.opencontainers.image.authors="DeepAlpha Team" \
      org.opencontainers.image.licenses="MIT"

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y --no-install-recommends \
    libpq5 \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser -u 1000 appuser

# ä»ä¾èµ–é˜¶æ®µå¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=dependencies-builder /opt/venv /opt/venv

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PATH="/opt/venv/bin:$PATH" \
    PYTHONPATH="/app:$PYTHONPATH" \
    APP_HOME="/app"

# åˆ›å»ºåº”ç”¨ç›®å½•
WORKDIR $APP_HOME

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=appuser:appuser . .

# å¤åˆ¶Alembicè¿ç§»æ–‡ä»¶
COPY --chown=appuser:appuser alembic/ alembic/
COPY --chown=appuser:appuser alembic.ini ./

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p /app/logs /app/data /app/tmp && \
    chown -R appuser:appuser /app/logs /app/data /app/tmp

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER appuser

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

#### 21.1.3 Docker Composeå¼€å‘ç¯å¢ƒ

```yaml
# docker-compose.yml
version: '3.8'

services:
  # ========================================
  # ä¸»åº”ç”¨
  # ========================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    image: deepalpha-api:latest
    container_name: deepalpha-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://deepalpha:deepalpha_pass@postgres:5432/deepalpha
      - REDIS_URL=redis://redis:6379/0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - ENVIRONMENT=development
      - LOG_LEVEL=DEBUG
      - SENTRY_DSN=${SENTRY_DSN}
    volumes:
      - ./src:/app/src:ro
      - ./logs:/app/logs
      - ./data:/app/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - deepalpha-network

  # ========================================
  # PostgreSQLæ•°æ®åº“
  # ========================================
  postgres:
    image: postgres:16-alpine
    container_name: deepalpha-postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=deepalpha
      - POSTGRES_PASSWORD=deepalpha_pass
      - POSTGRES_DB=deepalpha
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U deepalpha -d deepalpha"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - deepalpha-network

  # ========================================
  # Redisç¼“å­˜
  # ========================================
  redis:
    image: redis:7-alpine
    container_name: deepalpha-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes --requirepass redis_pass
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - deepalpha-network

  # ========================================
  # Prometheusç›‘æ§
  # ========================================
  prometheus:
    image: prom/prometheus:latest
    container_name: deepalpha-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - deepalpha-network

  # ========================================
  # Grafanaå¯è§†åŒ–
  # ========================================
  grafana:
    image: grafana/grafana:latest
    container_name: deepalpha-grafana
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_SERVER_ROOT_URL=http://localhost:3000
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    networks:
      - deepalpha-network

  # ========================================
  # Jaegeré“¾è·¯è¿½è¸ª
  # ========================================
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: deepalpha-jaeger
    restart: unless-stopped
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    networks:
      - deepalpha-network

  # ========================================
  # å‰ç«¯å¼€å‘æœåŠ¡å™¨
  # ========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.dev
    container_name: deepalpha-frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    volumes:
      - ./frontend:/app:delegated
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000
    command: npm run dev
    networks:
      - deepalpha-network

  # ========================================
  # Nginxåå‘ä»£ç†
  # ========================================
  nginx:
    image: nginx:alpine
    container_name: deepalpha-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./config/nginx/ssl:/etc/nginx/ssl:ro
      - ./frontend/dist:/usr/share/nginx/html:ro
    depends_on:
      - api
      - frontend
    networks:
      - deepalpha-network

  # ========================================
  # Workeråå°ä»»åŠ¡
  # ========================================
  worker:
    build:
      context: .
      dockerfile: Dockerfile
    image: deepalpha-worker:latest
    container_name: deepalpha-worker
    restart: unless-stopped
    command: celery -A src.tasks.worker worker --loglevel=info --concurrency=4
    environment:
      - DATABASE_URL=postgresql://deepalpha:deepalpha_pass@postgres:5432/deepalpha
      - REDIS_URL=redis://:redis_pass@redis:6379/0
      - CELERY_BROKER_URL=redis://:redis_pass@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:redis_pass@redis:6379/0
    volumes:
      - ./src:/app/src:ro
      - ./logs:/app/logs
    depends_on:
      - redis
      - postgres
    networks:
      - deepalpha-network

  # ========================================
  # Celery Beatå®šæ—¶ä»»åŠ¡
  # ========================================
  celery-beat:
    build:
      context: .
      dockerfile: Dockerfile
    image: deepalpha-beat:latest
    container_name: deepalpha-beat
    restart: unless-stopped
    command: celery -A src.tasks.beat beat --loglevel=info --scheduler redbeat.RedBeatScheduler
    environment:
      - REDIS_URL=redis://:redis_pass@redis:6379/0
      - CELERY_BROKER_URL=redis://:redis_pass@redis:6379/0
    volumes:
      - ./src:/app/src:ro
    depends_on:
      - redis
    networks:
      - deepalpha-network

  # ========================================
  # Flowerä»»åŠ¡ç›‘æ§
  # ========================================
  flower:
    build:
      context: .
      dockerfile: Dockerfile
    image: deepalpha-flower:latest
    container_name: deepalpha-flower
    restart: unless-stopped
    ports:
      - "5555:5555"
    command: celery -A src.tasks.worker flower --port=5555
    environment:
      - CELERY_BROKER_URL=redis://:redis_pass@redis:6379/0
      - CELERY_RESULT_BACKEND=redis://:redis_pass@redis:6379/0
      - FLOWER_BASIC_AUTH=${FLOWER_USER}:${FLOWER_PASSWORD}
    depends_on:
      - redis
      - worker
    networks:
      - deepalpha-network

networks:
  deepalpha-network:
    driver: bridge

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
```

---

## ç¬¬22ç«  Kubernetesç”Ÿäº§éƒ¨ç½²

### 22.1 Namespaceä¸èµ„æºé…é¢

```yaml
# k8s/00-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: deepalpha-production
  labels:
    name: deepalpha-production
    environment: production

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: deepalpha-quota
  namespace: deepalpha-production
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    services.loadbalancers: "2"
    services.nodeports: "0"

---
apiVersion: v1
kind: LimitRange
metadata:
  name: deepalpha-limits
  namespace: deepalpha-production
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container
```

### 22.2 ConfigMapé…ç½®ç®¡ç†

```yaml
# k8s/01-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: deepalpha-config
  namespace: deepalpha-production
data:
  ENVIRONMENT: "production"
  LOG_LEVEL: "INFO"
  DATABASE_POOL_SIZE: "20"
  REDIS_MAX_CONNECTIONS: "50"

  # APIé…ç½®
  API_HOST: "0.0.0.0"
  API_PORT: "8000"
  API_WORKERS: "4"
  API_TIMEOUT: "30"

  # å¸‚åœºæ•°æ®é…ç½®
  MARKET_DATA_PROVIDERS: "tushare,akshare"
  TUSHARE_TOKEN: "${TUSHARE_TOKEN}"
  AKSHARE_TIMEOUT: "10"

  # LLMé…ç½®
  LLM_PROVIDER: "deepseek"
  LLM_MODEL: "deepseek-chat"
  LLM_TEMPERATURE: "0.7"
  LLM_MAX_TOKENS: "2000"
  LLM_TIMEOUT: "30"

  # äº¤æ˜“é…ç½®
  TRADING_ENABLED: "true"
  DRY_RUN: "false"
  MAX_POSITION_SIZE: "100000"
  MAX_DAILY_LOSS: "50000"

  # ç›‘æ§é…ç½®
  SENTRY_DSN: "${SENTRY_DSN}"
  PROMETHEUS_PORT: "9090"
  JAEGER_HOST: "jaeger"
  JAEGER_PORT: "6831"
```

### 22.3 Secretå¯†é’¥ç®¡ç†

```yaml
# k8s/02-secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: deepalpha-secrets
  namespace: deepalpha-production
type: Opaque
stringData:
  # æ•°æ®åº“å‡­è¯
  DATABASE_URL: "postgresql://deepalpha:${DB_PASSWORD}@postgres:5432/deepalpha"

  # Rediså‡­è¯
  REDIS_URL: "redis://:${REDIS_PASSWORD}@redis:6379/0"

  # JWTå¯†é’¥
  JWT_SECRET_KEY: "${JWT_SECRET_KEY}"
  JWT_ALGORITHM: "HS256"

  # APIåŠ å¯†
  API_ENCRYPTION_KEY: "${API_ENCRYPTION_KEY}"

  # ç¬¬ä¸‰æ–¹æœåŠ¡å¯†é’¥
  TUSHARE_TOKEN: "${TUSHARE_TOKEN}"
  DEEPSEEK_API_KEY: "${DEEPSEEK_API_KEY}"
  QWEN_API_KEY: "${QWEN_API_KEY}"

  # Broker APIå‡­è¯
  BROKER_API_KEY: "${BROKER_API_KEY}"
  BROKER_API_SECRET: "${BROKER_API_SECRET}"

  # é€šçŸ¥æœåŠ¡
  SLACK_WEBHOOK_URL: "${SLACK_WEBHOOK_URL}"
  DINGTALK_WEBHOOK: "${DINGTALK_WEBHOOK}"
  EMAIL_SMTP_PASSWORD: "${EMAIL_SMTP_PASSWORD}"

---
# ä½¿ç”¨Sealed Secretsæˆ–External Secrets Operatorç®¡ç†æ•æ„Ÿæ•°æ®
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metadata:
  name: deepalpha-sealed-secrets
  namespace: deepalpha-production
spec:
  encryptedData:
    DATABASE_PASSWORD: AgBy3i4OJSWK+PiTySY...
    JWT_SECRET_KEY: AgBy3i4OJSWK+PiTySY...
  template:
    metadata:
      name: deepalpha-secrets
      namespace: deepalpha-production
    type: Opaque
```

### 22.4 Deploymentéƒ¨ç½²é…ç½®

```yaml
# k8s/03-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepalpha-api
  namespace: deepalpha-production
  labels:
    app: deepalpha
    component: api
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: deepalpha
      component: api
  template:
    metadata:
      labels:
        app: deepalpha
        component: api
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: deepalpha-sa

      # å®‰å…¨ä¸Šä¸‹æ–‡
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000

      # åˆå§‹åŒ–å®¹å™¨
      initContainers:
      - name: wait-for-postgres
        image: busybox:1.36
        command: ['sh', '-c', 'until nc -z postgres 5432; do echo waiting for postgres; sleep 2; done;']

      - name: wait-for-redis
        image: busybox:1.36
        command: ['sh', '-c', 'until nc -z redis 6379; do echo waiting for redis; sleep 2; done;']

      - name: run-migrations
        image: ghcr.io/your-org/deepalpha:latest
        command: ['alembic', 'upgrade', 'head']
        envFrom:
        - secretRef:
            name: deepalpha-secrets

      # ä¸»å®¹å™¨
      containers:
      - name: api
        image: ghcr.io/your-org/deepalpha:{{ .Values.image.tag }}
        imagePullPolicy: Always

        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP

        env:
        - name: ENVIRONMENT
          value: "production"

        envFrom:
        - configMapRef:
            name: deepalpha-config
        - secretRef:
            name: deepalpha-secrets

        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 2000m
            memory: 2Gi

        # æ¢é’ˆé…ç½®
        livenessProbe:
          httpGet:
            path: /health/live
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health/ready
            port: http
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3

        startupProbe:
          httpGet:
            path: /health/startup
            port: http
          initialDelaySeconds: 0
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 30

        # ç”Ÿå‘½å‘¨æœŸé’©å­
        lifecycle:
          preStop:
            exec:
              command: ["/bin/sh", "-c", "sleep 15"]

      # ä¼˜é›…ç»ˆæ­¢
      terminationGracePeriodSeconds: 30

      # äº²å’Œæ€§è§„åˆ™
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - deepalpha
              topologyKey: kubernetes.io/hostname

---
# Workeréƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deepalpha-worker
  namespace: deepalpha-production
  labels:
    app: deepalpha
    component: worker
spec:
  replicas: 2
  selector:
    matchLabels:
      app: deepalpha
      component: worker
  template:
    metadata:
      labels:
        app: deepalpha
        component: worker
    spec:
      containers:
      - name: worker
        image: ghcr.io/your-org/deepalpha:latest
        command: ["celery", "-A", "src.tasks.worker", "worker", "--loglevel=info"]

        envFrom:
        - configMapRef:
            name: deepalpha-config
        - secretRef:
            name: deepalpha-secrets

        resources:
          requests:
            cpu: 1000m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 4Gi

        livenessProbe:
          exec:
            command:
            - celery
            - -A
            - src.tasks.worker
            - inspect
            - ping
          initialDelaySeconds: 30
          periodSeconds: 60
```

### 22.5 ServiceæœåŠ¡é…ç½®

```yaml
# k8s/04-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: deepalpha-api
  namespace: deepalpha-production
  labels:
    app: deepalpha
    component: api
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: "http"
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "60"
spec:
  type: ClusterIP
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: metrics
    port: 9090
    targetPort: metrics
    protocol: TCP
  selector:
    app: deepalpha
    component: api

---
# Headless Serviceç”¨äºStatefulSet
apiVersion: v1
kind: Service
metadata:
  name: deepalpha-api-headless
  namespace: deepalpha-production
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - port: 8000
    targetPort: http
    name: http
  selector:
    app: deepalpha
    component: api
```

### 22.6 Ingressè·¯ç”±é…ç½®

```yaml
# k8s/05-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: deepalpha-ingress
  namespace: deepalpha-production
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"

    # é€Ÿç‡é™åˆ¶
    nginx.ingress.kubernetes.io/limit-rps: "100"
    nginx.ingress.kubernetes.io/limit-burst-multiplier: "2"

    # è¶…æ—¶è®¾ç½®
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "30"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "600"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "600"

    # WebSocketæ”¯æŒ
    nginx.ingress.kubernetes.io/proxy-http-version: "1.1"
    nginx.ingress.kubernetes.io/upgrade: "$http_upgrade"
    nginx.ingress.kubernetes.io/connection: "upgrade"

    # CORS
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://deepalpha.example.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"

    # å®‰å…¨å¤´
    nginx.ingress.kubernetes.io/configuration-snippet: |
      add_header X-Frame-Options "SAMEORIGIN" always;
      add_header X-Content-Type-Options "nosniff" always;
      add_header X-XSS-Protection "1; mode=block" always;
      add_header Referrer-Policy "strict-origin-when-cross-origin" always;
spec:
  tls:
  - hosts:
    - deepalpha.example.com
    - api.deepalpha.example.com
    secretName: deepalpha-tls

  rules:
  - host: deepalpha.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: deepalpha-frontend
            port:
              number: 80

  - host: api.deepalpha.example.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: deepalpha-api
            port:
              number: 80
      - path: /ws
        pathType: Prefix
        backend:
          service:
            name: deepalpha-api
            port:
              number: 80
      - path: /metrics
        pathType: Prefix
        backend:
          service:
            name: deepalpha-api
            port:
              number: 9090
```

### 22.7 HPAè‡ªåŠ¨æ‰©ç¼©å®¹

```yaml
# k8s/06-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: deepalpha-api-hpa
  namespace: deepalpha-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: deepalpha-api
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
      - type: Pods
        value: 4
        periodSeconds: 30
      selectPolicy: Max

---
# KEDAäº‹ä»¶é©±åŠ¨æ‰©ç¼©å®¹
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: deepalpha-worker-scaler
  namespace: deepalpha-production
spec:
  scaleTargetRef:
    name: deepalpha-worker
  minReplicaCount: 2
  maxReplicaCount: 10
  triggers:
  - type: redis
    metadata:
      address: redis:6379
      listName: celery
      listLength: "5"
      enableTLS: "false"
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: celery_queue_length
      threshold: "100"
      query: celery_queue_length
```

### 22.8 StatefulSetæœ‰çŠ¶æ€æœåŠ¡

```yaml
# k8s/07-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: deepalpha-trader
  namespace: deepalpha-production
spec:
  serviceName: deepalpha-trader-headless
  replicas: 3
  selector:
    matchLabels:
      app: deepalpha
      component: trader
  template:
    metadata:
      labels:
        app: deepalpha
        component: trader
    spec:
      containers:
      - name: trader
        image: ghcr.io/your-org/deepalpha:latest
        command: ["python", "-m", "src.trader.main"]

        ports:
        - containerPort: 8001
          name: trader

        envFrom:
        - configMapRef:
            name: deepalpha-config
        - secretRef:
            name: deepalpha-secrets

        volumeMounts:
        - name: trader-data
          mountPath: /app/data
        - name: trader-logs
          mountPath: /app/logs

        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1000m
            memory: 1Gi

  volumeClaimTemplates:
  - metadata:
      name: trader-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 10Gi
  - metadata:
      name: trader-logs
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: standard
      resources:
        requests:
          storage: 5Gi
```

### 22.9 PodDisruptionBudgetä¸­æ–­é¢„ç®—

```yaml
# k8s/08-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: deepalpha-api-pdb
  namespace: deepalpha-production
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: deepalpha
      component: api

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: deepalpha-worker-pdb
  namespace: deepalpha-production
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: deepalpha
      component: worker
```

### 22.10 NetworkPolicyç½‘ç»œç­–ç•¥

```yaml
# k8s/09-networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deepalpha-network-policy
  namespace: deepalpha-production
spec:
  podSelector:
    matchLabels:
      app: deepalpha
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # å…è®¸æ¥è‡ªIngressçš„æµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000

  # å…è®¸æ¥è‡ªç›‘æ§ç³»ç»Ÿçš„æµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 9090

  egress:
  # å…è®¸DNSæŸ¥è¯¢
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: UDP
      port: 53

  # å…è®¸è®¿é—®æ•°æ®åº“
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432

  # å…è®¸è®¿é—®Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379

  # å…è®¸è®¿é—®å¤–éƒ¨API
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443
```

---

## ç¬¬23ç«  æ•°æ®è¿ç§»ä¸å¤‡ä»½ç­–ç•¥

### 23.1 NOFX Goåˆ°Pythonæ•°æ®è¿ç§»

```python
# scripts/migrate_nofx_data.py
"""
NOFX Goæ•°æ®è¿ç§»åˆ°Pythonç³»ç»Ÿ
"""

import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import json

import asyncpg
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

from src.models.trader import TraderModel
from src.models.position import PositionModel
from src.models.order import OrderModel
from src.models.trade import TradeModel
from src.core.config import settings

logger = logging.getLogger(__name__)


class NOFXDataMigrator:
    """NOFXæ•°æ®è¿ç§»å™¨"""

    def __init__(self, nofx_db_url: str, target_db_url: str):
        self.nofx_db_url = nofx_db_url
        self.target_db_url = target_db_url
        self.nofx_conn = None
        self.target_engine = None
        self.target_session_factory = None

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'traders': {'migrated': 0, 'failed': 0, 'skipped': 0},
            'positions': {'migrated': 0, 'failed': 0, 'skipped': 0},
            'orders': {'migrated': 0, 'failed': 0, 'skipped': 0},
            'trades': {'migrated': 0, 'failed': 0, 'skipped': 0},
        }

    async def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        # è¿æ¥NOFXæ•°æ®åº“ï¼ˆPostgreSQLï¼‰
        self.nofx_conn = await asyncpg.connect(self.nofx_db_url)

        # åˆ›å»ºç›®æ ‡æ•°æ®åº“è¿æ¥
        self.target_engine = create_async_engine(
            self.target_db_url,
            pool_size=20,
            max_overflow=40,
        )
        self.target_session_factory = sessionmaker(
            self.target_engine,
            class_=AsyncSession,
            expire_on_commit=False,
        )

        logger.info("æ•°æ®åº“è¿æ¥å»ºç«‹æˆåŠŸ")

    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.nofx_conn:
            await self.nofx_conn.close()
        if self.target_engine:
            await self.target_engine.dispose()
        logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")

    async def migrate_all(self):
        """æ‰§è¡Œå®Œæ•´è¿ç§»"""
        try:
            await self.connect()

            # æŒ‰ä¾èµ–é¡ºåºè¿ç§»
            await self.migrate_traders()
            await self.migrate_positions()
            await self.migrate_orders()
            await self.migrate_trades()

            self.print_summary()

        finally:
            await self.close()

    async def migrate_traders(self):
        """è¿ç§»äº¤æ˜“å‘˜æ•°æ®"""
        logger.info("å¼€å§‹è¿ç§»äº¤æ˜“å‘˜æ•°æ®...")

        # ä»NOFXè¯»å–äº¤æ˜“å‘˜
        rows = await self.nofx_conn.fetch("""
            SELECT
                id,
                name,
                type,
                initial_capital,
                is_active,
                created_at,
                updated_at,
                config,
                metadata
            FROM traders
            ORDER BY created_at
        """)

        async with self.target_session_factory() as session:
            for row in rows:
                try:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = await session.get(TraderModel, row['id'])
                    if existing:
                        self.stats['traders']['skipped'] += 1
                        continue

                    # æ•°æ®æ˜ å°„ä¸è½¬æ¢
                    trader_data = {
                        'id': row['id'],
                        'name': row['name'],
                        'type': self._convert_trader_type(row['type']),
                        'initial_capital': row['initial_capital'],
                        'is_active': row['is_active'],
                        'created_at': row['created_at'],
                        'updated_at': row['updated_at'],
                        'config': self._convert_config(row['config']),
                        'metadata': self._convert_metadata(row['metadata']),
                    }

                    trader = TraderModel(**trader_data)
                    session.add(trader)
                    await session.flush()

                    self.stats['traders']['migrated'] += 1
                    logger.info(f"è¿ç§»äº¤æ˜“å‘˜: {trader.name}")

                except Exception as e:
                    logger.error(f"è¿ç§»äº¤æ˜“å‘˜å¤±è´¥ {row['id']}: {e}")
                    self.stats['traders']['failed'] += 1
                    session.rollback()

            await session.commit()

        logger.info(f"äº¤æ˜“å‘˜è¿ç§»å®Œæˆ: {self.stats['traders']}")

    async def migrate_positions(self):
        """è¿ç§»æŒä»“æ•°æ®"""
        logger.info("å¼€å§‹è¿ç§»æŒä»“æ•°æ®...")

        rows = await self.nofx_conn.fetch("""
            SELECT
                id,
                trader_id,
                symbol,
                exchange,
                quantity,
                entry_price,
                current_price,
                market_value,
                unrealized_pnl,
                created_at,
                updated_at
            FROM positions
            WHERE quantity > 0  # åªè¿ç§»å½“å‰æŒä»“
            ORDER BY trader_id, symbol
        """)

        async with self.target_session_factory() as session:
            for row in rows:
                try:
                    # éªŒè¯äº¤æ˜“å‘˜å­˜åœ¨
                    trader = await session.get(TraderModel, row['trader_id'])
                    if not trader:
                        logger.warning(f"äº¤æ˜“å‘˜ä¸å­˜åœ¨ï¼Œè·³è¿‡æŒä»“: {row['trader_id']}")
                        self.stats['positions']['skipped'] += 1
                        continue

                    position_data = {
                        'id': row['id'],
                        'trader_id': row['trader_id'],
                        'symbol': self._convert_symbol(row['symbol']),
                        'exchange': self._convert_exchange(row['exchange']),
                        'quantity': row['quantity'],
                        'entry_price': row['entry_price'],
                        'current_price': row['current_price'],
                        'market_value': row['market_value'],
                        'unrealized_pnl': row['unrealized_pnl'],
                        'created_at': row['created_at'],
                        'updated_at': row['updated_at'],
                    }

                    position = PositionModel(**position_data)
                    session.add(position)
                    await session.flush()

                    self.stats['positions']['migrated'] += 1

                except Exception as e:
                    logger.error(f"è¿ç§»æŒä»“å¤±è´¥ {row['id']}: {e}")
                    self.stats['positions']['failed'] += 1
                    session.rollback()

            await session.commit()

        logger.info(f"æŒä»“è¿ç§»å®Œæˆ: {self.stats['positions']}")

    async def migrate_orders(self):
        """è¿ç§»è®¢å•æ•°æ®"""
        logger.info("å¼€å§‹è¿ç§»è®¢å•æ•°æ®...")

        # åˆ†æ‰¹è¿ç§»ä»¥æé«˜æ€§èƒ½
        batch_size = 1000
        offset = 0

        while True:
            rows = await self.nofx_conn.fetch("""
                SELECT
                    id,
                    trader_id,
                    symbol,
                    exchange,
                    side,
                    order_type,
                    quantity,
                    price,
                    status,
                    filled_quantity,
                    avg_fill_price,
                    created_at,
                    updated_at,
                    filled_at,
                    cancelled_at,
                    rejected_reason
                FROM orders
                ORDER BY created_at
                LIMIT $1 OFFSET $2
            """, batch_size, offset)

            if not rows:
                break

            async with self.target_session_factory() as session:
                for row in rows:
                    try:
                        order_data = {
                            'id': row['id'],
                            'trader_id': row['trader_id'],
                            'symbol': self._convert_symbol(row['symbol']),
                            'exchange': self._convert_exchange(row['exchange']),
                            'side': row['side'],
                            'order_type': self._convert_order_type(row['order_type']),
                            'quantity': row['quantity'],
                            'price': row['price'],
                            'status': self._convert_order_status(row['status']),
                            'filled_quantity': row['filled_quantity'],
                            'avg_fill_price': row['avg_fill_price'],
                            'created_at': row['created_at'],
                            'updated_at': row['updated_at'],
                            'filled_at': row['filled_at'],
                            'cancelled_at': row['cancelled_at'],
                            'rejected_reason': row['rejected_reason'],
                        }

                        order = OrderModel(**order_data)
                        session.add(order)
                        await session.flush()

                        self.stats['orders']['migrated'] += 1

                    except Exception as e:
                        logger.error(f"è¿ç§»è®¢å•å¤±è´¥ {row['id']}: {e}")
                        self.stats['orders']['failed'] += 1
                        session.rollback()

                await session.commit()

            offset += batch_size
            logger.info(f"å·²è¿ç§» {offset + len(rows)} æ¡è®¢å•")

        logger.info(f"è®¢å•è¿ç§»å®Œæˆ: {self.stats['orders']}")

    async def migrate_trades(self):
        """è¿ç§»æˆäº¤è®°å½•"""
        logger.info("å¼€å§‹è¿ç§»æˆäº¤è®°å½•...")

        batch_size = 1000
        offset = 0

        while True:
            rows = await self.nofx_conn.fetch("""
                SELECT
                    id,
                    order_id,
                    trader_id,
                    symbol,
                    exchange,
                    side,
                    quantity,
                    price,
                    commission,
                    timestamp,
                    external_trade_id
                FROM trades
                ORDER BY timestamp
                LIMIT $1 OFFSET $2
            """, batch_size, offset)

            if not rows:
                break

            async with self.target_session_factory() as session:
                for row in rows:
                    try:
                        trade_data = {
                            'id': row['id'],
                            'order_id': row['order_id'],
                            'trader_id': row['trader_id'],
                            'symbol': self._convert_symbol(row['symbol']),
                            'exchange': self._convert_exchange(row['exchange']),
                            'side': row['side'],
                            'quantity': row['quantity'],
                            'price': row['price'],
                            'commission': row['commission'],
                            'timestamp': row['timestamp'],
                            'external_trade_id': row['external_trade_id'],
                        }

                        trade = TradeModel(**trade_data)
                        session.add(trade)
                        await session.flush()

                        self.stats['trades']['migrated'] += 1

                    except Exception as e:
                        logger.error(f"è¿ç§»æˆäº¤å¤±è´¥ {row['id']}: {e}")
                        self.stats['trades']['failed'] += 1
                        session.rollback()

                await session.commit()

            offset += batch_size
            logger.info(f"å·²è¿ç§» {offset + len(rows)} æ¡æˆäº¤è®°å½•")

        logger.info(f"æˆäº¤è®°å½•è¿ç§»å®Œæˆ: {self.stats['trades']}")

    @staticmethod
    def _convert_trader_type(nofx_type: str) -> str:
        """è½¬æ¢äº¤æ˜“å‘˜ç±»å‹"""
        type_mapping = {
            'manual': 'discretionary',
            'ai': 'ai',
            'hybrid': 'hybrid',
        }
        return type_mapping.get(nofx_type, 'discretionary')

    @staticmethod
    def _convert_symbol(nofx_symbol: str) -> str:
        """è½¬æ¢äº¤æ˜“ä»£ç æ ¼å¼"""
        # NOFXä½¿ç”¨BTC/USDï¼Œæ–°ç³»ç»Ÿä½¿ç”¨BTCUSD
        return nofx_symbol.replace('/', '').upper()

    @staticmethod
    def _convert_exchange(nofx_exchange: str) -> str:
        """è½¬æ¢äº¤æ˜“æ‰€ä»£ç """
        exchange_mapping = {
            'binance': 'BN',
            'okx': 'OKX',
            'bybit': 'BYBIT',
        }
        return exchange_mapping.get(nofx_exchange.lower(), 'XSHE')  # é»˜è®¤æ·±äº¤æ‰€

    @staticmethod
    def _convert_order_type(nofx_type: str) -> str:
        """è½¬æ¢è®¢å•ç±»å‹"""
        return nofx_type.lower()  # MARKET, LIMIT

    @staticmethod
    def _convert_order_status(nofx_status: str) -> str:
        """è½¬æ¢è®¢å•çŠ¶æ€"""
        status_mapping = {
            'PENDING': 'pending',
            'OPEN': 'open',
            'FILLED': 'filled',
            'PARTIALLY_FILLED': 'partially_filled',
            'CANCELLED': 'cancelled',
            'REJECTED': 'rejected',
            'EXPIRED': 'expired',
        }
        return status_mapping.get(nofx_status, 'pending')

    @staticmethod
    def _convert_config(nofx_config: Dict) -> Dict:
        """è½¬æ¢é…ç½®æ ¼å¼"""
        if isinstance(nofx_config, str):
            nofx_config = json.loads(nofx_config)

        # è½¬æ¢é…ç½®å­—æ®µ
        return {
            'risk_limit': nofx_config.get('risk_limit', 0.02),
            'max_position': nofx_config.get('max_position', 100000),
            'strategy_params': nofx_config.get('strategy', {}),
        }

    @staticmethod
    def _convert_metadata(nofx_metadata: Dict) -> Dict:
        """è½¬æ¢å…ƒæ•°æ®æ ¼å¼"""
        if isinstance(nofx_metadata, str):
            nofx_metadata = json.loads(nofx_metadata)
        return nofx_metadata or {}

    def print_summary(self):
        """æ‰“å°è¿ç§»æ‘˜è¦"""
        print("\n" + "="*50)
        print("æ•°æ®è¿ç§»æ‘˜è¦")
        print("="*50)

        for entity, stats in self.stats.items():
            print(f"\n{entity.upper()}:")
            print(f"  è¿ç§»æˆåŠŸ: {stats['migrated']}")
            print(f"  è·³è¿‡:     {stats['skipped']}")
            print(f"  å¤±è´¥:     {stats['failed']}")


async def main():
    """ä¸»å‡½æ•°"""
    migrator = NOFXDataMigrator(
        nofx_db_url="postgresql://user:pass@localhost:5432/nofx",
        target_db_url=str(settings.DATABASE_URL),
    )

    await migrator.migrate_all()


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
```

### 23.2 å¢é‡æ•°æ®åŒæ­¥

```python
# scripts/incremental_sync.py
"""
å¢é‡æ•°æ®åŒæ­¥è„šæœ¬
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List

from sqlalchemy import select, func
from sqlalchemy.ext.asyncio import AsyncSession

from src.database import get_session
from src.models.trader import TraderModel
from src.models.order import OrderModel
from src.models.trade import TradeModel

logger = logging.getLogger(__name__)


class IncrementalSyncer:
    """å¢é‡åŒæ­¥å™¨"""

    def __init__(self, source_db_url: str, target_db_url: str):
        self.source_db_url = source_db_url
        self.target_db_url = target_db_url
        self.last_sync_time = self._get_last_sync_time()

    @staticmethod
    def _get_last_sync_time() -> datetime:
        """è·å–ä¸Šæ¬¡åŒæ­¥æ—¶é—´"""
        # ä»æ–‡ä»¶æˆ–æ•°æ®åº“è¯»å–
        # è¿™é‡Œç®€åŒ–ä¸º10åˆ†é’Ÿå‰
        return datetime.now() - timedelta(minutes=10)

    async def sync_orders(self):
        """åŒæ­¥å¢é‡è®¢å•"""
        logger.info(f"åŒæ­¥ {self.last_sync_time} ä¹‹åçš„è®¢å•...")

        async with get_session() as source_session:
            # æŸ¥è¯¢å¢é‡æ•°æ®
            query = select(OrderModel).where(
                OrderModel.updated_at > self.last_sync_time
            ).order_by(OrderModel.updated_at)

            result = await source_session.execute(query)
            orders = result.scalars().all()

            logger.info(f"æ‰¾åˆ° {len(orders)} æ¡å¢é‡è®¢å•")

            # å†™å…¥ç›®æ ‡æ•°æ®åº“
            async with get_session() as target_session:
                for order in orders:
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    existing = await target_session.get(OrderModel, order.id)
                    if existing:
                        # æ›´æ–°
                        for key, value in order.__dict__.items():
                            if not key.startswith('_'):
                                setattr(existing, key, value)
                    else:
                        # æ’å…¥
                        target_session.add(order)

                await target_session.commit()

        # æ›´æ–°åŒæ­¥æ—¶é—´
        self._update_last_sync_time()

    async def sync_trades(self):
        """åŒæ­¥å¢é‡æˆäº¤"""
        logger.info(f"åŒæ­¥ {self.last_sync_time} ä¹‹åçš„æˆäº¤...")

        async with get_session() as source_session:
            query = select(TradeModel).where(
                TradeModel.timestamp > self.last_sync_time
            ).order_by(TradeModel.timestamp)

            result = await source_session.execute(query)
            trades = result.scalars().all()

            logger.info(f"æ‰¾åˆ° {len(trades)} æ¡å¢é‡æˆäº¤")

            async with get_session() as target_session:
                for trade in trades:
                    existing = await target_session.get(TradeModel, trade.id)
                    if existing:
                        for key, value in trade.__dict__.items():
                            if not key.startswith('_'):
                                setattr(existing, key, value)
                    else:
                        target_session.add(trade)

                await target_session.commit()

        self._update_last_sync_time()

    def _update_last_sync_time(self):
        """æ›´æ–°åŒæ­¥æ—¶é—´"""
        self.last_sync_time = datetime.now()
        # æŒä¹…åŒ–åˆ°æ–‡ä»¶æˆ–æ•°æ®åº“

    async def run(self):
        """æ‰§è¡ŒåŒæ­¥"""
        await self.sync_orders()
        await self.sync_trades()
        logger.info("å¢é‡åŒæ­¥å®Œæˆ")


async def main():
    """å®šæ—¶ä»»åŠ¡"""
    syncer = IncrementalSyncer(
        source_db_url="postgresql://...",
        target_db_url="postgresql://...",
    )

    while True:
        try:
            await syncer.run()
        except Exception as e:
            logger.error(f"åŒæ­¥å¤±è´¥: {e}")

        # æ¯10åˆ†é’ŸåŒæ­¥ä¸€æ¬¡
        await asyncio.sleep(600)


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
```

### 23.3 å¤‡ä»½æ¢å¤è„šæœ¬

```bash
#!/bin/bash
# scripts/backup.sh

set -e

# é…ç½®
BACKUP_DIR="/data/backups/deepalpha"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DATABASE_HOST="postgres"
DATABASE_NAME="deepalpha"
DATABASE_USER="deepalpha"
S3_BUCKET="s3://deepalpha-backups"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "$BACKUP_DIR/$TIMESTAMP"

echo "========================================="
echo "DeepAlphaå¤‡ä»½è„šæœ¬"
echo "æ—¶é—´: $(date)"
echo "========================================="

# 1. æ•°æ®åº“å¤‡ä»½
echo "å¤‡ä»½æ•°æ®åº“..."
pg_dump -h "$DATABASE_HOST" \
        -U "$DATABASE_USER" \
        -d "$DATABASE_NAME" \
        -F c \
        -f "$BACKUP_DIR/$TIMESTAMP/database.dump"

# 2. Rediså¤‡ä»½
echo "å¤‡ä»½Redis..."
redis-cli --rdb "$BACKUP_DIR/$TIMESTAMP/redis.rdb"

# 3. æ•°æ®ç›®å½•å¤‡ä»½
echo "å¤‡ä»½æ•°æ®ç›®å½•..."
tar -czf "$BACKUP_DIR/$TIMESTAMP/data.tar.gz" /app/data

# 4. é…ç½®æ–‡ä»¶å¤‡ä»½
echo "å¤‡ä»½é…ç½®æ–‡ä»¶..."
tar -czf "$BACKUP_DIR/$TIMESTAMP/config.tar.gz" /etc/deepalpha

# 5. ç”Ÿæˆå¤‡ä»½æ¸…å•
echo "ç”Ÿæˆå¤‡ä»½æ¸…å•..."
cat > "$BACKUP_DIR/$TIMESTAMP/manifest.txt" << EOF
å¤‡ä»½æ—¶é—´: $(date)
æ•°æ®åº“æ–‡ä»¶: database.dump
Redisæ–‡ä»¶: redis.rdb
æ•°æ®æ–‡ä»¶: data.tar.gz
é…ç½®æ–‡ä»¶: config.tar.gz
EOF

# 6. è®¡ç®—æ ¡éªŒå’Œ
echo "è®¡ç®—æ ¡éªŒå’Œ..."
sha256sum "$BACKUP_DIR/$TIMESTAMP"/* > "$BACKUP_DIR/$TIMESTAMP/sha256sums.txt"

# 7. ä¸Šä¼ åˆ°S3
echo "ä¸Šä¼ åˆ°S3..."
aws s3 sync "$BACKUP_DIR/$TIMESTAMP" "$S3_BUCKET/$TIMESTAMP/"

# 8. æ¸…ç†æ—§å¤‡ä»½
echo "æ¸…ç†æ—§å¤‡ä»½..."
find "$BACKUP_DIR" -maxdepth 1 -type d -mtime +$RETENTION_DAYS -exec rm -rf {} \;

# 9. å‘é€é€šçŸ¥
echo "å‘é€å¤‡ä»½é€šçŸ¥..."
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-Type: application/json' \
  -d "{\"text\": \"âœ… DeepAlphaå¤‡ä»½å®Œæˆ: $TIMESTAMP\"}"

echo "å¤‡ä»½å®Œæˆ!"
```

```bash
#!/bin/bash
# scripts/restore.sh

set -e

if [ -z "$1" ]; then
  echo "ç”¨æ³•: $0 <å¤‡ä»½æ—¶é—´æˆ³>"
  echo "ç¤ºä¾‹: $0 20240101_000000"
  exit 1
fi

BACKUP_ID="$1"
BACKUP_DIR="/data/backups/deepalpha/$BACKUP_ID"
S3_BUCKET="s3://deepalpha-backups"

echo "========================================="
echo "DeepAlphaæ¢å¤è„šæœ¬"
echo "å¤‡ä»½ID: $BACKUP_ID"
echo "========================================="

# 1. ä»S3ä¸‹è½½
if [ ! -d "$BACKUP_DIR" ]; then
  echo "ä»S3ä¸‹è½½å¤‡ä»½..."
  aws s3 sync "$S3_BUCKET/$BACKUP_ID/" "$BACKUP_DIR/"
fi

# 2. éªŒè¯æ ¡éªŒå’Œ
echo "éªŒè¯æ ¡éªŒå’Œ..."
cd "$BACKUP_DIR"
sha256sum -c sha256sums.txt
if [ $? -ne 0 ]; then
  echo "æ ¡éªŒå’ŒéªŒè¯å¤±è´¥!"
  exit 1
fi

# 3. åœæ­¢æœåŠ¡
echo "åœæ­¢æœåŠ¡..."
kubectl scale deployment deepalpha-api --replicas=0 -n deepalpha-production

# 4. æ¢å¤æ•°æ®åº“
echo "æ¢å¤æ•°æ®åº“..."
pg_restore -h postgres -U deepalpha -d deepalpha --clean --if-exists "$BACKUP_DIR/database.dump"

# 5. æ¢å¤Redis
echo "æ¢å¤Redis..."
redis-cli --rdb "$BACKUP_DIR/redis.rdb"

# 6. æ¢å¤æ•°æ®ç›®å½•
echo "æ¢å¤æ•°æ®ç›®å½•..."
tar -xzf "$BACKUP_DIR/data.tar.gz" -C /

# 7. æ¢å¤é…ç½®æ–‡ä»¶
echo "æ¢å¤é…ç½®æ–‡ä»¶..."
tar -xzf "$BACKUP_DIR/config.tar.gz" -C /

# 8. å¯åŠ¨æœåŠ¡
echo "å¯åŠ¨æœåŠ¡..."
kubectl scale deployment deepalpha-api --replicas=3 -n deepalpha-production

# 9. ç­‰å¾…æœåŠ¡å°±ç»ª
echo "ç­‰å¾…æœåŠ¡å°±ç»ª..."
kubectl wait --for=condition=ready pod -l app=deepalpha -n deepalpha-production --timeout=300s

# 10. éªŒè¯
echo "éªŒè¯æœåŠ¡..."
curl -f http://api.deepalpha.example.com/health || exit 1

echo "æ¢å¤å®Œæˆ!"
```

---

## ç¬¬24ç«  å‰ç«¯é«˜çº§æ¨¡å¼ä¸çŠ¶æ€ç®¡ç†

### 24.1 ReactçŠ¶æ€ç®¡ç†æ¶æ„

```typescript
// frontend/src/store/useAppStore.ts
import { create } from 'zustand';
import { devtools, persist } from 'zustand/middleware';
import { immer } from 'zustand/middleware/immer';
import { subscribeWithSelector } from 'zustand/middleware';

// ============================================
// ç±»å‹å®šä¹‰
// ============================================
interface Trader {
  id: string;
  name: string;
  type: 'discretionary' | 'ai' | 'hybrid';
  equity: number;
  pnl: number;
  isActive: boolean;
}

interface Position {
  id: string;
  traderId: string;
  symbol: string;
  exchange: string;
  quantity: number;
  entryPrice: number;
  currentPrice: number;
  unrealizedPnl: number;
}

interface Order {
  id: string;
  traderId: string;
  symbol: string;
  side: 'buy' | 'sell';
  type: 'market' | 'limit';
  quantity: number;
  price?: number;
  status: 'pending' | 'open' | 'filled' | 'cancelled';
  createdAt: string;
}

interface MarketData {
  symbol: string;
  price: number;
  change: number;
  changePercent: number;
  volume: number;
  timestamp: string;
}

// ============================================
// Sliceå®šä¹‰
// ============================================

// Auth Slice
interface AuthState {
  user: User | null;
  token: string | null;
  isAuthenticated: boolean;
  login: (email: string, password: string) => Promise<void>;
  logout: () => void;
  refreshToken: () => Promise<void>;
}

interface User {
  id: string;
  email: string;
  name: string;
  role: 'admin' | 'trader' | 'viewer';
}

// Trader Slice
interface TraderState {
  traders: Trader[];
  selectedTraderId: string | null;
  isLoading: boolean;
  error: string | null;
  fetchTraders: () => Promise<void>;
  selectTrader: (id: string) => void;
  createTrader: (data: Partial<Trader>) => Promise<void>;
  updateTrader: (id: string, data: Partial<Trader>) => Promise<void>;
  deleteTrader: (id: string) => Promise<void>;
}

// Position Slice
interface PositionState {
  positions: Position[];
  filterByTrader: string | null;
  fetchPositions: () => Promise<void>;
  updatePosition: (position: Position) => void;
}

// Order Slice
interface OrderState {
  orders: Order[];
  pendingOrders: Order[];
  fetchOrders: (traderId?: string) => Promise<void>;
  submitOrder: (order: Omit<Order, 'id' | 'status' | 'createdAt'>) => Promise<void>;
  cancelOrder: (orderId: string) => Promise<void>;
}

// Market Data Slice
interface MarketDataState {
  data: Map<string, MarketData>;
  watchlist: string[];
  subscribe: (symbols: string[]) => void;
  unsubscribe: (symbols: string[]) => void;
  updatePrice: (symbol: string, data: Partial<MarketData>) => void;
}

// UI Slice
interface UIState {
  sidebarOpen: boolean;
  theme: 'light' | 'dark';
  notifications: Notification[];
  addNotification: (notification: Omit<Notification, 'id'>) => void;
  removeNotification: (id: string) => void;
  toggleSidebar: () => void;
  setTheme: (theme: 'light' | 'dark') => void;
}

interface Notification {
  id: string;
  type: 'success' | 'error' | 'warning' | 'info';
  message: string;
  duration?: number;
}

// ============================================
// Storeåˆ›å»º
// ============================================
interface AppState extends AuthState, TraderState, PositionState, OrderState, MarketDataState, UIState {}

export const useAppStore = create<AppState>()(
  devtools(
    persist(
      subscribeWithSelector(
        immer((set, get) => ({
          // ============================================
          // Auth State & Actions
          // ============================================
          user: null,
          token: null,
          isAuthenticated: false,

          login: async (email: string, password: string) => {
            try {
              const response = await fetch('/api/v1/auth/login', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ email, password }),
              });

              if (!response.ok) throw new Error('Login failed');

              const data = await response.json();

              set((state) => {
                state.user = data.user;
                state.token = data.token;
                state.isAuthenticated = true;
              });
            } catch (error) {
              set((state) => {
                state.error = (error as Error).message;
              });
              throw error;
            }
          },

          logout: () => {
            set((state) => {
              state.user = null;
              state.token = null;
              state.isAuthenticated = false;
            });
          },

          refreshToken: async () => {
            const { token } = get();
            if (!token) return;

            try {
              const response = await fetch('/api/v1/auth/refresh', {
                method: 'POST',
                headers: {
                  'Content-Type': 'application/json',
                  'Authorization': `Bearer ${token}`,
                },
              });

              if (!response.ok) throw new Error('Token refresh failed');

              const data = await response.json();
              set((state) => {
                state.token = data.token;
              });
            } catch (error) {
              get().logout();
              throw error;
            }
          },

          // ============================================
          // Trader State & Actions
          // ============================================
          traders: [],
          selectedTraderId: null,
          isLoading: false,
          error: null,

          fetchTraders: async () => {
            set((state) => {
              state.isLoading = true;
              state.error = null;
            });

            try {
              const { token } = get();
              const response = await fetch('/api/v1/traders', {
                headers: {
                  'Authorization': `Bearer ${token}`,
                },
              });

              if (!response.ok) throw new Error('Failed to fetch traders');

              const data = await response.json();

              set((state) => {
                state.traders = data;
                state.isLoading = false;
              });
            } catch (error) {
              set((state) => {
                state.error = (error as Error).message;
                state.isLoading = false;
              });
            }
          },

          selectTrader: (id: string) => {
            set((state) => {
              state.selectedTraderId = id;
            });
          },

          createTrader: async (data: Partial<Trader>) => {
            const { token } = get();
            const response = await fetch('/api/v1/traders', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${token}`,
              },
              body: JSON.stringify(data),
            });

            if (!response.ok) throw new Error('Failed to create trader');

            const newTrader = await response.json();

            set((state) => {
              state.traders.push(newTrader);
            });
          },

          updateTrader: async (id: string, data: Partial<Trader>) => {
            const { token } = get();
            const response = await fetch(`/api/v1/traders/${id}`, {
              method: 'PATCH',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${token}`,
              },
              body: JSON.stringify(data),
            });

            if (!response.ok) throw new Error('Failed to update trader');

            const updatedTrader = await response.json();

            set((state) => {
              const index = state.traders.findIndex((t) => t.id === id);
              if (index !== -1) {
                state.traders[index] = updatedTrader;
              }
            });
          },

          deleteTrader: async (id: string) => {
            const { token } = get();
            const response = await fetch(`/api/v1/traders/${id}`, {
              method: 'DELETE',
              headers: {
                'Authorization': `Bearer ${token}`,
              },
            });

            if (!response.ok) throw new Error('Failed to delete trader');

            set((state) => {
              state.traders = state.traders.filter((t) => t.id !== id);
              if (state.selectedTraderId === id) {
                state.selectedTraderId = null;
              }
            });
          },

          // ============================================
          // Position State & Actions
          // ============================================
          positions: [],
          filterByTrader: null,

          fetchPositions: async () => {
            const { token, filterByTrader } = get();
            const url = filterByTrader
              ? `/api/v1/positions?trader_id=${filterByTrader}`
              : '/api/v1/positions';

            const response = await fetch(url, {
              headers: {
                'Authorization': `Bearer ${token}`,
              },
            });

            if (!response.ok) throw new Error('Failed to fetch positions');

            const data = await response.json();

            set((state) => {
              state.positions = data;
            });
          },

          updatePosition: (position: Position) => {
            set((state) => {
              const index = state.positions.findIndex((p) => p.id === position.id);
              if (index !== -1) {
                state.positions[index] = position;
              } else {
                state.positions.push(position);
              }
            });
          },

          // ============================================
          // Order State & Actions
          // ============================================
          orders: [],
          pendingOrders: [],

          fetchOrders: async (traderId?: string) => {
            const { token } = get();
            const url = traderId
              ? `/api/v1/orders?trader_id=${traderId}`
              : '/api/v1/orders';

            const response = await fetch(url, {
              headers: {
                'Authorization': `Bearer ${token}`,
              },
            });

            if (!response.ok) throw new Error('Failed to fetch orders');

            const data = await response.json();

            set((state) => {
              state.orders = data;
              state.pendingOrders = data.filter((o: Order) => o.status === 'pending' || o.status === 'open');
            });
          },

          submitOrder: async (order: Omit<Order, 'id' | 'status' | 'createdAt'>) => {
            const { token } = get();
            const response = await fetch('/api/v1/orders', {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${token}`,
              },
              body: JSON.stringify(order),
            });

            if (!response.ok) throw new Error('Failed to submit order');

            const newOrder = await response.json();

            set((state) => {
              state.orders.push(newOrder);
              if (newOrder.status === 'pending' || newOrder.status === 'open') {
                state.pendingOrders.push(newOrder);
              }
            });
          },

          cancelOrder: async (orderId: string) => {
            const { token } = get();
            const response = await fetch(`/api/v1/orders/${orderId}/cancel`, {
              method: 'POST',
              headers: {
                'Authorization': `Bearer ${token}`,
              },
            });

            if (!response.ok) throw new Error('Failed to cancel order');

            set((state) => {
              const order = state.orders.find((o) => o.id === orderId);
              if (order) {
                order.status = 'cancelled';
              }
              state.pendingOrders = state.pendingOrders.filter((o) => o.id !== orderId);
            });
          },

          // ============================================
          // Market Data State & Actions
          // ============================================
          data: new Map(),
          watchlist: [],

          subscribe: (symbols: string[]) => {
            // WebSocketè®¢é˜…é€»è¾‘
            const ws = new WebSocket(`${import.meta.env.VITE_WS_URL}/ws/market`);

            ws.onopen = () => {
              ws.send(JSON.stringify({
                action: 'subscribe',
                symbols,
              }));
            };

            ws.onmessage = (event) => {
              const data = JSON.parse(event.data);
              set((state) => {
                state.data.set(data.symbol, data);
              });
            };

            set((state) => {
              state.watchlist = [...new Set([...state.watchlist, ...symbols])];
            });
          },

          unsubscribe: (symbols: string[]) => {
            // WebSocketå–æ¶ˆè®¢é˜…é€»è¾‘
            set((state) => {
              state.watchlist = state.watchlist.filter((s) => !symbols.includes(s));
              symbols.forEach((symbol) => {
                state.data.delete(symbol);
              });
            });
          },

          updatePrice: (symbol: string, data: Partial<MarketData>) => {
            set((state) => {
              const existing = state.data.get(symbol);
              state.data.set(symbol, { ...existing, ...data } as MarketData);
            });
          },

          // ============================================
          // UI State & Actions
          // ============================================
          sidebarOpen: true,
          theme: 'light',
          notifications: [],

          addNotification: (notification: Omit<Notification, 'id'>) => {
            const id = crypto.randomUUID();
            set((state) => {
              state.notifications.push({ ...notification, id });
            });

            // è‡ªåŠ¨ç§»é™¤é€šçŸ¥
            if (notification.duration !== 0) {
              setTimeout(() => {
                get().removeNotification(id);
              }, notification.duration || 5000);
            }
          },

          removeNotification: (id: string) => {
            set((state) => {
              state.notifications = state.notifications.filter((n) => n.id !== id);
            });
          },

          toggleSidebar: () => {
            set((state) => {
              state.sidebarOpen = !state.sidebarOpen;
            });
          },

          setTheme: (theme: 'light' | 'dark') => {
            set((state) => {
              state.theme = theme;
            });
            document.documentElement.setAttribute('data-theme', theme);
          },
        }))
      ),
      {
        name: 'deepalpha-storage',
        partialize: (state) => ({
          theme: state.theme,
          sidebarOpen: state.sidebarOpen,
          watchlist: state.watchlist,
        }),
      }
    ),
    { name: 'DeepAlphaStore' }
  )
);

// ============================================
// Selectors
// ============================================
export const selectTraders = (state: AppState) => state.traders;
export const selectActiveTraders = (state: AppState) => state.traders.filter((t) => t.isActive);
export const selectSelectedTrader = (state: AppState) =>
  state.traders.find((t) => t.id === state.selectedTraderId) || null;
export const selectPositionsByTrader = (traderId: string) => (state: AppState) =>
  state.positions.filter((p) => p.traderId === traderId);
export const selectOrdersByTrader = (traderId: string) => (state: AppState) =>
  state.orders.filter((o) => o.traderId === traderId);
export const selectMarketData = (symbol: string) => (state: AppState) => state.data.get(symbol);
```

### 24.2 è‡ªå®šä¹‰Hooks

```typescript
// frontend/src/hooks/useWebSocket.ts
import { useEffect, useRef, useCallback } from 'react';
import { useAppStore } from '../store/useAppStore';

interface WebSocketMessage {
  type: 'price' | 'order' | 'position' | 'trade' | 'notification';
  data: unknown;
}

export function useWebSocket(url: string) {
  const wsRef = useRef<WebSocket | null>(null);
  const reconnectTimeoutRef = useRef<NodeJS.Timeout>();
  const reconnectAttempts = useRef(0);
  const maxReconnectAttempts = 5;

  const addNotification = useAppStore((state) => state.addNotification);
  const updatePrice = useAppStore((state) => state.updatePrice);

  const connect = useCallback(() => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      return;
    }

    const ws = new WebSocket(url);
    wsRef.current = ws;

    ws.onopen = () => {
      console.log('WebSocket connected');
      reconnectAttempts.current = 0;

      // å‘é€è®¤è¯æ¶ˆæ¯
      const token = localStorage.getItem('deepalpha-storage');
      if (token) {
        const parsed = JSON.parse(token);
        ws.send(JSON.stringify({
          type: 'auth',
          token: parsed.state.token,
        }));
      }
    };

    ws.onmessage = (event) => {
      try {
        const message: WebSocketMessage = JSON.parse(event.data);

        switch (message.type) {
          case 'price':
            updatePrice((message.data as MarketData).symbol, message.data as MarketData);
            break;

          case 'order':
            // å¤„ç†è®¢å•æ›´æ–°
            break;

          case 'notification':
            addNotification(message.data as Omit<Notification, 'id'>);
            break;

          default:
            console.warn('Unknown message type:', message.type);
        }
      } catch (error) {
        console.error('Failed to parse WebSocket message:', error);
      }
    };

    ws.onerror = (error) => {
      console.error('WebSocket error:', error);
      addNotification({
        type: 'error',
        message: 'WebSocketè¿æ¥é”™è¯¯',
      });
    };

    ws.onclose = () => {
      console.log('WebSocket closed');

      // è‡ªåŠ¨é‡è¿
      if (reconnectAttempts.current < maxReconnectAttempts) {
        reconnectAttempts.current++;
        const delay = Math.min(1000 * Math.pow(2, reconnectAttempts.current), 30000);

        reconnectTimeoutRef.current = setTimeout(() => {
          console.log(`Reconnecting... Attempt ${reconnectAttempts.current}`);
          connect();
        }, delay);
      } else {
        addNotification({
          type: 'error',
          message: 'WebSocketè¿æ¥å¤±è´¥ï¼Œè¯·åˆ·æ–°é¡µé¢',
          duration: 0,
        });
      }
    };
  }, [url, updatePrice, addNotification]);

  const disconnect = useCallback(() => {
    if (reconnectTimeoutRef.current) {
      clearTimeout(reconnectTimeoutRef.current);
    }

    if (wsRef.current) {
      wsRef.current.close();
      wsRef.current = null;
    }
  }, []);

  const send = useCallback((message: unknown) => {
    if (wsRef.current?.readyState === WebSocket.OPEN) {
      wsRef.current.send(JSON.stringify(message));
    } else {
      console.warn('WebSocket is not connected');
    }
  }, []);

  useEffect(() => {
    connect();

    return () => {
      disconnect();
    };
  }, [connect, disconnect]);

  return { send, disconnect };
}

// ============================================
// useTraders Hook
// ============================================
export function useTraders() {
  const {
    traders,
    selectedTraderId,
    isLoading,
    error,
    fetchTraders,
    selectTrader,
    createTrader,
    updateTrader,
    deleteTrader,
  } = useAppStore();

  useEffect(() => {
    fetchTraders();
  }, [fetchTraders]);

  return {
    traders,
    selectedTrader: traders.find((t) => t.id === selectedTraderId) || null,
    selectedTraderId,
    isLoading,
    error,
    selectTrader,
    createTrader,
    updateTrader,
    deleteTrader,
    refreshTraders: fetchTraders,
  };
}

// ============================================
// usePositions Hook
// ============================================
export function usePositions(traderId?: string) {
  const { positions, fetchPositions, filterByTrader } = useAppStore();

  useEffect(() => {
    if (traderId && filterByTrader !== traderId) {
      // æ›´æ–°è¿‡æ»¤æ¡ä»¶å¹¶é‡æ–°è·å–
    }
    fetchPositions();
  }, [traderId, fetchPositions, filterByTrader]);

  const filteredPositions = traderId
    ? positions.filter((p) => p.traderId === traderId)
    : positions;

  return {
    positions: filteredPositions,
    refreshPositions: fetchPositions,
  };
}

// ============================================
// useOrders Hook
// ============================================
export function useOrders(traderId?: string) {
  const {
    orders,
    pendingOrders,
    fetchOrders,
    submitOrder,
    cancelOrder,
  } = useAppStore();

  useEffect(() => {
    fetchOrders(traderId);
  }, [traderId, fetchOrders]);

  const filteredOrders = traderId
    ? orders.filter((o) => o.traderId === traderId)
    : orders;

  return {
    orders: filteredOrders,
    pendingOrders: traderId
      ? pendingOrders.filter((o) => o.traderId === traderId)
      : pendingOrders,
    refreshOrders: () => fetchOrders(traderId),
    submitOrder,
    cancelOrder,
  };
}

// ============================================
// useMarketData Hook
// ============================================
export function useMarketData(symbols: string[]) {
  const { data, subscribe, unsubscribe } = useAppStore();

  useEffect(() => {
    subscribe(symbols);

    return () => {
      unsubscribe(symbols);
    };
  }, [symbols.join(','), subscribe, unsubscribe]);

  const getMarketData = (symbol: string) => data.get(symbol);

  return {
    getMarketData,
    allData: data,
  };
}

// ============================================
// useNotifications Hook
// ============================================
export function useNotifications() {
  const { notifications, addNotification, removeNotification } = useAppStore();

  const showSuccess = useCallback((message: string, duration?: number) => {
    addNotification({ type: 'success', message, duration });
  }, [addNotification]);

  const showError = useCallback((message: string, duration?: number) => {
    addNotification({ type: 'error', message, duration });
  }, [addNotification]);

  const showWarning = useCallback((message: string, duration?: number) => {
    addNotification({ type: 'warning', message, duration });
  }, [addNotification]);

  const showInfo = useCallback((message: string, duration?: number) => {
    addNotification({ type: 'info', message, duration });
  }, [addNotification]);

  return {
    notifications,
    removeNotification,
    showSuccess,
    showError,
    showWarning,
    showInfo,
  };
}

// ============================================
// useDebounce Hook
// ============================================
export function useDebounce<T>(value: T, delay: number): T {
  const [debouncedValue, setDebouncedValue] = useState(value);

  useEffect(() => {
    const handler = setTimeout(() => {
      setDebouncedValue(value);
    }, delay);

    return () => {
      clearTimeout(handler);
    };
  }, [value, delay]);

  return debouncedValue;
}

// ============================================
// useLocalStorage Hook
// ============================================
export function useLocalStorage<T>(key: string, initialValue: T) {
  const [storedValue, setStoredValue] = useState<T>(() => {
    try {
      const item = window.localStorage.getItem(key);
      return item ? JSON.parse(item) : initialValue;
    } catch (error) {
      console.error(`Error loading ${key} from localStorage:`, error);
      return initialValue;
    }
  });

  const setValue = useCallback((value: T | ((val: T) => T)) => {
    try {
      const valueToStore = value instanceof Function ? value(storedValue) : value;
      setStoredValue(valueToStore);
      window.localStorage.setItem(key, JSON.stringify(valueToStore));
    } catch (error) {
      console.error(`Error saving ${key} to localStorage:`, error);
    }
  }, [key, storedValue]);

  return [storedValue, setValue] as const;
}
```

### 24.3 é«˜çº§ç»„ä»¶æ¨¡å¼

```typescript
// frontend/src/components/common/AsyncBoundary.tsx
import { ComponentType, Suspense, lazy } from 'react';
import { ErrorBoundary } from 'react-error-boundary';
import { PulseLoader } from 'react-spinners';

interface AsyncBoundaryProps {
  children: React.ReactNode;
  fallback?: React.ReactNode;
  errorFallback?: React.ReactNode;
}

export function AsyncBoundary({
  children,
  fallback = <LoadingFallback />,
  errorFallback = <ErrorFallback />,
}: AsyncBoundaryProps) {
  return (
    <Suspense fallback={fallback}>
      <ErrorBoundary FallbackComponent={ErrorFallback}>
        {children}
      </ErrorBoundary>
    </Suspense>
  );
}

function LoadingFallback() {
  return (
    <div className="flex items-center justify-center h-64">
      <PulseLoader color="#3b82f6" size={15} />
    </div>
  );
}

function ErrorFallback({ error, resetErrorBoundary }: { error: Error; resetErrorBoundary: () => void }) {
  return (
    <div className="flex flex-col items-center justify-center h-64 text-center">
      <div className="text-red-500 text-6xl mb-4">âš ï¸</div>
      <h3 className="text-lg font-semibold mb-2">å‡ºé”™äº†</h3>
      <p className="text-gray-600 mb-4">{error.message}</p>
      <button
        onClick={resetErrorBoundary}
        className="px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600"
      >
        é‡è¯•
      </button>
    </div>
  );
}

// ============================================
// æ‡’åŠ è½½HOC
// ============================================
export function withLazyLoading<P extends object>(
  component: ComponentType<P>,
  loadingComponent?: React.ReactNode
) {
  return lazy(() => {
    return new Promise((resolve) => {
      setTimeout(() => {
        resolve({
          default: component,
        });
      }, 300);
    });
  });
}

// ============================================
// VirtualListç»„ä»¶
// ============================================
import { useVirtualizer } from '@tanstack/react-virtual';

interface VirtualListProps<T> {
  items: T[];
  renderItem: (item: T, index: number) => React.ReactNode;
  itemHeight: number;
  height: number;
  overscan?: number;
}

export function VirtualList<T>({
  items,
  renderItem,
  itemHeight,
  height,
  overscan = 5,
}: VirtualListProps<T>) {
  const parentRef = useRef<HTMLDivElement>(null);

  const virtualizer = useVirtualizer({
    count: items.length,
    getScrollElement: () => parentRef.current,
    estimateSize: () => itemHeight,
    overscan,
  });

  return (
    <div ref={parentRef} style={{ height, overflow: 'auto' }}>
      <div
        style={{
          height: `${virtualizer.getTotalSize()}px`,
          width: '100%',
          position: 'relative',
        }}
      >
        {virtualizer.getVirtualItems().map((virtualItem) => (
          <div
            key={virtualItem.key}
            style={{
              position: 'absolute',
              top: 0,
              left: 0,
              width: '100%',
              transform: `translateY(${virtualItem.start}px)`,
            }}
          >
            {renderItem(items[virtualItem.index], virtualItem.index)}
          </div>
        ))}
      </div>
    </div>
  );
}

// ============================================
// InfiniteScrollç»„ä»¶
// ============================================
interface InfiniteScrollProps<T> {
  fetchMore: (page: number) => Promise<T[]>;
  renderItem: (item: T, index: number) => React.ReactNode;
  initialPage?: number;
  pageSize?: number;
}

export function InfiniteScroll<T>({
  fetchMore,
  renderItem,
  initialPage = 1,
  pageSize = 20,
}: InfiniteScrollProps<T>) {
  const [items, setItems] = useState<T[]>([]);
  const [page, setPage] = useState(initialPage);
  const [loading, setLoading] = useState(false);
  const [hasMore, setHasMore] = useState(true);

  const loadMore = useCallback(async () => {
    if (loading || !hasMore) return;

    setLoading(true);
    try {
      const newItems = await fetchMore(page);
      setItems((prev) => [...prev, ...newItems]);
      setPage((prev) => prev + 1);

      if (newItems.length < pageSize) {
        setHasMore(false);
      }
    } catch (error) {
      console.error('Failed to load more items:', error);
    } finally {
      setLoading(false);
    }
  }, [fetchMore, page, loading, hasMore, pageSize]);

  useEffect(() => {
    loadMore();
  }, []);

  const observerTarget = useRef<HTMLDivElement>(null);

  useEffect(() => {
    const observer = new IntersectionObserver(
      (entries) => {
        if (entries[0].isIntersecting && hasMore && !loading) {
          loadMore();
        }
      },
      { threshold: 1.0 }
    );

    if (observerTarget.current) {
      observer.observe(observerTarget.current);
    }

    return () => observer.disconnect();
  }, [loadMore, hasMore, loading]);

  return (
    <div>
      {items.map((item, index) => renderItem(item, index))}
      <div ref={observerTarget} className="h-4" />
      {loading && <LoadingFallback />}
    </div>
  );
}
```

### 24.4 æ€§èƒ½ä¼˜åŒ–ç»„ä»¶

```typescript
// frontend/src/components/common/MemoizedComponents.tsx
import { memo, useMemo, useCallback } from 'react';

// ============================================
// Memoized Trader Card
// ============================================
interface TraderCardProps {
  trader: Trader;
  onSelect: (id: string) => void;
  isSelected: boolean;
}

export const TraderCard = memo<TraderCardProps>(({ trader, onSelect, isSelected }) => {
  const handleClick = useCallback(() => {
    onSelect(trader.id);
  }, [trader.id, onSelect]);

  const pnlColor = useMemo(() => {
    if (trader.pnl > 0) return 'text-green-500';
    if (trader.pnl < 0) return 'text-red-500';
    return 'text-gray-500';
  }, [trader.pnl]);

  const pnlPercent = useMemo(() => {
    return ((trader.pnl / trader.equity) * 100).toFixed(2);
  }, [trader.pnl, trader.equity]);

  return (
    <div
      onClick={handleClick}
      className={`
        p-4 rounded-lg cursor-pointer transition-all
        ${isSelected ? 'bg-blue-500 text-white' : 'bg-white hover:bg-gray-50'}
      `}
    >
      <h3 className="font-semibold">{trader.name}</h3>
      <p className={`text-sm ${pnlColor}`}>
        Â¥{trader.pnl.toLocaleString()} ({pnlPercent}%)
      </p>
      <p className="text-xs text-gray-500">
        æƒç›Š: Â¥{trader.equity.toLocaleString()}
      </p>
    </div>
  );
}, (prevProps, nextProps) => {
  return (
    prevProps.trader.id === nextProps.trader.id &&
    prevProps.trader.equity === nextProps.trader.equity &&
    prevProps.trader.pnl === nextProps.trader.pnl &&
    prevProps.isSelected === nextProps.isSelected
  );
});

TraderCard.displayName = 'TraderCard';

// ============================================
// Optimized Position Table
// ============================================
import { useTable, useSortBy, usePagination } from 'react-table';
import { useMemo } from 'react';

interface PositionTableProps {
  positions: Position[];
}

export function PositionTable({ positions }: PositionTableProps) {
  const data = useMemo(() => positions, [positions]);

  const columns = useMemo(() => [
    {
      Header: 'è‚¡ç¥¨ä»£ç ',
      accessor: 'symbol',
    },
    {
      Header: 'äº¤æ˜“æ‰€',
      accessor: 'exchange',
    },
    {
      Header: 'æ•°é‡',
      accessor: 'quantity',
      Cell: ({ value }: { value: number }) => value.toLocaleString(),
    },
    {
      Header: 'æŒä»“ä»·',
      accessor: 'entryPrice',
      Cell: ({ value }: { value: number }) => `Â¥${value.toFixed(2)}`,
    },
    {
      Header: 'ç°ä»·',
      accessor: 'currentPrice',
      Cell: ({ value }: { value: number }) => `Â¥${value.toFixed(2)}`,
    },
    {
      Header: 'æœªå®ç°ç›ˆäº',
      accessor: 'unrealizedPnl',
      Cell: ({ value }: { value: number }) => (
        <span className={value >= 0 ? 'text-green-500' : 'text-red-500'}>
          Â¥{value.toFixed(2)}
        </span>
      ),
    },
  ], []);

  const tableInstance = useTable(
    { columns, data },
    useSortBy,
    usePagination
  );

  const {
    getTableProps,
    getTableBodyProps,
    headerGroups,
    page,
    prepareRow,
  } = tableInstance;

  return (
    <div className="overflow-x-auto">
      <table {...getTableProps()} className="min-w-full divide-y divide-gray-200">
        <thead className="bg-gray-50">
          {headerGroups.map((headerGroup) => (
            <tr {...headerGroup.getHeaderGroupProps()}>
              {headerGroup.headers.map((column) => (
                <th
                  {...column.getHeaderProps(column.getSortByToggleProps())}
                  className="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider"
                >
                  {column.render('Header')}
                  <span>
                    {column.isSorted
                      ? column.isSortedDesc
                        ? ' ğŸ”½'
                        : ' ğŸ”¼'
                      : ''}
                  </span>
                </th>
              ))}
            </tr>
          ))}
        </thead>
        <tbody {...getTableBodyProps()} className="bg-white divide-y divide-gray-200">
          {page.map((row) => {
            prepareRow(row);
            return (
              <tr {...row.getRowProps()}>
                {row.cells.map((cell) => (
                  <td
                    {...cell.getCellProps()}
                    className="px-6 py-4 whitespace-nowrap text-sm text-gray-900"
                  >
                    {cell.render('Cell')}
                  </td>
                ))}
              </tr>
            );
          })}
        </tbody>
      </table>
    </div>
  );
}

// ============================================
// Debounced Search Input
// ============================================
import { useDebouncedCallback } from 'use-debounce';

interface SearchInputProps {
  onSearch: (query: string) => void;
  placeholder?: string;
  debounceMs?: number;
}

export function SearchInput({
  onSearch,
  placeholder = 'æœç´¢...',
  debounceMs = 300,
}: SearchInputProps) {
  const [query, setQuery] = useState('');

  const debouncedSearch = useDebouncedCallback(
    (value: string) => onSearch(value),
    debounceMs
  );

  const handleChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    const value = e.target.value;
    setQuery(value);
    debouncedSearch(value);
  };

  return (
    <div className="relative">
      <input
        type="text"
        value={query}
        onChange={handleChange}
        placeholder={placeholder}
        className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-blue-500 focus:border-transparent"
      />
      <svg
        className="absolute right-3 top-2.5 h-5 w-5 text-gray-400"
        fill="none"
        stroke="currentColor"
        viewBox="0 0 24 24"
      >
        <path
          strokeLinecap="round"
          strokeLinejoin="round"
          strokeWidth={2}
          d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"
        />
      </svg>
    </div>
  );
}
```

---

## ç¬¬25ç«  ç”Ÿäº§è¿ç»´æ‰‹å†Œ

### 25.1 æ—¥å¸¸è¿ç»´æ£€æŸ¥æ¸…å•

#### æ¯æ—¥æ£€æŸ¥é¡¹

```bash
#!/bin/bash
# scripts/daily_health_check.sh

echo "========================================="
echo "DeepAlpha æ—¥å¸¸å¥åº·æ£€æŸ¥"
echo "æ—¥æœŸ: $(date)"
echo "========================================="

# 1. æœåŠ¡çŠ¶æ€æ£€æŸ¥
echo -e "\n[1] æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
kubectl get pods -n deepalpha-production

# 2. æ£€æŸ¥Podå¥åº·çŠ¶æ€
echo -e "\n[2] æ£€æŸ¥Podå¥åº·çŠ¶æ€..."
kubectl get pods -n deepalpha-production -o json | \
  jq -r '.items[] | select(.status.containerStatuses[].ready != true) | "\(.metadata.name): \(.status.containerStatuses[].state)"'

# 3. æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
echo -e "\n[3] æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ..."
kubectl top nodes
kubectl top pods -n deepalpha-production

# 4. æ£€æŸ¥ç£ç›˜ç©ºé—´
echo -e "\n[4] æ£€æŸ¥ç£ç›˜ç©ºé—´..."
df -h | grep -E '(Filesystem|/dev/)'

# 5. æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo -e "\n[5] æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
kubectl exec -n deepalpha-production deployment/deepalpha-api -- \
  pg_isready -h postgres -U deepalpha

# 6. æ£€æŸ¥Redisè¿æ¥
echo -e "\n[6] æ£€æŸ¥Redisè¿æ¥..."
kubectl exec -n deepalpha-production deployment/deepalpha-api -- \
  redis-cli -h redis ping

# 7. æ£€æŸ¥APIå¥åº·ç«¯ç‚¹
echo -e "\n[7] æ£€æŸ¥APIå¥åº·ç«¯ç‚¹..."
curl -f http://api.deepalpha.example.com/health || echo "APIå¥åº·æ£€æŸ¥å¤±è´¥"

# 8. æ£€æŸ¥æ—¥å¿—é”™è¯¯
echo -e "\n[8] æ£€æŸ¥æœ€è¿‘1å°æ—¶é”™è¯¯æ—¥å¿—..."
kubectl logs -n deepalpha-production -l app=deepalpha --since=1h | grep -i error | tail -20

# 9. æ£€æŸ¥Celeryä»»åŠ¡é˜Ÿåˆ—
echo -e "\n[9] æ£€æŸ¥Celeryä»»åŠ¡é˜Ÿåˆ—..."
curl -s http://flower.deepalpha.example.com/api/workers | jq -r '.[] | "\(.name): \(.status)"'

# 10. æ£€æŸ¥Prometheuså‘Šè­¦
echo -e "\n[10] æ£€æŸ¥æ´»åŠ¨å‘Šè­¦..."
curl -s 'http://prometheus.deepalpha.example.com/api/v1/alerts' | \
  jq -r '.data.alerts[] | select(.state=="firing") | "\(.labels.alertname): \(.annotations.summary)"'

echo -e "\n========================================="
echo "æ£€æŸ¥å®Œæˆ"
echo "========================================="
```

#### æ¯å‘¨æ£€æŸ¥é¡¹

```bash
#!/bin/bash
# scripts/weekly_check.sh

echo "========================================="
echo "DeepAlpha æ¯å‘¨æ£€æŸ¥"
echo "æ—¥æœŸ: $(date)"
echo "========================================="

# 1. æ•°æ®åº“æ€§èƒ½åˆ†æ
echo -e "\n[1] æ•°æ®åº“æ…¢æŸ¥è¯¢åˆ†æ..."
kubectl exec -n deepalpha-production postgres-0 -- \
  psql -U deepalpha -d deepalpha -c "
  SELECT query, calls, total_time, mean_time
  FROM pg_stat_statements
  ORDER BY mean_time DESC
  LIMIT 10;
"

# 2. æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
echo -e "\n[2] æ£€æŸ¥æœªä½¿ç”¨çš„ç´¢å¼•..."
kubectl exec -n deepalpha-production postgres-0 -- \
  psql -U deepalpha -d deepalpha -c "
  SELECT schemaname, tablename, indexname
  FROM pg_stat_user_indexes
  WHERE idx_scan = 0
  AND indisunique = false;
"

# 3. åˆ†æè¡¨è†¨èƒ€æƒ…å†µ
echo -e "\n[3] åˆ†æè¡¨è†¨èƒ€..."
kubectl exec -n deepalpha-production postgres-0 -- \
  psql -U deepalpha -d deepalpha -c "
  SELECT
    schemaname,
    tablename,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size,
    pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) AS table_size,
    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) AS index_size
  FROM pg_tables
  WHERE schemaname = 'public'
  ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
  LIMIT 10;
"

# 4. æ£€æŸ¥SSLè¯ä¹¦æœ‰æ•ˆæœŸ
echo -e "\n[4] æ£€æŸ¥SSLè¯ä¹¦..."
openssl s_client -connect api.deepalpha.example.com:443 -servername api.deepalpha.example.com </dev/null 2>/dev/null | \
  openssl x509 -noout -dates

# 5. æ£€æŸ¥ä¾èµ–æ›´æ–°
echo -e "\n[5] æ£€æŸ¥Pythonä¾èµ–æ›´æ–°..."
pip list --outdated

# 6. æ£€æŸ¥å®‰å…¨æ¼æ´
echo -e "\n[6] æ£€æŸ¥å®‰å…¨æ¼æ´..."
safety check --json || true

echo -e "\n========================================="
echo "æ¯å‘¨æ£€æŸ¥å®Œæˆ"
echo "========================================="
```

### 25.2 æ•…éšœæ’æŸ¥æµç¨‹

#### APIå“åº”ç¼“æ…¢

```bash
#!/bin/bash
# scripts/troubleshoot/slow_api.sh

echo "è¯Šæ–­APIå“åº”ç¼“æ…¢é—®é¢˜..."

# 1. æ£€æŸ¥Podèµ„æºé™åˆ¶
echo "[1] æ£€æŸ¥Podèµ„æºä½¿ç”¨..."
kubectl top pods -n deepalpha-production -l app=deepalpha-api

# 2. æ£€æŸ¥æ•°æ®åº“æ…¢æŸ¥è¯¢
echo "[2] æ£€æŸ¥æ•°æ®åº“æ…¢æŸ¥è¯¢..."
kubectl exec -n deepalpha-production postgres-0 -- \
  psql -U deepalpha -d deepalpha -c "
  SELECT pid, now() - query_start as duration, query
  FROM pg_stat_activity
  WHERE state = 'active'
  ORDER BY duration DESC;
"

# 3. æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± 
echo "[3] æ£€æŸ¥æ•°æ®åº“è¿æ¥æ± ..."
kubectl exec -n deepalpha-production deployment/deepalpha-api -- \
  python -c "
import asyncpg
import asyncio

async def check_connections():
    conn = await asyncpg.connect('postgresql://deepalpha:password@postgres:5432/deepalpha')
    result = await conn.fetchval('SELECT count(*) FROM pg_stat_activity WHERE datname = $1', 'deepalpha')
    print(f'å½“å‰æ•°æ®åº“è¿æ¥æ•°: {result}')
    await conn.close()

asyncio.run(check_connections())
"

# 4. æ£€æŸ¥Redisæ€§èƒ½
echo "[4] æ£€æŸ¥Redisæ€§èƒ½..."
kubectl exec -n deepalpha-production redis-0 -- redis-cli INFO stats | grep -E '(instantaneous_ops_per_sec|used_memory)'

# 5. æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ
echo "[5] æ£€æŸ¥Podé—´ç½‘ç»œå»¶è¿Ÿ..."
kubectl exec -n deepalpha-production deployment/deepalpha-api -- \
  ping -c 10 postgres.deepalpha-production.svc.cluster.local

# 6. åˆ†æåº”ç”¨æ—¥å¿—
echo "[6] æ£€æŸ¥æ…¢è¯·æ±‚æ—¥å¿—..."
kubectl logs -n deepalpha-production -l app=deepalpha-api --tail=1000 | \
  grep -i "slow request" | tail -20

# 7. æ£€æŸ¥CPU Throttling
echo "[7] æ£€æŸ¥CPUé™é€Ÿ..."
kubectl get pods -n deepalpha-production -l app=deepalpha-api -o json | \
  jq -r '.items[] | "\(.metadata.name): \(.status.containerStatuses[].state.terminated.reason)"'
```

#### æ•°æ®åº“è¿æ¥æ± è€—å°½

```python
# scripts/troubleshoot/db_pool.py
import asyncio
import asyncpg
from typing import List

async def diagnose_db_pool():
    """è¯Šæ–­æ•°æ®åº“è¿æ¥æ± é—®é¢˜"""

    conn = await asyncpg.connect(
        'postgresql://deepalpha:password@localhost:5432/deepalpha'
    )

    try:
        # 1. æ£€æŸ¥å½“å‰è¿æ¥æ•°
        result = await conn.fetchval("""
            SELECT count(*)
            FROM pg_stat_activity
            WHERE datname = 'deepalpha'
        """)
        print(f"å½“å‰è¿æ¥æ•°: {result}")

        # 2. æ£€æŸ¥è¿æ¥çŠ¶æ€åˆ†å¸ƒ
        rows = await conn.fetch("""
            SELECT state, count(*)
            FROM pg_stat_activity
            WHERE datname = 'deepalpha'
            GROUP BY state
        """)
        print("\nè¿æ¥çŠ¶æ€åˆ†å¸ƒ:")
        for row in rows:
            print(f"  {row['state']}: {row['count']}")

        # 3. æ£€æŸ¥é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
        rows = await conn.fetch("""
            SELECT
                pid,
                now() - query_start as duration,
                state,
                query
            FROM pg_stat_activity
            WHERE datname = 'deepalpha'
            AND state = 'active'
            AND now() - query_start > interval '1 minute'
        """)
        if rows:
            print("\né•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢ (>1åˆ†é’Ÿ):")
            for row in rows:
                print(f"  PID: {row['pid']}, æ—¶é•¿: {row['duration']}")
                print(f"  æŸ¥è¯¢: {row['query'][:100]}...")

        # 4. æ£€æŸ¥ç©ºé—²è¿æ¥
        result = await conn.fetchval("""
            SELECT count(*)
            FROM pg_stat_activity
            WHERE datname = 'deepalpha'
            AND state = 'idle'
            AND now() - query_start > interval '5 minutes'
        """)
        print(f"\né•¿æ—¶é—´ç©ºé—²è¿æ¥ (>5åˆ†é’Ÿ): {result}")

        # 5. å»ºè®®æ¸…ç†
        if result > 0:
            print("\nå»ºè®®: ç»ˆæ­¢é•¿æ—¶é—´ç©ºé—²è¿æ¥")
            print("æ‰§è¡Œ: SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE ...")

    finally:
        await conn.close()

if __name__ == '__main__':
    asyncio.run(diagnose_db_pool())
```

### 25.3 åº”æ€¥å“åº”ç¨‹åº

#### ç´§æ€¥å›æ»š

```bash
#!/bin/bash
# scripts/emergency/rollback.sh

set -e

VERSION_TO_ROLLBACK=$1
NAMESPACE=${2:-deepalpha-production}

if [ -z "$VERSION_TO_ROLLBACK" ]; then
  echo "ç”¨æ³•: $0 <ç‰ˆæœ¬> [å‘½åç©ºé—´]"
  echo "ç¤ºä¾‹: $0 v1.2.3 deepalpha-production"
  exit 1
fi

echo "========================================="
echo "ç´§æ€¥å›æ»šåˆ°ç‰ˆæœ¬: $VERSION_TO_ROLLBACK"
echo "å‘½åç©ºé—´: $NAMESPACE"
echo "æ—¶é—´: $(date)"
echo "========================================="

# 1. é€šçŸ¥Slack
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-Type: application/json' \
  -d "{\"text\": \"ğŸš¨ å¼€å§‹ç´§æ€¥å›æ»šåˆ° $VERSION_TO_ROLLBACK\"}"

# 2. è®°å½•å½“å‰ç‰ˆæœ¬
CURRENT_VERSION=$(kubectl get deployment deepalpha-api -n $NAMESPACE -o jsonpath='{.spec.template.spec.containers[0].image}')
echo "å½“å‰ç‰ˆæœ¬: $CURRENT_VERSION"

# 3. æ‰§è¡Œå›æ»š
echo "æ‰§è¡Œå›æ»š..."
kubectl set image deployment/deepalpha-api \
  deepalpha-api=ghcr.io/your-org/deepalpha:$VERSION_TO_ROLLBACK \
  -n $NAMESPACE

# 4. ç­‰å¾…å›æ»šå®Œæˆ
echo "ç­‰å¾…å›æ»šå®Œæˆ..."
kubectl rollout status deployment/deepalpha-api -n $NAMESPACE --timeout=5m

# 5. éªŒè¯å¥åº·çŠ¶æ€
echo "éªŒè¯å¥åº·çŠ¶æ€..."
sleep 30

HEALTH_CHECK_URL="http://api.deepalpha.example.com/health"
MAX_ATTEMPTS=10
ATTEMPT=0

while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
  if curl -f $HEALTH_CHECK_URL; then
    echo "å¥åº·æ£€æŸ¥é€šè¿‡"
    break
  fi

  ATTEMPT=$((ATTEMPT + 1))
  echo "å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œé‡è¯• ($ATTEMPT/$MAX_ATTEMPTS)..."
  sleep 10
done

if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
  echo "å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œå›æ»šå¯èƒ½å­˜åœ¨é—®é¢˜!"
  exit 1
fi

# 6. é€šçŸ¥å›æ»šå®Œæˆ
curl -X POST "$SLACK_WEBHOOK" \
  -H 'Content-Type: application/json' \
  -d "{\"text\": \"âœ… å›æ»šåˆ° $VERSION_TO_ROLLBACK å®Œæˆ\"}"

echo "========================================="
echo "å›æ»šæˆåŠŸå®Œæˆ"
echo "========================================="
```

#### æ•°æ®åº“æ•…éšœæ¢å¤

```bash
#!/bin/bash
# scripts/emergency/db_recovery.sh

echo "========================================="
echo "æ•°æ®åº“æ•…éšœæ¢å¤ç¨‹åº"
echo "æ—¶é—´: $(date)"
echo "========================================="

# 1. æ£€æŸ¥ä¸»åº“çŠ¶æ€
echo "[1] æ£€æŸ¥ä¸»åº“çŠ¶æ€..."
kubectl get pod -n deepalpha-production -l role=postgres,position=primary

# 2. æ£€æŸ¥ä»åº“çŠ¶æ€
echo "[2] æ£€æŸ¥ä»åº“çŠ¶æ€..."
kubectl get pod -n deepalpha-production -l role=postgres,position=replica

# 3. æå‡ä»åº“ä¸ºä¸»åº“
echo "[3] æå‡ä»åº“ä¸ºä¸»åº“..."
kubectl exec -n deepalpha-production postgres-1 -- \
  pg_ctl promote -D /var/lib/postgresql/data

# 4. æ›´æ–°ServiceæŒ‡å‘æ–°ä¸»åº“
echo "[4] æ›´æ–°Service..."
kubectl patch svc postgres -n deepalpha-production -p '{"spec":{"selector":{"position":"primary"}}}'

# 5. éªŒè¯è¿æ¥
echo "[5] éªŒè¯æ•°æ®åº“è¿æ¥..."
kubectl exec -n deepalpha-production deployment/deepalpha-api -- \
  pg_isready -h postgres -U deepalpha

echo "========================================="
echo "æ•…éšœè½¬ç§»å®Œæˆ"
echo "========================================="
```

### 25.4 æ€§èƒ½è°ƒä¼˜æŒ‡å—

#### æ•°æ®åº“è°ƒä¼˜

```sql
-- postgresql-tuning.sql

-- 1. é…ç½®å‚æ•°è°ƒæ•´
ALTER SYSTEM SET shared_buffers = '4GB';
ALTER SYSTEM SET effective_cache_size = '12GB';
ALTER SYSTEM SET maintenance_work_mem = '1GB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET default_statistics_target = 100;
ALTER SYSTEM SET random_page_cost = 1.1;
ALTER SYSTEM SET effective_io_concurrency = 200;
ALTER SYSTEM SET work_mem = '32MB';
ALTER SYSTEM SET min_wal_size = '1GB';
ALTER SYSTEM SET max_wal_size = '4GB';

-- é‡æ–°åŠ è½½é…ç½®
SELECT pg_reload_conf();

-- 2. åˆ›å»ºå…³é”®ç´¢å¼•
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_orders_trader_status
ON orders(trader_id, status);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_orders_created_at
ON orders(created_at DESC);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_trades_symbol_timestamp
ON trades(symbol, timestamp DESC);

CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_positions_trader_symbol
ON positions(trader_id, symbol);

-- 3. åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE orders;
ANALYZE trades;
ANALYZE positions;
ANALYZE traders;

-- 4. æŸ¥çœ‹é…ç½®
SELECT name, setting, unit, context
FROM pg_settings
WHERE name IN (
  'shared_buffers',
  'effective_cache_size',
  'work_mem',
  'maintenance_work_mem'
);
```

#### åº”ç”¨è°ƒä¼˜

```python
# config/production.py
from pydantic_settings import BaseSettings

class ProductionSettings(BaseSettings):
    # æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
    DB_POOL_SIZE: int = 20
    DB_MAX_OVERFLOW: int = 40
    DB_POOL_TIMEOUT: int = 30
    DB_POOL_RECYCLE: int = 3600

    # Redisè¿æ¥ä¼˜åŒ–
    REDIS_MAX_CONNECTIONS: int = 50
    REDIS_SOCKET_TIMEOUT: int = 5
    REDIS_SOCKET_CONNECT_TIMEOUT: int = 5

    # APIä¼˜åŒ–
    API_WORKERS: int = 4
    API_MAX_REQUEST_SIZE: int = 10 * 1024 * 1024  # 10MB
    API_TIMEOUT: int = 30

    # ç¼“å­˜ä¼˜åŒ–
    CACHE_TTL: int = 300  # 5åˆ†é’Ÿ
    CACHE_MAX_SIZE: int = 10000

    # LLMè°ƒç”¨ä¼˜åŒ–
    LLM_TIMEOUT: int = 30
    LLM_MAX_RETRIES: int = 3
    LLM_RATE_LIMIT: int = 100  # æ¯åˆ†é’Ÿ

    class Config:
        env_file = ".env.production"

settings = ProductionSettings()
```

---

## æ€»ç»“

æœ¬è¡¥å……æ–‡æ¡£æ¶µç›–DeepAlphaäº¤æ˜“ç³»ç»Ÿçš„ç”Ÿäº§éƒ¨ç½²å’Œè¿ç»´çš„è¯¦ç»†å†…å®¹ï¼š

**ç¬¬21ç«  - CI/CDæµæ°´çº¿**
- GitHub Actionså®Œæ•´å·¥ä½œæµ
- ä»£ç è´¨é‡æ£€æŸ¥ã€æµ‹è¯•ã€å®‰å…¨æ‰«æ
- Dockerå¤šé˜¶æ®µæ„å»º
- è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹

**ç¬¬22ç«  - Kuberneteséƒ¨ç½²**
- å®Œæ•´çš„K8sèµ„æºé…ç½®
- æ»šåŠ¨æ›´æ–°ã€è“ç»¿éƒ¨ç½²ã€é‡‘ä¸é›€å‘å¸ƒ
- HPAè‡ªåŠ¨æ‰©ç¼©å®¹
- ç½‘ç»œç­–ç•¥å’Œå®‰å…¨é…ç½®

**ç¬¬23ç«  - æ•°æ®è¿ç§»ä¸å¤‡ä»½**
- NOFX Goåˆ°Pythonçš„æ•°æ®è¿ç§»
- å¢é‡æ•°æ®åŒæ­¥
- å®Œæ•´çš„å¤‡ä»½æ¢å¤è„šæœ¬

**ç¬¬24ç«  - å‰ç«¯é«˜çº§æ¨¡å¼**
- ZustandçŠ¶æ€ç®¡ç†
- è‡ªå®šä¹‰Hooks
- æ€§èƒ½ä¼˜åŒ–ç»„ä»¶
- è™šæ‹Ÿåˆ—è¡¨ã€æ— é™æ»šåŠ¨

**ç¬¬25ç«  - ç”Ÿäº§è¿ç»´**
- æ—¥å¸¸æ£€æŸ¥æ¸…å•
- æ•…éšœæ’æŸ¥æµç¨‹
- åº”æ€¥å“åº”ç¨‹åº
- æ€§èƒ½è°ƒä¼˜æŒ‡å—

---

*æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ä¸­...*

*æœ€åæ›´æ–°: 2026-01-05*

<function_calls>
<invoke name="TodoWrite">
<parameter name="todos">[{"activeForm": "Creating CI/CD pipeline configuration documentation", "content": "Create Chapter 21: CI/CD Pipeline Configuration", "status": "completed"}, {"activeForm": "Creating Docker & Kubernetes deployment documentation", "content": "Create Chapter 22: Docker & Kubernetes Deployment", "status": "completed"}, {"activeForm": "Creating data migration & backup documentation", "content": "Create Chapter 23: Data Migration & Backup Strategies", "status": "completed"}, {"activeForm": "Creating advanced frontend patterns documentation", "content": "Create Chapter 24: Advanced Frontend Patterns & State Management", "status": "in_progress"}, {"activeForm": "Creating production runbooks documentation", "content": "Create Chapter 25: Production Runbooks & Operational Procedures", "status": "pending"}]